citanceNumber,referenceArticle,citingArticle,preTwo,preOne,citationSentence,postOne,postTwo,citationCategory
1,C00-2123.xml,C02-1050.xml,"This problem is known to be NP-Complete (Knight, 1999), for the reordering property in the model further complicates the search.",One of the solution is the left-to-right generation of output by consuming input words in any-order.,"Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.","(2001), though they all based on almost linearly Lexical Model t( f j |ei ) Translation Model Fertility Model n(φi |ei ) Distortion Model Head d1 ( j − cρi |A(eρi )B( f j )) NULL Translation Model m−φ0O pm−2φ0 φ0 Non-Head d1> ( j − j |B( f j )) φ0 0 p1 Figure 2: Translation Model (IBM Model 4) aligned language pairs, and not suitable for language pairs with totally different alignment correspondence, such as Japanese and English.","The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.(2001), and operation applied to each hypothesis is similar to those explained in Berger et al.(1996), Och et al.(2001) and Germann et al.",Multi Citance
2,C00-2123.xml,C02-1050.xml,"According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993).","Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words.","There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.","The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English.","Germann et al.(2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.",Multi Citance
3,C00-2123.xml,C02-1050.xml,One of the solution is the left-to-right generation of output by consuming input words in any-order.,"Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.(1996), Och et al.(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.(2001), though they all based on almost linearly Lexical Model t( f j |ei ) Translation Model Fertility Model n(φi |ei ) Distortion Model Head d1 ( j − cρi |A(eρi )B( f j )) NULL Translation Model m−φ0O pm−2φ0 φ0 Non-Head d1> ( j − j |B( f j )) φ0 0 p1 Figure 2: Translation Model (IBM Model 4) aligned language pairs, and not suitable for language pairs with totally different alignment correspondence, such as Japanese and English.","The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.","(2001), and operation applied to each hypothesis is similar to those explained in Berger et al.(1996), Och et al.(2001) and Germann et al.(2001).",The algorithm is depicted in Algorithm 1 where C = { jk : k = 1...|C|} represents a set of input string position 1.,Single Citance
4,C00-2123.xml,C02-1050.xml,"Then, the two hypotheses are merged when both are open and can share the same output word e, which resulted in raising the fertility of e. If both of them are closed hypotheses, then an additional sequence of zero fertility words (or NULL sequence) are inserted (refer to Figure 5).",3.4 Computational Complexity.,"The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.","The bidirectional method involves merging of two hypotheses, hence additional O( m O) is required.",3.5 Effects of Decoding Direction.,Single Citance
5,C00-2123.xml,C04-1091.xml,"Independently, Christoph Tillman adapted the Held-Karp dynamic programming algorithm for TSP (Held and Karp, 1962) to Decoding (Tillman, 2001).",The original Held- Karp algorithm for TSP is an exponential time dynamic programming algorithm and Tillman’s adaptation to Decoding has a prohibitive com plexity of O (l3m2 2m ) ≈ O (m5 2m ) (where mand l are the lengths of the source and tar get sentences respectively).,"Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).","An optimal decoder based on the well-known A∗ heuristic was implemented and benchmarked in (Och et al., 2001).","Since optimal solution can not be computed for practical problem instances in a reasonable amount of time, much of recent work has focused on good quality suboptimal solutions.",Single Citance
6,C00-2123.xml,E06-1004.xml,"The computational tasks involving IBM Models are the following: • Viterbi Alignment Given the model parameters and a sentence pair (f , e), determine the most probable alignment between f and e. a∗ = argmax P (f , a|e) a • Expectation Evaluation This forms the core of model training via the EM algorithm.",Please see Section 2.3 for a description of the computational task involved in the EM iterations.,"â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).","Exact Decoding is the original decoding problem as defined in (Brown et al., 1993) and Relaxed Decoding is the relaxation of the decoding problem typically used in practice.","While several heuristics have been developed by practitioners of SMT for the computational tasks involving IBM models, not much is known about the computational complexity of these tasks.",Multi Citance
7,C00-2123.xml,H01-1062.xml,Figure 5: Illustration of bottom-to-top search.,perimental tests were performed for both text and speech input.,"To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];","For more details, see [23].",The offline tests were performed on text input for the translation direction from German to English.,Single Citance
8,C00-2123.xml,J03-1005.xml,"Although the last approach is guaranteed to find the optimal solution, it is tested only for input sentences of length eight or shorter.",This article will present a DP-based beam search decoder for the IBM4 translation model.,A preliminary version of the work presented here was published in Tillmann and Ney (2000).,3.1 Inverted Alignment Concept.,"To explicitly describe the word order difference between source and target language, Brown et al.(1993) introduced an alignment concept, in which a source position j is mapped to exactly one target position i: regular alignment: j → i = aj 101 . May of fourth the on you visit not can colleague my case this In I d F k m K S a v M n b . n i a a e o i m i a i e Figure 2 e l n s l n e m i l e e i c s n l r h u e t t c g e h e n e n Regular alignment example for the translation direction German to English.",Pre Contiguous
9,C00-2123.xml,J04-2003.xml,1.2 Statistical Machine Translation.,"In statistical machine translation, every target language string eI = e1 ··· eI is assigned a probability Pr(eI ) of being a valid word sequence in the target language and a probability Pr(eI |f J ) of being a translation for the given source language string f J = 1 1 1 f1 ··· fJ . According to Bayes’ decision rule, the optimal translation for f J is the target string that maximizes the product of the target language model Pr(eI ) and the string translation model Pr(f J |eI ).","Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.",The probability that a certain target language word will occur in the target string is assumed to depend basically only on the source words aligned with it.,1.3 Related Work.,Multi Citance
10,C00-2123.xml,J04-4002.xml,"In the second step, we determine a set of probable target language words for each target word position in the alignment template instantiation.",Only these words are then hypothesized in the search.,We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).,"As a criterion for a word e at position i in the alignment template instantiation, we use δ(Ei , C(e)) · J· j=0 A˜ (i, j) A˜ (i·, j) · p(e | fj ) (28) In our experiments, we hypothesize only the five best-scoring words.","A decision is a triple d = (Z, e, l) consisting of an alignment template instantiation Z, the generated word e, and the index l of the generated word in Z. A hypothesis n corresponds to a valid sequence of decisions di . The possible decisions are as follows: 1.",Single Citance
11,C00-2123.xml,N03-1010.xml,"Using the same evaluation metric (but different evaluation data), Wang and Waibel (1997) report search error rates of 7.9% and 9.3%, respectively, for their decoders.",Och et al.(2001) and Germann et al.(2001) both implemented optimal decoders and benchmarked approximative algorithms against them.,"Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).","Germann et al.(2001) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem (cf.Knight, 1999).",Their overall performance metric is the sentence error rate (SER).,Single Citance
12,C00-2123.xml,P01-1027.xml,"Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors.",The PER is guaranteed to be less than or equal to the WER.,"We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.",The translation results in terms of error rates are shown in Table 8.,We use Model 4 in order to perform the translation experiments because Model 4 typically gives better translation results than Model 5.,Single Citance
13,C00-2123.xml,P03-1039.xml,We directly applied the Lexicon Model and Fertility Model to the chunk-based translation model but set other parameters as uniform.,"Table 1: Basic Travel Expression Corpus Japanese English # of sentences 171,894 3.4 Decoding.","The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.",The decoder consists of two stages: 1.,Generate possible output chunks for all possi-.,Single Citance
14,C00-2123.xml,P03-1039.xml,Generate hypothesized output by consuming.,input chunks in arbitrary order and combining possible output chunks in left-to-right order.,"The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).","In addition, an example-based method is also introduced, which generates candidate chunks by looking up the viterbi chunking and alignment from a training corpus.","Since the combination of all possible chunks is computationally very expensive, we have introduced the following pruning and scoring strategies.",Single Citance
15,C00-2123.xml,W01-1404.xml,"The advantage is increased processing speed, which benefits real- time applications involving spoken language.",Several studies have investigated automatic or partly automatic learning of transductions for machine translation.,"Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).",In this paper we will investigate both context- free and finite-state models.,"The basis for our study is context-free transduction since that is a powerful model of translation, which can in many cases adequately describe the changes of word The second address is the current contact address; supported by the Royal Netherlands Academy of Arts and Sciences; current secondary affiliation is the German Research Center for Artificial Intelligence (DFKI).",Multi Citance
16,C00-2123.xml,W01-1407.xml,Table 3: Statistics of the Verbmobil test corpus for German-to-English translation.,Unknowns are word forms not contained in the training corpus.,"We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).","7.3.1 Lexicon Combination So far we have performed experiments with hierarchical lexica, where two levels are combined, i.e. in Equation (2) is set to 1.","and are set to and is modeled as a uniform distribution over all derivations of the lemma occurring in the training data plus the base form itself, in case it is not contained.",Multi Citance
17,C00-2123.xml,W01-1408.xml,"The trigram language model we use has the property that if the count of the bigram N (u, v) = 0, then the probability P (w|u, v) depends only on v. In this case the recombination can be significantly improved by recombining all nodes whose language model state has the property N (u, v) = 0 only with respect to v. Obviously, this could be generalized to other types of language models as well.","Experiments have shown that by using this efficient recombination, the number of needed hypotheses can be reduced by about a factor of 4.","Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.",The search algorithm is based on a dynamic programming approach and applies various pruning techniques in order to restrict the number of considered hypotheses.,"For more details see (Tillmann, 2001).",Multi Citance
18,C00-2123.xml,W02-1020.xml,"To model word-sequence probabilities, we apply the chain rule: where J is the number of tokens in s and V is the size of the target-language vocabulary.","Compared to an equivalent noisy-channel combination of the form p(t)p(s|t), where t is the target sentence, our model is faster but less accurate.","It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).","It m−1 p(v1|h, s) n p(wi|h, v1, wi−1, s) × i=2 p(um|h, v1, wm−1, s).",(3) The probabilities of v1 and um can be expressed in terms of word probabilities as follows.,Multi Citance
19,C02-1025.xml,C10-2104.xml,"Complementary context In supervised learning, NER systems often suffer from low recall, which is caused by lack of both resource and context.","For example, a word like “Arkansas” may not appear in the training set and in the test set, there may not be enough context to infer its NE tag.","In such cases, neither global features (Chieu and Ng, 2002) nor aggregated contexts (Chieu and Ng, 2003) can help.","To overcome this deficiency, we employed the following unsupervised procedure: first, the baseline NER is applied to the target un-annotated corpus.","Second, we associate each word of the corpus with the most frequent NE category assigned in the previous step.",Multi Citance
20,C02-1025.xml,C10-2167.xml,Statistics from the corpus are used to determine the confidence scores of the extraction.,"In general information extraction, there are two approaches: rule-based and statistical.Early extraction systems are mainly based on rules (e.g., Riloff, 1993).","In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001).",CRF has been shown to be the most effective method.,"It was used in (Stoyanov et al., 2008).",Multi Citance
21,C02-1025.xml,I05-3013.xml,"It typically aims to recognize names for person, organization, location, and expressions of number, time and cur rency.",The objective is achieved by employing either handcrafted knowledge or supervised learning techniques.,"The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature.","Firstly, named entity is typically noun or noun phrase (NP), but NIL expression can be any kind, e.g. number “94” in NIL represents “ህᰃ” which is a verb meaning “exactly be”.","Secondly, named entities often have well-defined meanings in text and are tractable from a standard dictionary; but NIL expressions are either unknown to the dictionary or ambiguous.",Multi Citance
22,C02-1025.xml,I05-3030.xml,"The same as other modules, it is defined over HhT in segmentation T # arg max P(W | T )P(T | T ) , i i T1T2 Tn i 1 i i 1 disambiguation, where H is the set of possible contexts around target word that will be tagged, where Ti represents the tag of current word, and T is the set of allowable tags.",Then the Viterbi algorithm is used to search the best path.,"modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002).",Take Chinese person name as example.,"Firstly, we combine p(t | h) p(h, t ) ¦t 'T p(h, t ' ) , where HMM and Maximum Entropy (ME) model to lable the person name tag, e.g. “ྮ/CPB 䪰/CPI ṙ/CPI” (Tongmei Yao); Secondly, the tagged p(h, t) k SP D j 1 f j ( h,t ) j name is merged by combining ME Model and Support Vector Machine (SVM) and some aided rules, e.g. merged into “ྮ/䪰ṙ” in PKU test.",Multi Citance
23,C02-1025.xml,P03-1028.xml,Seed Word The sentence of the current slot fill contains a seed word for a different incident type.,"A number of seed words are automatically learned for each of the incident types ATTACK, BOMBING, and KIDNAPPING.","They are automatically derived based on the correlation metric value used in (Chieu and Ng, 2002a).","For the remaining incident types, there are too few incidents in the training data for seed words to be collected.",The seeds words used are shown in Table 2.,Single Citance
24,C02-1025.xml,P03-1028.xml,"The one notable exception is the work of UMass at MUC6 (Fisher et al., 1995).","Unfortunately, their learning approach did considerably worse than the best MUC6 systems.","Soder- land (1999) and Chieu and Ng (2002a) attempted machine learning approaches for a scaled-down version of the ST task, where it was assumed that the information needed to fill one template came from one sentence only.","In this paper, we present a learning approach to the full-scale ST task of extracting information from free texts.","The task we tackle is considerably more complex than that of (Soderland, 1999; Chieu and Ng, 2002a), since we need to deal with merging information from multiple sentences to fill one template.",Single Citance
25,C02-1025.xml,P05-1045.xml,"Instead of finding the maximum likelihood sequence over the entire document, they classify one sentence at a time, allowing them to condition on the maximum likelihood sequence of previous sentences.","This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward flow of information only, which is not sufficient for all cases of interest.","Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.",This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these “global” features relative to the local ones.,"However, this approach cannot easily be extended to incorporate other types of non-local structure.",Single Citance
26,C02-1025.xml,P05-1051.xml,People have spent considerable effort in engineering appropriate features to improve performance; most of these involve internal name structure or the immediate local context of the name.,Some other named entity systems have explored global information for name tagging.,"(Borthwick, 1999) made a second tagging pass which uses information on token sequences tagged in the first pass; (Chieu and Ng, 2002) used as features information about features assigned to other instances of the same token.Recently, in (Ji and Grishman, 2004) we pro posed a name tagging method which applied an SVM based on coreference information to filter the names with low confidence, and used coreference rules to correct and recover some names.","One limitation of this method is that in the process of discarding many incorrect names, it also discarded some correct names.",We attempted to recover some of these names by heuristic rules which are quite language specific.,Multi Citance
27,C02-1025.xml,W03-0423.xml,"In this year’s CoNLL, the NER task is to tag noun phrases with the following four classes: person (PER), organization (ORG), location (LOC), and miscellaneous (MISC).","This paper presents a maximum entropy approach to the NER task, where NER not only made use of local context within a sentence, but also made use of other occurrences of each word within the same document to extract useful features (global features).","Such global features enhance the performance of NER (Chieu and Ng, 2002b).","The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed.","Such constraints are derived from training data, expressing some relationship between features and outcome.",Single Citance
28,C02-1025.xml,W03-0423.xml,These lists are derived automatically from the training data.,Frequent Word List (FWL) This list consists of words that occur in more than 5 different documents.,"Useful Unigrams (UNI) For each name class, words that precede the name class are ranked using correlation metric (Chieu and Ng, 2002a), and the top 20 are compiled into a list.",Useful Bigrams (UBI) This list consists of bigrams of words that precede a name class.,"Examples are “CITY OF”, “ARRIVES IN”, etc. The list is compiled by taking bigrams with higher probability to appear before a name class than the unigram itself (e.g., “CITY OF” has higher probability to appear before a location than “OF”).",Single Citance
29,C02-1025.xml,W03-0423.xml,"For example, the ORG class often terminates with tokens such as INC and COMMITTEE, and the MISC class often terminates with CUP, OPEN, etc. Function Words (FUN) Lower case words that occur within a name class.","These include “van der”, “of”, etc. 3.2 Local Features.","The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).",Local features of a token w are those that are derived from the sentence containing w. Global features are derived by looking up other occurrences of w within the same document.,"In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).",Single Citance
30,C02-1025.xml,W03-0423.xml,"Case and Zone of w+1 and w−1 Similarly, if w+1 (or w−1 ) is initCaps, a feature (initCaps, zone)NEXT (or (initCaps, zone)PREV ) is set to 1, etc. Case Sequence Suppose both w−1 and w+1 are init- Caps.","Then if w is initCaps, a feature I is set to 1, else a feature N I is set to 1.","Token Information These features are based on the string w, such as contains-digits, contains-dollar-sign, etc (Chieu and Ng, 2002b).",Lexicon Feature The string of w is used as a feature.,This group contains a large number of features (one for each token string present in the training data).,Single Citance
31,C02-1025.xml,W03-0423.xml,"The basic features used by both ME1 and ME2 can be divided into two classes: local and global (Chieu and Ng, 2002b).",Local features of a token w are those that are derived from the sentence containing w. Global features are derived by looking up other occurrences of w within the same document.,"In this paper, w−i refers to the ith word before w, and w+i refers to the ith word after w. The features used are similar to those used in (Chieu and Ng, 2002b).","Local features include: First Word, Case, and Zone For English, each document is segmented by simple rules into 4 zones: headline (HL), author (AU), dateline (DL), and text (TXT).","To identify the zones, a DL sentence is first identified using a regular expression.",Single Citance
32,C02-1025.xml,W03-0432.xml,"Previous work in NER has been aware of this problem of dealing with words without accurate case information, and various workarounds have been exploited.","Most commonly, feature-based classifiers use a set of capitalisation features and a sentence-initial feature (Bikel et al., 1997).","Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach a â€œweakerâ€ classifier that did not use case information at all (Chieu and Ng, 2002b).",We propose a different solution to the problem of case- less words.,"Rather than noting their lack of case and treating them separately, we propose to restore the correct capitalisation as a preprocessing step, allowing all words to be treated in the same manner.",Multi Citance
33,C02-1025.xml,W04-0705.xml,These combined methods yield an absolute improvement of about 3.1% in tagger F score.,"The problem of name recognition and classification has been intensively studied since1995, when it was introduced as part of the MUC 6 Evaluation (Grishman and Sundheim, 1996).","AMaximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002)","However, the performance of even the best of these models1 has been limited by the amount of labeled training data available to them and the range of features which they employ.","In particular, most of these methods classify an instance of a name based on the information about that instance alone, and very local context of that instance – typically, one or 1 The best results reported for Chinese named entity recognition, on the MET-2 test corpus, are 0.92 to 0.95 F-measure for the different name types (Ye et al. 2002).",Multi Citance
34,C02-1025.xml,W04-0705.xml,8.4 Comparison to Cache Model.,"Some named entity systems use a name cache, in which tokens or complete names which have been previously assigned a tag are available as features in tagging the remainder of a document.","Other systems have made a second tagging pass which uses information on token sequences tagged in the first pass (Borthwick 1999), or have used as features information about features assigned to other instances of the same token (Chieu and Ng 2002).","Our system, while more complex, makes use of a richer set of global features, involving the detailed structure of individual mentions, and in particular makes use of both name – name and name – nominal relations.","We have compared the performance of our method (applied to single documents) with a voted cache model, which takes into account the number of times a particular name has been previously assigned each type of tag: System Precision Recall F baseline 88.8 90.5 89.1 voted cache 87.6 92.8 90.1 current 92.2 89.6 90.9 Table 9.",Multi Citance
35,C02-1025.xml,W06-0119.xml,"Conventionally, there are four approaches to develop a CWS: (1) Dictionary-based approach (Cheng et al. 1999), especial forward and backward maximum matching (Wong and Chan, 1996); (2) Linguistic approach based on syntax-semantic knowledge (Chen et al. 2002); (3) Statistical approach based on statistical language model (SLM) (Sproat and Shih, 1990; Teahan et al. 2000; Gao et al. 2003); and (4) Hybrid approach trying to combine the benefits of dictionary-based, linguistic and statistical approaches (Tsai et al. 2003; Ma and Chen, 2003).","In practice, statistical approaches are most widely used because their effective and reasonable performance.","To develop UWI, there are three approaches: (1) Statistical approach, researchers use common statistical features, such as maximum entropy (Chieu et al. 2002), association strength, mutual information, ambiguous matching, and multi-statistical features for unknown word detection and extraction;",Statistical approaches are simple and efficient whereas linguistic approaches are effective in identifying low frequency unknown words (Chen et al. 2002).,"To develop WSD, there are two major types of word segmentation ambiguities while there are no unknown word problems with them: (1) Overlap Ambiguity (OA).",Single Citance
36,C04-1089.xml,C10-1070.xml,"We also implemented the discounted log-odds (LO) described by (Evert, 2005, p. 86) in his work on collocation mining.","To our knowledge, this association measure has not been used yet in translation spotting.","It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).","In this work, we examine the performance of the best configuration of parameters we found, combined with a simple heuristic based on graphic similarity between source and target terms, similar to the orthographic features in (Haghighi et al., 2008)’s generative model.","This is very specific to our task where medical terms often (but not always) share Latin or Greek roots, such as microvillosite´s in French and microvilli in English.",Pre Contiguous
37,C04-1089.xml,C10-2164.xml,This paper also attempts to apply the OOV term translation mechanism above in EnglishChinese CLIR.,"It can be observed from the experimental results on the data sets of Text Retrieval Evaluation Conference (TREC) that the obvious performance improvement for query translation can be obtained, which is very beneficial to CLIR and can improve the whole retrieval performance.","At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).","In recent years, many researchers have utilized Web to find the translation candidates on webpages (Wu and Chang, 2007).","AlOnaizan and Knight (2002) used Web statistics information to validate the translation candidates generated by language model, and obtained the accuracy of 72.6% in ArabicEnglish OOV word translation.",Multi Citance
38,C04-1089.xml,D10-1042.xml,2.2 Corpus-based Approaches.,"Corpus-based approach can mine arbitrary translation pairs, by mining bilingual co-occurrences from parallel and comparable bilingual corpora.","Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.","Alternatively, (Lin et al., 2008) extracts bilingual co- occurrences from bilingual sentences, such as annotating terms with their corresponding translations in English inside parentheses.","Similarly, (Jiang et al., 2009) identifies potential translation pairs from bilingual sentences using lexical pattern analysis.",Multi Citance
39,C04-1089.xml,D12-1003.xml,"Various definitions for the context have been used: distance-based context (e.g. in a sentence (Laroche and Langlais, 2010), in a paragraph (Fung and McKeown, 1997), in a predefined window (Rapp, 1999; Andrade et al., 2010)), and syntactic-based context (e.g. predecessors and successors in dependency trees (Garera et al., 2009), certain dependency position (Otero and Campos, 2008)).","Some treated context words equally regardless of their positions (Fung and Yee, 1998), while others treated the words separately for each position (Rapp, 1999).","Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.",Andrade et al.(2010) used a set of words with a positive association as a context.,Andrade et al.(2011a) used dependency relations instead of context words.,Multi Citance
40,C04-1089.xml,D12-1003.xml,Fisˇer et al.(2011) and Kaji (2005) calculated 2-way similarities.,Step 3.Finding translation pairs: A pair of words is treated as a translation pair when their context similarity is high.,"Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).",2.1 Problems from Previous Works.,Most of previous methods used a seed bilingual lexicon for mapping modeled contexts in two different the method is not applicable for language pairs with different types of characters such as English and Japanese.,Multi Citance
41,C04-1089.xml,N09-1048.xml,"One is the purely monolingual corpora, wherein frequency-based expectation-maximization (EM, refer to (Dempster et al., 1977)) algorithms and cognate clues play a central role (Koehn and Knight, 2002).","Haghighi et al.(2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account.","The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).","In this paper, we focus on a special type of comparable corpus, parenthetical translations.","The issue is motivated by the observation that Web pages and technical papers written in Asian languages (e.g., Chinese, Japanese) sometimes annotate named entities or technical terms with their translations in English inside a pair of parentheses.",Multi Citance
42,C04-1089.xml,N09-1048.xml,"In order to deal with these problems, supervised (Cao et al., 2007) and unsupervised (Li et al., 2008) methods have been proposed.","However, supervised Numerous researchers have proposed a variety of automatic approaches to mine lexicons from the Web pages or other large-scale corpora.","Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.","Kuo et al.(2006) used active learning and unsupervised learning for mining transliteration lexicon from the Web pages, in which an EM process was used for estimating the phonetic similarities between English syllables and Chinese characters.","Cao et al.(2007) split parenthetical translation mining task into two parts, transliteration detection and translation detection.",Single Citance
43,C04-1089.xml,P06-1011.xml,"The Fragment-noLLR datasets bring no translation performance improvements; moreover, when the initial corpus is small (1M words) and the comparable corpus is noisy (BBC), the data has a negative impact on the BLEU score.","This indicates that LLRLex is a higher-quality lexicon than GIZALex, and an important component of our method.","Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).","Another related research effort is that of Resnik and Smith (2003), whose system is designed to discover parallel document pairs on the Web.","Our work lies between these two directions; we attempt to discover parallelism at the level of fragments, which are longer than one word but shorter than a document.",Multi Citance
44,C04-1089.xml,P13-1059.xml,But little reported work has shown the impact of joint name tagging on overall word alignment.,"Most of the previous name translation work combined supervised transliteration approaches with LM based re-scoring (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Huang et al., 2004).","Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).","However, most of these approaches required large amount of seeds, suffered from Information Extraction errors, and relied on phonetic similarity, context co-occurrence and document similarity for re-scoring.","In contrast, our name pair mining approach described in this paper does not require any machine translation or transliteration features.",Multi Citance
45,C04-1089.xml,P13-1062.xml,"based on their statements Si ∑ and Sj , respectively: TS (si,r , sj,r ) In this section, we present experimental settings and results of translating entity names using our TD (di , dj ) = (sE ,sC )∈B E C (6) methods compared with several baselines.",E C i i where B ⊂ Si ×Sj |SE | + |SE | − |B| is a greedy approximate solu 5.1 Data and.,"Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.(2011) and Shao and Ng (2004).","The English corpus consists of 100,746 news articles, edges in Si × Sj that maximize the sum of the and the Chinese corpus consists of 88,031 news selected edge weights and that do not share a node as their anchor point.",4.5 Iteration on TN.,Multi Citance
46,C04-1089.xml,P13-2036.xml,"As many new named entities appear every day in newspapers and web sites, their translations are nontrivial yet essential.Early efforts of named entity translation have focused on using phonetic feature (called PH) to estimate a phonetic similarity between two names (Knight and Graehl, 1998; Li et al., 2004; Virga and Khudanpur, 2003).","In contrast, some approaches have focused on using context feature (called CX) which compares surrounding words of entities (Fung and Yee, 1998; Diab and Finch, 2000; Laroche and Langlais, 2010).","Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).","(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.","(You et al., 2010) compute initial translation scores using PH and iteratively update the scores using relationship feature (called R).",Multi Citance
47,C04-1089.xml,P13-2036.xml,"In contrast, some approaches have focused on using context feature (called CX) which compares surrounding words of entities (Fung and Yee, 1998; Diab and Finch, 2000; Laroche and Langlais, 2010).","Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).","(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.","(You et al., 2010) compute initial translation scores using PH and iteratively update the scores using relationship feature (called R).","(Kim et al., 2011) boost You’s approach by additionally leveraging CX.",Single Citance
48,C04-1089.xml,W11-1215.xml,"However, all of these approaches made limited eﬀorts at disambiguating entities in queries and limited use of fact extraction in answer search and validation.","Several recent IE studies have stressed the beneﬁts of using information redundancy on estimating the correctness of the IE output (Downey et al., 2005; Yangarber, 2006; Patwardhan and Riloﬀ, 2009; Ji and Grish man, 2008).","Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).","To the best of our knowledge, this is the ﬁrst work on mining facts from comparable corpora for answer validation in a new crosslingual entity proﬁling task.",3.1 Task Deﬁnition.,Multi Citance
49,C04-1089.xml,W13-2501.xml,"1 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 1–10, Sofia, Bulgaria, August 8, 2013.",Qc 2013 Association for Computational Linguistics,"The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.","A translation candidate is generally considered as correct if it is an appropriate translation for at least one sense of the source word in the dictionary, which often corresponds to its most frequent sense.","An alternative consists in considering all translations provided for a word in the dictionary but weighting them by their frequency in the target language (Prochasson et al., 2009; Hazem and Morin, 2012).",Multi Citance
50,C04-1089.xml,W13-2512.xml,"Wu et al.(2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese.","The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007).","Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).","For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target Lepage and Denoual (2005) presented an analog- ical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance.","The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?]",Multi Citance
51,C08-1098.xml,C10-2023.xml,"Our test sets correspond to news-test2008 and new- stest2009 file sets, hereinafter referred to as Tune and Test respectively.","French, German and English Part-of-speech tags are computed by means of the TreeTagger 1 toolkit.","Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.).",L a n g. S e nt . W or ds V o c. O O V Ref s Train Fr en ch En gli sh 1.7 5 M 1.7 5 M 52.,4 M 47.,Single Citance
52,C08-1098.xml,D12-1133.xml,The feature set of the tagger was optimized for English and German and provides state-of-the- art accuracy for these two languages.,"The 1-best tagging accuracy for section 23 of the Penn Tree- bank is 97.28, which is on a par with Toutanova et al.(2003).","For German, we obtain a tagging accuracy of 97.24, which is close to the 97.39 achieved by the RFTagger (Schmid and Laws, 2008), which to our knowledge is the best tagger for German",We could not use the larger training set as it contains the test set of the CoNLL 2009 data that we use to evaluate the joint model.,"For Czech, the 1- best tagging accuracy is 99.11 and for Chinese 92.65 on the CoNLL 2009 test set.We trained parsers with 25 iterations and report 3 Training: 001–815, 1001–1136.",Single Citance
53,C08-1098.xml,D12-1133.xml,"For Czech, we get the best T- LAS with k = 3 and α = 0.2, where POS improves by 0.06 and LAS by 0.46.","For English, the best setting is k = 2 and α = 0.1 with a POS improvement of 0.17 and a LAS improvement of 0.62.","For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008).",6 While tagging accuracy (POS) increases with larger values.,"of α, TLAS decreases because of a drop in LAS.",Single Citance
54,C08-1098.xml,D13-1032.xml,For Arabic and German the improvements of higher-order models are bigger than the improvements due to MAs.,4.7 Comparison with Baselines.,"We use the following baselines: SVMTool (GimeÂ´nez and Ma`rquez, 2004), an SVM-based dis- criminative tagger; RFTagger (Schmid and Laws, 2008), an n-gram Hidden Markov Model (HMM) tagger developed for POS+MORPH tagging;","For POS+MORPH tagging, all baselines are trained on the concatenation of POS tag and MORPH tag.","We run SVM- Tool with the standard feature set and the optimalc-values ∈ {0.1, 1, 10}.",Multi Citance
55,C08-1098.xml,D13-1033.xml,"Again, we run experiments with and without syntactic features.","For Czech, we also show results from featurama8 with the feature set developed by Votrubec (2006).","For German, we show results for RFTagger (Schmid and Laws, 2008).","As expected, the information from the morphological lexicon improves the overall performance 7 Lexicons are also often used to speed up processing considerably by restricting the search space of the statistical model.","8 http://sourceforge.net/projects/featurama/ considerably compared to the results in Table 3, especially on unknown tokens.",Single Citance
56,C08-1098.xml,E09-1079.xml,The context probability of a tag is the product of the probabilities of its attributes.,The probability of an attribute given the previous tags is estimated with a decision tree.,"The decisiontree uses different context features for the predic tion of different attributes (Schmid and Laws, 2008).","“Table 2: Statistics of training and test data.”The RF tagger is well suited for lan guages with a rich morphology and a large fine grained tagset.The RF tagger was evaluated onthe German Tiger Treebank and Czech Academ ic corpus which contain 700 and 1200 POS tags, respectively.",The RF tagger achieved a higher accuracy than TnT and SVMTool.,Single Citance
57,C08-1098.xml,P10-1020.xml,We tried two methods to obtain grammatical roles.,"First, we tried extracting grammatical roles from the parse trees which we obtained from the Berkeley parser, as this information is present in the edge labels that can be recovered from the parse.","However, we found that we achieved better accuracy by using RFTagger (Schmid and Laws, 2008), which tags nouns with their morphological case.","Morphological case is distinct from grammatical role, as noun phrases can function as adjuncts in possessive constructions and preposi Table 3: Accuracy of automatic annotations of noun phrases with coreferents.",#NAME?,Single Citance
58,C08-1098.xml,P10-1068.xml,"Yet, this is language-specific restriction: Russian finite verbs, for example, show gender congruency in past tense.",Figs.2 and 4 show excerpts of category and feature hierarchies in the Reference Model.,"With respect to morphosyntactic annotations (parts of speech, pos) and morphological annotations (morph), five Annotation Models for German are currently available: STTS (Schiller et al., 1999, pos), TIGER (Brants and Hansen, 2002, morph), Morphisto (Zielinski and Simon, 2008, pos, morph), RFTagger (Schmid and Laws, 2008, pos, morph)","Further Annotation Models for pos and morph cover five different annotation schemes for English (Marcus et al., 1994; Sampson, 1995; Mandel, 2006; Kim et al., 2003, Connexor), two annotation schemes for Russian (Meyer, 2003; Sharoff et al., 2008), an annotation scheme designed for typological research and currently applied to approx.","30 different languages (Dipper et al., 2007), an annotation scheme for Old High German (Petrova et al., 2009), and an annotation scheme for Tibetan (Wagner and Zeisler, 2004).",Multi Citance
59,C08-1098.xml,P10-1068.xml,"Accordingly, the predicates that describe the token diese can be reformulated in terms of the Reference Model.","rdf:type(connexor:Pronoun) entails rdf:type(olia:Pronoun), etc. Similarly, we know that for some i:olia:Nominative it is true that olia:hasCase(i), abbreviated here as olia:hasCase(some olia:Nominative).In this way, the grammatical information conveyed in the original Connexor annotation can be represented in an annotation-independent and tagset-neutral way as shown for the Connexor analysis in (4).","Analogously, the corresponding RFTagger analysis (Schmid and Laws, 2008) given in (5) can be transformed into a description in terms of the OLiA Reference Model such as in (6).","(5) PRO.Dem.Attr.-3.Acc.Sg.Fem (RFTagger) (6) rdf:type(olia:PronounOrDeterminer) olia:hasNumber(some olia:Singular) olia:hasGender(some olia:Feminine) olia:hasCase(some olia:Accusative) rdf:type(olia:DemonstrativeDeterminer) rdf:type(olia:Determiner) For every description obtained from these (and further) analyses, an integrated and consistent generalization can be established as described in the following section.",3.1 Evaluation setup.,Single Citance
60,C08-1098.xml,P10-1068.xml,6 sketches the architecture of the evaluation environment set up for this study.5 The input to the system is a set of documents with 5 The code used for the evaluation setup is available under.,"http://multiparse.sourceforge.net.Figure 6: Evaluation setup TIGER/NEGRA-style morphosyntactic or morphological annotation (Skut et al., 1998; Brants and Hansen, 2002) whose annotations are used as gold standard.","(iii) the RFTagger that performs part of speech and morphological analysis (Schmid and Laws, 2008)","These tools annotate parts of speech, and those in (i), (iii) and (v) also provide morphological features.","All components ran in parallel threads on the same machine, with the exception of Morphisto that was addressed as a web service.",Single Citance
61,C08-1098.xml,W10-1704.xml,"In a nutshell, normalizing amounts to collapsing several German forms of a given lemma into a unique representative, using manually written normalization patterns.",A pattern typically specifies which forms of a given morphological paradigm should be considered equivalent when translating into English.,"These normalization patterns use the lemma information computed by the TreeTagger and the fine-grained POS information computed by the RFTagger (Schmid and Laws, 2008), which uses a tagset containing approximately 800 tags.",Table 1 displays the analysis of an example sentence.,"2 In most cases, normalization patterns replace a word form by its lemma; in order to partially pre 1 For the plural forms, gender distinctions are neutralized and the same 4 forms are used for all genders . 2 The English reference: Subsequently , the energized judiciary continued ruling against government decisions , embarrassing the government – especially its intelligence agencies . serve some inflection marks, we introduced two generic suffixes, +s and +en which respectively denote plural and genitive wherever needed.",Single Citance
62,C08-1098.xml,W10-1727.xml,"We utilized the factored translation framework in Moses, to enrich the baseline system with an additional target sequence model.","For English we used part-of-speech tags obtained using Tree- Tagger (Schmid, 1994), enriched with more fine- grained tags for the number of determiners, in order to target more agreement issues, since nouns already have number in the tagset.","For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs.","We used the extra factor in an additional sequence model on the target side, which can improve word order System Bleu Meteor Baseline 13.42 48.83 + morph 13.85 49.69 + comp 14.24 49.41 Table 1: Results for morphological processing, English→German System Bleu Meteor Baseline 18.34 38.13 + morph 18.39 37.86 + comp 18.50 38.47 Table 2: Results for morphological processing, German→English and agreement between words.",For German the factor was also used for compound merging.,Single Citance
63,C08-1098.xml,W11-2135.xml,"To train the target language models, we also used all provided data and monolingual corpora released by the LDC for French and English.","Moreover, all parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994).","For German, the fine-grained POS information used for pre-processing was computed by the RFTagger (Schmid and Laws, 2008).",3.1 Tokenization.,"We took advantage of our in-house text processing tools for the tokenization and detokenization steps (De´chelotte et al., 2008).",Single Citance
64,C08-1098.xml,W11-2145.xml,"2.5.4 POS Language Models In addition to surface word language models, we did experiments with language models based on part-of-speech for EnglishGerman.",We expect that having additional information in form of probabilities of part-of-speech sequences should help especially in case of the rich morphology of German and #pairs(G) Moses ∗103(s) KIT ∗103(s) 0.203 25.99 17.58 1.444 184.19 103.41 1.693 230.97 132.79 Table 2: Comparison of Moses and KIT phrase extraction systems therefore the more difficult target language generation.,"The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.",We tried n-gram lengths of 4 and 7.,"While no improvement in translation quality could be achieved using the POS language models based on the normal POS tags, the 4-gram POS language model based on fine-grained tags could improve the translation system by 0.2 BLEU points as shown in Table 3.",Single Citance
65,C08-1098.xml,W11-2147.xml,"morphology To improve target word order and agreement in the translation output, we added an extra output factor in our translation models consisting of tags with POS and morphological features.","For English we used tags that were obtained by enriching POS tags from TreeTagger (Schmid, 1994) with additional morphological features such as number for determiners.","For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs.",We trained two sequence models for each system over this output factor and added them as features in our baseline system.,The first sequence model is a 7-gram model interpolated from models of bilingual Europarl and News Commentary.,Single Citance
66,C08-1098.xml,W12-3141.xml,"As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to unknown forms).","When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar ElKahlout and Yvon, 2010)), which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds.","All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).",5.2 Bilingual corpora.,"As for last year’s evaluation, we used all the available parallel data for the GermanEnglish language pair, while only a subpart of the French-English parallel data was selected.",Multi Citance
67,C08-1098.xml,W12-3144.xml,"In this evaluation, the POS language model is applied for the EnglishGerman system.",We expect that having additional information in form of probabilities of POS sequences should help especially in case of the rich morphology of German.,"The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information.",We use a 9-gram language model on the News Shuffle corpus and the German side of all parallel corpora.,More details and discussions about the POS BLEU score of 22.31 on the test data.,Single Citance
68,C08-1098.xml,W12-3402.xml,"For the processing of this 1.7 billion word corpus, we use a pipeline that relies on deterministic dependency parsing to provide complete dependency parses at a speed that is suitable for the processing of Web-scale corpora.","The parsing model is based on MALTParser, a transition-based parser, and uses part-of-speech and morphological information as input.","Morphological information is annotated using RFTagger (Schmid and Laws, 2008), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normal trigram-based sequence tagger).","While transition- based parsers are quite fast in general, an SVM classifier (which is used in MALTParser by default) becomes slower with increasing training set.In contrast, using the MALTParser interface to LibLinear by Cassel (2009), we were able to reach a much larger speed of 55 sentences per second (against 0.4 sentences per second for a more feature-rich SVM- based model that reaches state of the art performance).For lemmatization, we use the syntax-based Tu¨ BaD/Z lemmatizer (Versley et al., 2010), which uses a separate morphological analyzer and some fallback heuristics.","The SMOR morphology (Schmid et al., 2004) serves to provide morphological analyses for novel words, covering inflection, derivation and composition processes.",Single Citance
69,C08-1098.xml,W13-2204.xml,"As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which severely impacts both training (alignment) and decoding (due to unknown forms).","When translating from German into English, the German side is thus normalized using a specific pre-processing scheme (Allauzen et al., 2010; Durgar ElKahlout and Yvon, 2010) which aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds.","All parallel corpora were POS-tagged with the TreeTagger (Schmid, 1994); in addition, for German, fine-grained POS labels were also needed for pre-processing and were obtained using the RFTagger (Schmid and Laws, 2008).","For Spanish, all the availaible data are tokenized eralization power of these rules.","Hence, rewrite using FreeLing2 toolkit (Padro´ and Stanilovsky, rules are built using POS, rather than surface word forms (Crego and Marin˜ o, 2006).",Multi Citance
70,C08-1098.xml,W13-2210.xml,"For the German↔English system, reordering rules learned from syntactic parse trees were used in addition.",4.1 POS-based Reordering Model.,"In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment.","As described in Rottmann and Vogel (2007), continuous reordering rules are extracted.","This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with non- continuous rules (Niehues and Kolss, 2009), for German↔English systems.",Single Citance
71,C08-1098.xml,W13-2210.xml,6.1 POS Language Models.,"For the English→German system, we use the POS language model, which is trained on the POS sequence of the target language.","The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German.","The RFTagger generates fine- grained tags which include person, gender, and case information.","The language model is trained with up to 9-gram information, using the German side of the parallel EPPS and NC corpus, as well as the News Shuffle corpus.",Single Citance
72,C08-1098.xml,W13-2211.xml,"We also experimented adding # as a delimiter for the splitted words except the last word (e.g., Finanzkrisen is splitted as finanz# krisen) (CS2).","On top of the compound splitting, we applied the lexical redundancy normalization (CS+Norm1).","We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the fine- grained part-of-speech tags generated by RFTagger (Schmid and Laws, 2008).","Similar to CS2, We tested the delimited version of normalized words (CS+Norm2).",Table 7 shows the results of compound splitting and normalization methods.,Single Citance
73,C08-1098.xml,W13-2228.xml,This generates a morphologically reduced Russian which is used in parallel with English for the training of the machine translation system.,Further details on the morphological processing of Russian are described in Weller et al.(2013).,"6.1.1 POS Tagging We use RFTagger (Schmid and Laws, 2008) for POS tagging.","Despite the good quality of tagging provided by RFTagger, some errors seem to be unavoidable due to the ambiguity of certain grammatical forms in Russian.","A good example of this is neuter nouns that have the same form in all cases, or feminine nouns, which have identical forms in singular genitive and plural nominative (Sharoff et al., 2008).",Single Citance
74,C08-1098.xml,W13-2230.xml,We try to overcome this problem by simplifying French inflected forms in a pre-processing step in order to adapt the French input better to the English output.,Processing of the training and test data The pre-processing of the French input consists of two steps: (1) normalizing not well-formed data (cf.table 1) and (2) morphological simplification.,"In the second step, the normalized training data is annotated with Part-of-Speech tags (PoS-tags) and word lemmas using RFTagger (Schmid and Laws, 2008) which was trained on the French tree- bank (AbeilleÂ´ et al., 2003).",French forms are then simplified according to the rules given in table 2.,Data and experiments We trained a French to English Moses system on the preprocessed and Table 3: Results of the French to English system (WMT2012).,Multi Citance
75,C08-1098.xml,W13-2230.xml,Fraser et al.(2012) applied this method to the language pair EnglishGerman with an additional special focus on word formation issues such as the splitting and merging of portmanteau prepositions and compounds.,The presented inflection prediction systems focuses on nominal inflection; verbal inflection is not addressed.,"Morphological analysis and resources The morphological analysis of the French training data is obtained using RFTagger, which is designed for annotating fine-grained morphological tags (Schmid and Laws, 2008).","For generating inflected forms based on stems and morphological features, we use an extended version of the finite-state morphology FRMOR (Zhou, 2007).","Additionally, we use a manually compiled list of abbreviations and named entities (names of countries) and their respective grammatical gender.",Single Citance
76,C08-1098.xml,W13-2230.xml,"For the WMT2013 set, we obtain BLEU scores of 29.6 (ci) and 28.30 (cs) with the inflection prediction system mes-inflection (marked in table 5).",The preparation of the Russian data includes the following stages: (1) tokenization and tagging and (2) morphological reduction.,"Tagging and tagging errors For tagging, we use a version of RFTagger (Schmid and Laws, 2008)","3 However, the large inflection-prediction system has a slightly better NIST score than the baseline (7.63 vs. 7.61).",S MT ou tp ut wit h stem ma rku p in bol d pri nt pr ed ict ed fea tur es ge ne rat ed for m s aft er post pr oce ssi ng glo ss av ert is se m en t< Ma sc >< Pl >[ N] si ni st re [A D J] d e [ P ] l e [ A R T ] p e n t a g o n e < M a s c > < S g > [ N ] s u r [ P ] de [P ] ´ r´ d e [ P ] l e [ A R T ] bu dg et <M as c> <S g> [N ] d e [ P ] l e [ A R T ] d´ Ma sc.,Single Citance
77,C08-1098.xml,W13-2302.xml,4 Part-of-speech tagging.,"While spelling normalization can be useful in itself (e.g., for search queries in the corpus), our main focus is on its usefulness for further processing of the data such as part-of-speech tagging.","The results presented here were achieved using the RFTagger (Schmid and Laws, 2008)","Text OrigP ModP NoP Berlin 85.78% 87.29% 87.07% Melk 85.21% 87.76% 87.74% LeichSermon 81.22% 80.59% 81.04% JubelFeste 90.41% 90.41% 90.03% Gottesdienst 93.24% 93.24% 92.27% Table 3: Tagging accuracy on the gold-standard normalizations (OrigP = original punctuation, ModP = modern punctuation, NoP = no punctuation) 4.1 Impact of punctuation.",Normalization tries to handle the problem of spelling inconsistencies found in historical language data.,Single Citance
78,C08-1098.xml,W13-2708.xml,These systems in turn need various kinds of preprocessing starting from tokenization over syntactic parsing up to coreference resolution.,The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists.,"So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008)",It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs.,"A common procedure in both, NLP and political sciences, is to annotate data.",Multi Citance
79,C10-1045.xml,D12-1046.xml,"Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009).","In (Jiang et al., 2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework.","Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).","A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew.","Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words.",Single Citance
80,C10-1045.xml,J13-1007.xml,"Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English.",The lattice-based parsing methodology is useful in any case where the input is uncertain.,"Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010).","Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with 123 a small treebank, and also for domain adaptation scenarios for English.","Finally, the agreement-as-filter methodology is applicable to any morphologically rich language, and although its contribution to the parsing task may be limited, it is of wide applicability to syntactic generation tasks, such as target-side-syntax machine translation in a morphologically rich language.",Multi Citance
81,C10-1045.xml,J13-1007.xml,"This is not the case in Hebrew, where many function words are not separated by white space but instead are prefixed to the next word and appear within the same token.","One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.","This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010).","As discussed in Section 2.1.2 (as well as in Tsarfaty [2006a], Goldberg and Tsarfaty [2008], and Cohen and Smith [2007]), however, the token-segmentation and syntactic-parsing tasks are closely intertwined and are better performed jointly instead of in a pipeline fashion, which is the approach we explore in this work.",2.2.3 Morphological Variation and High Out-of-Vocabulary Rate.,Pre Contiguous
82,C10-1045.xml,J13-1007.xml,"For the Hebrew segmentation task, all word segmentations of a given sentence are represented using a lattice structure.","Each lattice arc corresponds to a word and its corresponding POS tag, and a path through the lattice corresponds to a specific word- segmentation and POS tagging of the sentence.","This is by now a fairly standard representation for multiple morphological segmentations of Hebrew utterances (Adler 2001; Bar-Haim, Simaâ€™an, and Winter 2005; Adler 2007; Cohen and Smith 2007; Goldberg, Adler, and Elhadad 2008; Goldberg and Tsarfaty 2008; Goldberg and Elhadad 2011).",Figure 3 depicts the lattice for the two-words sentence םיענה םלצב.19 Double-circles indicate the space-delimited token boundaries.,Note that in this construction arcs can never cross token boundaries.,Multi Citance
83,C10-1045.xml,J13-1007.xml,We show in Section 9 that this methodology is indeed superior to the pipeline approach.,"Early descriptions of algorithms for parsing over word lattices can be found in Lang (1974, 1988) and Billott and Lang (1989).","Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010).",20 Lattice parsing for Hebrew is explored also in Cohen and Smith (2007).,"There, lattice arc weights are.",Multi Citance
84,C10-1045.xml,J13-1007.xml,"Most early work on constituency parsing of Arabic focused on straightforward adaptations of Bikel’s parser to Arabic, with little empirical success.",Attia et al.(2010) show that parsing accuracies of around 81% F1 can be achieved for Arabic (assuming gold word segmentation) by using a PCFG-LA parser with Arabic-specific unknown- word signatures.,"Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.","The work of Green and Manning also explored the use of lattice-parsing as suggested in Section 7 of this article, as well as earlier in Goldberg and Tsarfaty (2008) and Cohen and Smith (2007), and report promising results for joint segmentation and parsing of Arabic (an F1 score of 76% for sentences of up to 70 words).","The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).",Single Citance
85,C10-1045.xml,J13-1007.xml,"Recently, Green and Manning (2010) report on an extensive set of experiments with several kinds of tree annotations and refinements, and report parsing accuracies of 79% F1 using the Stanford-parser and 82% F1 using the PCFG-LA BerkeleyParser, both when assuming gold word segmentation.","The work of Green and Manning also explored the use of lattice-parsing as suggested in Section 7 of this article, as well as earlier in Goldberg and Tsarfaty (2008) and Cohen and Smith (2007), and report promising results for joint segmentation and parsing of Arabic (an F1 score of 76% for sentences of up to 70 words).","The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010).",152 Hebrew and relational-realizational parsing.,Some related work deals directly with constituency parsing of Modern Hebrew.,Single Citance
86,C10-1045.xml,J13-1008.xml,We also find that the number feature helps for Arabic.,"Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.","As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010)","Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.","Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.",Multi Citance
87,C10-1045.xml,J13-1008.xml,"Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.","As for work on Arabic (MSA), results have been reported on the PATB (Kulick, Gabbard, and Marcus 2006; Diab 2007; Green and Manning 2010), the Prague Dependency Treebank (PADT) (Buchholz and Marsi 2006; Nivre 2008) and the CATiB (Habash and Roth 2009).","Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses.","Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al. 2007), trained on the PADT.","His results are not directly comparable to ours because of the different treebank representations, even though all the experiments reported here were performed using the MaltParser.",Single Citance
88,C10-1045.xml,J13-1008.xml,Results are shown in Table 13.,8.2 Best Results on Length-Filtered Input.,"For better comparison with work of others, we adopt the suggestion made by Green and Manning (2010) to evaluate the parsing quality on sentences up to 70 tokens long.",We report these filtered results in Table 14.,Filtered results are consistently higher (as expected).,Single Citance
89,C10-1045.xml,J13-1009.xml,"Similarly, we mark MWNs that occur more than 600 times (e.g., “N P N” and “N N”) (MWNtype1 and MWNtype2).",Arabic Grammar Features.,"The Arabic grammar features come from Green and Manning (2010), which contains an ablation study similar to Table 2.","We added one additional feature, markMWEPOS, which marks POS tags dominated by MWE phrasal categories.",3.2 Head-Finding Rules.,Single Citance
90,C10-1045.xml,J13-1009.xml,"We added one additional feature, markMWEPOS, which marks POS tags dominated by MWE phrasal categories.",3.2 Head-Finding Rules.,"For Arabic, we use the head-finding rules from Green and Manning (2010).","For French, we use the head-finding rules of DybroJohansen (2004), which yielded an approximately 1% development set improvement over those of Arun (2004).",3.3 Unknown Word Models.,Single Citance
91,C10-1045.xml,J13-1009.xml,Tag Sets.,"We used the POS tag set described by Kulick, Gabbard, and Marcus (2006).",We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010).,MWE Tagging.,The ATB does not mark MWEs.,Single Citance
92,C10-1045.xml,J13-1009.xml,the morphological tagging evaluation because our parsers split punctuation deterministically.,languages.,We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010).,Morphological analysis accuracy was another experimental resource asymmetry between the two languages.,"The morphological analyses were obtained with significantly different tools: in Arabic, we had a morphological generator/ranker (MADA), whereas for French we had only a discriminative classifier (Morfette).Consequently, French analysis quality was lower (Section 5.3).",Single Citance
93,C10-1045.xml,J13-1009.xml,"The PAPCFG is the standard baseline for TSG models (Cohn, Goldwater, and Blunsom 2009).",Berkeley Parser.,We previously showed optimal Berkeley parser (Petrov et al. 2006) pa- rameterizations for both the Arabic (Green and Manning 2010) and French (Green et al. 2011) data sets,"Others had used the Berkeley parser for French, but on an older revision of the FTB.","To our knowledge, we are the first to use the Berkeley parser for MWE identification.",Multi Citance
94,C10-1045.xml,P11-1159.xml,"Looking at Hebrew, a Semitic language related to Arabic, Tsarfaty and Sima’an (2007) report that extending POS and phrase structure tags with definiteness information helps unlexicalized PCFG parsing.","As for work on Arabic, results have been reported on PATB (Kulick et al., 2006; Diab, 2007; Green and Manning, 2010), the Prague Dependency Tree- bank (PADT) (Buchholz and Marsi, 2006; Nivre, 2008) and the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009).","Recently, Green and Manning (2010) analyzed the PATB for annotation consistency","Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al., 2007), trained on the PADT.","His results are not directly comparable to ours because of the different treebanks’ representations, even though all the experiments reported here were performed using MaltParser.",Single Citance
95,C10-1045.xml,P11-2037.xml,"Finally, we train the Berkeley parser on the preprocessed training data.","Lattice parsing Unlike the training data, the test data does not mark any empty elements.","We allow the parser to produce empty elements by means of lattice-parsing (Chappelier et al., 1999), a general processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010).","Here, we use lattice parsing for empty- element recovery.",We use a modiﬁed version of the Berkeley parser which allows handling lattices as input.2 The modiﬁ- cation is fairly straightforward: Each lattice arc correspond to a lexical item.,Multi Citance
96,C10-1045.xml,P11-2122.xml,The internal consistency of the annotation in a tree- bank is crucial in order to provide reliable training and testing data for parsers and linguistic research.,"Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging.","Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010)","(Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010).","We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010).",Single Citance
97,C10-1045.xml,P11-2122.xml,"A particular etree may be reduced in one way for one nucleus, and then a different way for a different nucleus.",This is done for each etree in a derivation tree fragment.,Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB),"Their work is ideal for us, since they used the DECCA algorithm for the consistency evaluation.","They did not use the “non-fringe” heuristic, but instead manually examined a sample of 100 nuclei to determine whether they were annotation errors.",Single Citance
98,C10-1045.xml,P11-2122.xml,"Of the first 10 different types of derivation tree inconsistencies, which include 266 different nuclei, all 10 appear to real cases of annotation inconsistency, and the same seems to hold for each of the nuclei in those 10 types, although we have not checked every single nucleus.","For comparison, we chose a sample of 100 nuclei output by DECCA on this same data, and by our judgment the DECCA precision is about 74%, including 15 duplicates.","Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors.","One factor is that a system might report a variation nucleus, but still not report all the relevant instances of that nucleus.","For example, while both systems report $rm Al$yx as a sequence with inconsistent annotation, DECCA only reports the two instances that pass the “non-fringe heuristic”, while our system lists 132 instances of $rm Al$yx, partitioning them into the two derivation tree fragments.",Single Citance
99,C10-1045.xml,P11-2124.xml,"They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies.",Goldberg et al.(2009) showed that segmentation and parsing ac- curacies can be further improved by extending the lexical coverage of a lattice-parser using an external resource.,"Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic.","Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser (Petrov et al., 2006).","704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709, Portland, Oregon, June 1924, 2011.",Single Citance
100,C10-1045.xml,P12-1016.xml,An implementation issue may account for the decoding slowdown.,"(p.c.) 7 LDC catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).","The data was pre-processed with packages from the Stanford Arabic parser (Green and Manning, 2010).",The corpus split is available at http://nlp.stanford.edu/projects/arabic.shtml.,8 We ignore orthographic re-normalization performed by the annotators.,Single Citance
101,C10-1045.xml,P12-2002.xml,Morphologically complex words may be highly ambiguous and in order to segment them correctly their analysis has to be disambiguated.,The multiple morphological analyses of input words may be represented via a lattice that encodes the different segmentation possibilities of the entire word sequence.,"One can either select a segmentation path prior to parsing, or, as has been recently argued, one can let the parser pick a segmentation jointly with decoding (Tsarfaty, 2006; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).","If the selected segmentation is different from the gold segmentation, the gold and parse trees are rendered incomparable and standard evaluation metrics break down.","Evaluation scenarios restricted to gold input are often used to bypass this problem, but, as shall be seen shortly, they present an overly optimistic upper- bound on parser performance.",Multi Citance
102,C10-1045.xml,P12-2002.xml,"The word BCLM, for instance, can be segmented into the noun BCL (“onion”) and M (a genitive suffix, “of them”), or into the prefix B (“in”) followed by the noun CLM (“image”).2 The multitude of morphological analyses may be encoded in a lattice structure, as illustrated in Figure 2.",1 We use the Hebrew transliteration in Sima’an et al.(2001)..,2 The complete set of analyses for this word is provided in Goldberg and Tsarfaty (2008).,Figure 2: The morphological segmentation possibilities of BCLM HNEIM.,Double-circles are word boundaries.,Single Citance
103,C10-1045.xml,P12-2002.xml,Double-circles are word boundaries.,"In practice, a statistical component is required to decide on the correct morphological segmentation, that is, to pick out the correct path through the lattice.","This may be done based on linear local context (Adler and Elhadad, 2006; Shacham and Wintner, 2007; Bar-haim et al., 2008; Habash and Rambow, 2005), or jointly with parsing (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008; Green and Manning, 2010).","Either way, an incorrect morphological segmentation hypothesis introduces errors into the parse hypothesis, ultimately providing a parse tree which spans a different yield than the gold terminals.","In such cases, existing evaluation metrics break down.",Pre Contiguous
104,C10-1045.xml,W13-4904.xml,"Since we want a large annotated corpus with fine-grained labels, we create our own ATB conversion.","4.5.2 Transformations In addition to converting the ATB’s constituent parses to dependency trees, we make a handful of other changes.","Following Green and Manning (2010) and others, sentences headed by X nodes are deleted","Following Rambow et al.(2005), Treebank sentences headed by TOP elements containing multiple S daughters are split into separate sentences.8 Additionally, if the dependency converter concludes that an S node without treebank functional tags is dependent upon another S node and is separated from it via sentence-final punctuation (e.g., an exclamation point), these S nodes are separated into distinct sentences as well.","For the broadcast news data, we remove all subtrees headed by EDITED tags to make it more closely resemble newswire text.9 Since we adhere to the tokenization scheme used by the ATB, and we do not split off the determiner Al as its own tree token.",Single Citance
105,C10-1045.xml,W13-4904.xml,"For example, to parse Modern Hebrew, Cohen and Smith (2007) combine a morphological model with a syntactic model using a product of experts.","Another alternative is lattice parsing, which can be used to jointly model both tokenization and parsing (Chappelier et al., 1999).","Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1).",Why lattice parsing may help in some cases but not others is not clear.,Some Arabic parsing work focuses on the usefulness of various features and part-of-speech tagsets.,Single Citance
106,C10-1045.xml,W13-4904.xml,"There has been a flurry of recent research involving the joint modeling of dependency parsing and lower-level tasks14 for a variety of languages, with most of the attention focused on Chinese.","While lacking Arabic’s morphological richness, Chinese has its own challenges, such as word segmentation and part-of-speech ambiguities, which have led researchers to develop new unified approaches for processing it.","Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work.",14 Systems that jointly model POS tagging and constituent.,parsing have existed for some time.,Single Citance
107,C10-1045.xml,W13-4917.xml,"To obtain these dependency trees, we used the constituent-to-dependency tool (Habash and Roth, 2009).","Additional CATiB trees were annotated directly, but we only use the portions that are converted from phrase-structure representation, to ensure that the constituent and dependency yields can be aligned.","The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010)","At the phrasal level, we collapse unary chains with identical categories like NP → NP.","We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.",Single Citance
108,C10-1045.xml,W13-4917.xml,"The Stanford Arabic Phrase Structure Treebank In order to stay compatible with the state of the art, we provide the constituency data set with most of the pre-processing steps of Green and Manning (2010), as they were shown to improve baseline performance on the PATB parsing considerably.12 To convert the original PATB to preprocessed phrase-structure trees á la Stanford, we first discard all trees dominated by X, which indicates errors and non-linguistic text.","At the phrasal level, we collapse unary chains with identical categories like NP → NP.","We finally remove all traces, but, unlike Green and Manning (2010), we keep all function tags.","In the original Stanford instance, the pre-terminal morphological analyses were mapped to the shortened Bies tag set provided with the treebank (where Determiner markers, “DT”, were added to definite noun and adjectives, resulting in 32 POS tags).","Here we use the Kulick tagset (Kulick et al., 2006) for 12 Both the corpus split and pre-processing code are available with the Stanford parser at http://nlp.stanford.edu/ projects/arabic.shtml.",Single Citance
109,C10-1045.xml,W13-4917.xml,"It exhibits a high degree of morphological ambiguity due to the absence of the diacritics and inconsistent spelling of letters, such as Alif and Ya.","As a consequence, the Buckwalter Standard Arabic Morphological Analyzer (Buckwalter, 2004; Graff et al., 2009) produces an average of 12 analyses per word.","Data Sets The Arabic data set contains two tree- banks derived from the LDC Penn Arabic Treebanks (PATB) (Maamouri et al., 2004b):11 the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009), a dependency treebank, and the Stanford version of the PATB (Green and Manning, 2010)",We preprocessed the treebanks to obtain strict token matching between the treebanks and the morphological analyses.,"This required nontrivial synchronization at the tree token level between the PATB treebank, the CATiB treebank and the morphologically predicted data, using the PATB source tokens and CATiB feature word form as a dual synchronized pivot.",Multi Citance
110,C90-2039.xml,P99-1061.xml,"First, it performs a destructive (but reversible) check that the two structures are compatible, and only when that succeeds does it produce an output structure.","Thus, no output structures are built until it is certain that the unification will ultimately succeed.","While an improvement over simple destructive unification, Tomabechi&apos;s approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.","The new feature structures produced in the second phase of unification include copies of all the substructures of the input graphs, even when these structures are unchanged.","This can be avoided by reusing parts of the input structures in the output structure (Carroll and Malouf, 1999) without introducing significant book keeping overhead.",Single Citance
111,C90-2039.xml,C90-3046.xml,"""membe,'([a ~-- b 7, i,j, a, el, Cache), member( [b ,-, j, k, fl, f ], Chart), unify(e, [b: f], g)Is,~ ~ ~t~ rnember([a +-- 7, i, k, ctb,g], Agenda) Isn+, where e,f and g are feature structures, and .unify(x, y, z) means that z is the result of unifying x and y. Feature structures uniformly represent various lingtlistic constraints such as subcategorizations, gaps, unbounded dependencies, and logical forms.",A problem of this representation scheme is that it describes all possible constraints in one structure and deals with them at once.,This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].,"It seems that a preferable approach is to treat linguistic constraints piece wise, taking into consideration abductivity of parsing, uniform integration of various linguistic proc~ssings, and the problem of a unificat.ion-based approach.","From this point of view, we describe such treatments as, especially, incorporation of word properties, case analyses, composition of logical forms, and interpretMon of noun phrases with adnominal particles.",Single Citance
112,C98-1097.xml,C02-1033.xml,"The work of Bigi (Bigi et al., 1998) stands in the same perspective but focuses on much larger topics than TDT.",These systems have a limited scope due to their topic representations but they are also more precise for the same reason.,"Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, collocations and a thesaurus; (Beeferman et al., 1999) relied on both collocations and linguistic cues.","The topic analysis we propose implements such a hybrid approach: it relies on a general language resource, a collocation network, but exploits it together with word recurrence in texts.","Moreover, it simultaneously achieves topic segmentation and link detection, i.e. determining whether two segments discuss the same topic.",Multi Citance
113,C98-1097.xml,C02-1033.xml,"Furthermore, it shows that TOPICOLL gets better results than a system that only relies on a collocation network such as SEGCOHLEX.","It also gets better results than a system such as TextTiling that is based on word recurrence and as TOPICOLL, works with a local context.","Thus, Table 1 confirms the fact reported in (Jobbins and Evett, 1998) that using collocations together with word recurrence is an interesting approach for text segmentation.","Moreover, TOPICOLL is more accurate than a system such as SEGAPSITH that depends on topic rep resentations.","Its accuracy is also slightly higher than the one reported in (Bigi et al., 1998) for a system that uses topic representations in a probabilistic way: 0.75 as precision, 0.80 as recall and 0.77 as f1-measure got on a corpus made of Le Monde’s articles too.",Single Citance
114,C98-1097.xml,P07-1061.xml,"Such topic models are generally built from a large set of example documents as in (Yam- ron et al., 1998), (Blei and Moreno, 2001) or in one component of (Beeferman et al., 1999).",These statistical topic models enable segmenters to improve their precision but they also restrict their scope.,"Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Job- bins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues.",The work we report in this article takes place in the first category we have presented.,It does not rely on any a priori knowledge and exploits word usage rather than discourse cues.,Multi Citance
115,C98-1097.xml,P07-1061.xml,Segments are finally delimited by locating the areas where the similarity between units or groups of units is weak.,This quick overview highlights the important role of the evaluation of the similarity between discourse units in the segmentation process.,"When no external knowledge is used, this similarity is only based on the strict reiteration of words.",But it can be enhanced by taking into account semantic relations between words.,"This was done for instance in (Jobbins and Evett, 1998) by taking semantic relations from Rogets Thesaurus.",Post Contiguous
116,C98-1097.xml,P07-1061.xml,The evaluation of the lexical cohesion of a text relies as for TextTiling on a fixed-size focus window that is moved over the text to segment and stops at each sentence break.,The cohesion in the part of text delimited by this window is evaluated by measuring the word reiteration between its two sides.,"This is done in our case by applying the Dice coefficient between the two sides of the focus window, following (Jobbins and Evett, 1998).",This cohesion value is associated to the sentence break at the transition between the two sides of the window.,"More precisely, if Wl refers to the vocabulary of the left side of the focus window and Wr refers to the vocabulary of its right side, the cohesion in the window at position x is given by: LC (x) = 2 · card(Wl ∩ Wr ) (1) rec card(W ) + card(W ) The next step is done by removing as a possible topic shift each minimum that is not farther than 2 sentences from its preceding neighbor.",Pre Contiguous
117,C98-1097.xml,P07-1061.xml,"The heart of the algorithm we have presented above is the evaluation of lexical cohesion in the focus window, as given by Equation 1.",This evaluation is also a weak point as card(Wl  Wr ) only relies on word reiteration.,"As a consequence, two different words that respectively belongs to Wl and Wr but also belong to the same text topic cannot contribute l r to the identification of a possible topical similarity This measure was adopted instead of the Cosine measure used in TextTiling because its definition in terms of sets makes it easier to extend for taking into account other types of relations, as in (Jobbins and Evett, 1998).",A cohesion value is computed for each sentence break of the text to segment and the final result is a cohesion graph of the text.,"The last part of our algorithm is mainly taken from the LCseg system (Galley et al., 2003) and is divided into three steps: • computation of a score evaluating the probability of each minimum of the cohesion graph to be a topic shift; • removal of segments with a too small size; • selection of topic shifts.",Pre Contiguous
118,C98-1097.xml,P07-1061.xml,"In the work of Ponte and Croft (Ponte and Croft, 1997), the representations of sentences are expanded by adding to them words selected from an external corpus by the means of the Local Context Analysis (LCA) method.","Finally in (Caillet et al., 2004), a set of concepts are learnt from a corpus in an unsupervised way by using the X-means clustering algorithm and the paragraphs of documents are represented in the space defined by these concepts.","In fact, the way we use relations between words is closer to (Jobbins and Evett, 1998), even if the relations in this work come from a network of co-occurrences or a thesaurus rather than from text topics.","More globally, our work exploits the topics of a text for its segmentation.","This kind of approach was also explored in (Blei and Moreno, 2001) where probabilistic topic models were built in an unsupervised way.",Single Citance
119,C98-1097.xml,P07-1061.xml,"Hence, we plan to use a network of lexical co-occurrences, which is a source of knowledge that is easy to build automatically from a large corpus.","More precisely, we intend to extend our method for discovering text topics by combining the co-occurrence graph of a document with such a network.","This network could also be used more directly for topic segmentation as in (Job- bins and Evett, 1998).",$,$,Single Citance
120,C98-1097.xml,P06128.xml,"In other words, the task of linear text segmentation is to identify topic boundaries within a long text.","Linear text segmentation algorithms are widely used as an essential step in many natural language processing tasks, such as information retrieval and document summarization.","In information retrieval, to segment a long document into distinct topics is useful because only the topical segments relevant to the users needs are retrieved [1].","It not only provides more accurate information to the user, but also reduces the user’s burden to read the whole document.","In document summarization, a long document is often divided into topics and then each topic is summarized independently [2].",Single Citance
121,C98-1097.xml,S0885.xml,"Using semantic relations Many topic segmentation techniques, based on the lexical cohesion criterion, use semantic relations as additional information to take into account the semantic links that exist between words.","Indeed, the primary goal of semantic relations is obviously to ensure that two semantically related words, e.g., car and drive, contribute to the lexical cohesion, thus avoiding erroneous topic boundaries between two such words.","These different methods can use semantic relations that are manually defined by experts, as in Morris and Hirst (1991), or extracted automatically from corpora (Ferret, 2006; Jobbins and Evett, 1998).","For example, Ferret (2006) uses a network of lexical co-occurrences built from a large corpus to improve a topic segmenter based on lexical reiteration.","The algorithm in Jobbins and Evett (1998) compares adjacent windows of sentences and determines their lexical similarity, based on repetitions of words and collocations, to detect topic boundaries.",Pre Contiguous
122,D09-1023.xml,D09-1086.xml,divergences and some instances of sloppy (non- parallel or inexact) translation.,"Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a).","In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).",All the models in this paper are conditioned on the source tree tl.,"Conditionally-trained models of adaptation and projection also condition on the target string w and its alignment a to wl and thus have the form p(t | w, wl, tl, a); the unsupervised, generative projection models in §5 have the form p(w, t, a | wl, tl).",Multi Citance
123,D09-1023.xml,D11-1044.xml,"We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering.","To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding.","Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009).","The decoder involves generating a phrase lattice (Ueffing et al., 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice.","This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees.",Multi Citance
124,D09-1023.xml,D11-1044.xml,"We also discuss how our model improves translation quality and discuss future possibilities for combining approaches to ma 474 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 474–485, Edinburgh, Scotland, UK, July 27–31, 2011.",Qc 2011 Association for Computational Linguistics chine translation using our framework.,"We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level.","Here we generalize that model to function on phrases, enabling a tighter coupling be","Given a sentence s and its dependency tree τs, we formulate the translation problem as finding the target sentence t∗, the segmentation γ∗ of s into phrases, the segmentation φ∗ of t∗ into phrases, the dependency tree τ ∗ on the target phrases φ∗, and the one-to-one phrase alignment a∗ such that tween the phrase segmentation and syntactic structures.",Single Citance
125,D09-1023.xml,D11-1044.xml,"In modeling p(t, γ, φ, τφ, a | s, τs), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006).","Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree.","We denote this grammar by Gs,s ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009).",We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar.,"However, it is well-known in the MT community that translation quality is improved when larger units are modeled.",Multi Citance
126,D09-1023.xml,D11-1044.xml,"There are 15 features in this category, for a total of 23 QPDG features.",5 Decoding.,"For a QPDG model, decoding consists of finding the highest-scoring tuple (t, , , , a) for an in put sentence s and its parse s, i.e., finding the most probable derivation under the s/s-specific grammar Gs,s . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,s and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-to- fine strategy (Petrov, 2009) to speed up decoding.","It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008).","Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple (t, γ, φ, a) for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming.",Single Citance
127,D09-1023.xml,Pjournal.xml,"In the third column, ‘SL’ and ‘TL’ respectively stand for ‘source’ and ‘target’ languages of the translation pair from which the listed contextual features are extracted.",Table 4 Related research integrating context into alternative SMT models Authors SL→TL[DS][S/L] Contextual features Integrated into Bangalore et al.(2007) Ar→En[UN][L] SL:bag-of-words FST-based MT model Fr→En[CPH][L] Zh→En[IWSLT][S] Ittycheriah et al.,"(2007) ArEn[UN, NIST 06][L] SL:lexical, morphological and syntactic features Gimpel and Smith (2009) DeEn[BTEC][S] SL and TL:syntactic features from dependency trees Proposed DTM2 model Proposed MT model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, UN United Nations, BTEC basic travel expression corpus, FST finite state transducer Table 5 Related research integrating context into word alignment models Authors SLTL[DS][S/L] Contextual features Integrated into Berger et al.","(1996) Fr→En[CPH][L] TL:neighbouring words IBM model De→En[Verbmobil][L] García-Varea et al.(2001, 2002) De→En[Verbmobil][S] SL and TL:neighbouring words and word class IBM model Fr→En[CPH][L] Mauser et al.(2009) Zh→En[GALE][L] TL:neighbouring words and SL:sentence level lexical feature IBM model and proposed discriminative WA model Ar→En[NIST 08][L] Zh→En[NIST 08][L] Patry and Langlais (2009) Fr→En[Europarl][S] SL:bag-of-words Proposed WA model En English, Fr French, De German, Zh Chinese, Ar Arabic, CPH Canadian Parliament Hansards, WA word alignment 3.1 Source context modelling.","Approaches to integrating source-language contextual information into different stages in the SMT model can in turn be broadly divided into: (i) discriminative word alignment (e.g. Brunning et al. 2009; Patry and Langlais 2009) for creating improved word-to-word translation lexicons, and (ii) discriminative translation filtering (e.g. Carpuat and Wu 2007; Chan et al. 2007; Stroppa et al. 2007) by learning context- dependent translation probabilities.",Multi Citance
128,D09-1023.xml,Pjournal.xml,Bangalore et al.(2007) propose an SMT architecture based on stochastic finite state transducers that addresses global lexical selection in which parameters are discriminatively trained using a MaxEnt model considering n-gram features from the source sentence.,Ittycheriah et al.(2007) introduce the Direct Translation Model 2 (DTM2) which employs discriminative MaxEnt models to obtain the translation likelihoods.,Gimpel and Smith (2009) present an MT framework based on lattice parsing with a quasi-synchronous grammar that can incorporate arbitrary features from both source and target sentences (Table 5).,3.2 Target context modelling.,Table 6 enumerates related research that integrates context into word alignment models.,Single Citance
129,D09-1023.xml,Pproc_d09.xml,Galley & Manning (2009) use a de­ pendency parser in a phrase-based setup for assign­ ing a dependency structure to the target side during translation.,lbis allows for the integration of a de­ pendency language model directly into the system.,"Gimpel &amp; Smith (2009; 2011) treat translation as a monolingual dependency parsing problem, creating a dependency structure over the translation during decoding.",No syntactic structure is created during decoding in our approach.,Instead the dependency parser is used for the sole purpose of scoring the word order of the target sentence.,Single Citance
130,D09-1023.xml,W10-1730-parscit.xml,"1 � p(y|x) = Z(x)exp Aifi(x, y) i 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, Uppsala, Sweden, 1516 July 2010.","c�2010 Association for Computational Linguistics A deeper discussion on the potential advantagesof maximum entropy approach over the noisy channel approach can be found in (Foster, 2000)and (Och and Ney, 2002), in which another suc cessful applications of maxent translation models are shown.","Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel andSmith, 2009); the idea can be traced back to (Pap ineni et al., 1997).",What makes our approach different from the previously published works is that1.,"we show how the maximum entropy trans lation model can be used in a dependencyframework; we use deep-syntactic dependency trees (as defined in the Prague Depen dency Treebank (Hajiˇc et al., 2006)) as the transfer layer,2.",Multi Citance
131,D09-1023.xml,W11-2139.xml,Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor.,"For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do.","On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008).",Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar.Like the “soft syntactic features” used in pre 2 Removing long segments substantially reduces training time and does not appear to negatively affect performance.,"vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation.",Multi Citance
132,D09-1023.xml,N10-1040.xml,"Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations.","Recently, there have been a number of successful attempts at improving phrase-based statistical machine translation by exploiting linguistic knowledge such as morphology, part-of-speech tags, and syntax.","Many translation models use such knowledge before decoding (Xia and McCord, 2004) and during decoding (Birch et al., 2007; Gimpel and Smith, 2009; Koehn and Hoang, 2007; Chiang et al., 2009), but they are limited to simpler features for practical reasons, often restricted to conditioning left-to- right on the target sentence.","Traditionally, n-best rerankers (Shen et al., 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al., 2007).","We argue that it can be desirable to pre-translate parts of the source text before sentence-level decoding begins, using a richer model that would typically be out of reach during sentence-level decoding.",Multi Citance
133,D10-1058.xml,C16-1060.xml,"This is modeled using a distribution P (φi = k|si = e), where the fertility φi at position i is conditioned on the word e at that position.",This is particularly important when the languages have large differences in word formation strategies and the general level of morphological complexity.,"Zhao and Gildea (2010) explored a model with a word order and fertility model as described above, but based their work on the EM algorithm, using Gibbs sampling only for approximating the expectations.","An important conclusion from their work is that a simple HMM with fertility model is competitive with the more complex IBM model 4, and we follow them in using this model as our baseline.","Our full baseline model is given by P s, t, a,θ, ψ, π, α, β, γ)  K J (k)   E F  TI TI ∝  k=1 j=1 θs(k) a(k) j ,t(k)  ·  TI TI e=1 f =1 αf −1 e,f   K J (k) +1 TI TI   Imax TI mmax TI  βI ,m −1 (4) ·  ψa(k) (k)  ·  ψm  k=1 j=1 j −aj−1 I =Imin m=mmin  K I (k)  E nmax \ TI TI ·  k=1 i=1 i ,φi  TI TI e=1 n=0 γn −1 e,n where K is the number of parallel sentences, θ ∼ Dir α) are the lexical translation parameters, ψ ∼ Dir β) are the categorical distribution parameters for the word order model P aj − aj−1 = m|I ), and πe ∼ Dir γ) for the fertility model P (φi = k|si = e).2.4 PoS-guided word alignment.",Single Citance
134,D10-1058.xml,P13-2002.xml,"Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space.","Therefore, the standard forward-backward and Viterbi algorithms do not apply.","Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation.","However, they do not have a efficient means of MAP inference, which is necessary in many applications such as machine translation.",This paper introduces a method for exact MAP inference with the fertility HMM using dual decomposition.,Single Citance
135,D10-1058.xml,P13-2002.xml,"We are given a sequence of English words e = e1, . . .",", eI . This model produces distributions over French word sequences f = f1, . . .",", fJ and word alignment vectors a = estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gildea, 2010).","In this case, we make some initial estimate of the a vector, potentially randomly.","We then repeatedly re- sample each element of that vector conditioned on all other positions according to the distribua1, . . .",Single Citance
136,D10-1058.xml,P13-2002.xml,"Then for each French word position, first the alignment variable (English word index used to generate the current French word) is selected based on only the prior alignment variable.",Next the French word is predicted based on its aligned English word.,"Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution.","I Pr(f , a|e) =p(J |I ) n p(φi|ei) 2.2 MAP inference with dual decomposition.","Dual decomposition, also known as Lagrangian relaxation, is a method for solving complex combinatorial optimization problems (Rush and Collins, 2012).",Single Citance
137,D10-1058.xml,P13-2002.xml,"Original IBM models used a categorical distribution of fertility, one such distribution for each English word.","This gives EM a great amount of freedom in parameter estimation, with no smoothing or parameter tying of even rare words.","Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).",We explore instead a feature-rich approach to address this issue.,"Prior work has explored feature-rich approaches to modeling the translation distribution (Berg-Kirkpatrick et al., 2010); we use the same technique, but only for the fertility model.",Single Citance
138,D10-1058.xml,P13-2002.xml,The next line shows the fertility HMM with approximate posterior computation from Gibbs sampling but with final alignment selected by the Viterbi algorithm.,Clearly fertility modeling is improving alignment quality.,"The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).","Here, however, the difference between a dual decomposition and Viterbi is significant: their results were likely due to search error.",We have introduced a dual decomposition approach to alignment inference that substantially reduces alignment error.,Single Citance
139,D10-1058.xml,P59105ca.xml,Vaswani 1Part of this work was presented at a conference [12].,15587916/$31.00 © 2013 IEEE et al. [14] encourage sparsity in the translation model by placing an prior on the parameters and then optimize for the MAP objective.,"Zhao and Gildea [15] use sampling in their proposed fertility extensions to IBM Model 1 and HMM, but they do not place any prior on the parameters.","Their inference method is stochastic EM (also known as Monte Carlo EM), a maximum-likelihood technique in which sampling is used to approximate the expected counts in the E-step.","Even though they report substantial reductions in the alignment error rate, the translation performance measured in BLEU does not improve.",Single Citance
140,D10-1058.xml,Pcoling_D10.xml,their model to preferably align positions which are close to each other.,"Using standard results for series, they manage to make the posterior computations in their model extremely fast.",Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM.,"This made posterior computations in their model intractable, however, they avoided the use of heuristics and instead approximated the posterior using MCMC.",The idea of using Bayesian inference together with a Gibbs sampler for word alignment was first presented for IBM model 1 by Mermer and Sarac¸lar (2011).,Single Citance
141,D10-1058.xml,Pproc_D10.xml,"Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space.","Therefore, the standard forward-backward and Viterbi algorithms do not apply.","Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter es timation.","However, they do not have a efficient means of MAP inference, which is necessary in many applications such as machine translation.",This paper introduces a method for exact MAP inference with the fertility HMM using dual de­ composition.,Single Citance
142,D10-1058.xml,Pproc_D10.xml,"Then for each French word position, first the alignment variable (English word index used to generate the current French word) is selected based on only the prior alignment variable.",Next the French word is predicted based on its aligned English word.,"Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis tribution.","I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam­ pling (Zhao and Gildea, 2010).","In this case, we make some initial estimate of the a vector, potentially randomly.",Single Citance
143,D10-1058.xml,Pproc_D10.xml,Next the French word is predicted based on its aligned English word.,"Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility dis­ tribution.","I Pr(f, ale) =p(JII) ITP(cPilei) estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sam pling (Zhao and Gildea, 2010).","In this case, we make some initial estimate of the a vector, potentially randomly.","We then repeatedly re­ sample each element of that vector conditioned on all other positions according to the distribu­ tion Pr(aj la-j, e, f).",Single Citance
144,D10-1058.xml,Pproc_D10.xml,"Original IBM models used a categorical distribu­ tion of fertility, one such distribution for each En­ glish word.","This gives EM a great amount of free­ dom in parameter estimation, with no smoothing or parameter tying of even rare words.","Prior work addressed this by using the single parameter Pois son distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010).",We explore instead a feature-rich approach to address this issue.,"Prior work has explored feature-rich approaches to modeling the transla­ tion distribution (Berg-Kirkpatrick et al., 2010); we use the same technique, but only for the fertil­ ity model.",Single Citance
145,D10-1058.xml,Pproc_D10.xml,The next line shows the fer­ tility HMM with approximate posterior computa­ tion from Gibbs sampling but with final alignment selected by the Viterbi algorithm.,Clearly fertil­ ity modeling is improving alignment quality.,"The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010).","Here, however, the difference be­ tween a dual decomposition and Viterbi is signifi­ cant: their results were likely due to search error.",We have introduced a dual decomposition ap­ proach to alignment inference that substantially reduces alignment error.,Single Citance
146,D10-1058.xml,Q13-1024.xml,"with Dependency Cohesion Constraint The most influential generative word alignment models are the IBM Models 15 and the HMM model (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003).","These models can be classified into sequence-based models (IBM Models 1, 2 and HMM) and fertility-based models (IBM Models 3, 4 and 5).","The sequence-based model is easier to implement, and recent experiments have shown that appropriately modified sequence-based model can produce comparable performance with fertility-based models (Lopez and Resnik, 2005; Liang et al., 2006; DeNero and Klein, 2007; Zhao and Gildea, 2010; Bansal et al., 2011).",So we built a generative word alignment model with dependency cohesion constraint based on the sequence-based model.,3.1 The Sequence-based Alignment Model.,Multi Citance
147,D10-1058.xml,Q13-1024.xml,"However, their sampler updates parameters constantly and thus cannot run efficiently on large-scale tasks.","Instead, we take advantage of explicit Gibbs sampling to make a highly parallelizable sampler.","Our Gibbs sampler is similar to the MCMC algorithm in Zhao and Gildea (2010), but we assume Dirichlet priors when sampling model parameters and take a different sampling approach based on the source side dependency tree.",Our sampler performs a sequence of consecutive iterations.,Each iteration consists of two sampling steps.,Single Citance
148,D10-1083.xml,D11-1056.xml,"Specifically we evaluated the performance of a model with precision, recall and the F-score, all of which were based on tokens.",We report the score of the most frequent segmentation among 10 samples.,"Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values.","In order to evaluate the degree of difference between a pair of segmentations, we employed character-based evaluation.","Following Kudo et al.(2004), we converted a word sequence into character-based BI labels and examined labeling disagreements.",Single Citance
149,D10-1083.xml,D11-1059.xml,"We pursue this strategy here, developing a system based on Bayesian methods where the probabilistic model incorporates several insights from previous work.","Perhaps the most important property of our model is that it is type-based, meaning that all tokens of a given word type are assigned to the same cluster.","This property is not strictly true of linguistic data, but is a good approximation: as Lee et al.(2010) note, assigning each word type to its most frequent part of speech yields an upper bound accuracy of 93% or more for most languages.","Since this is much better than the performance of current unsupervised syntactic class induction systems, constraining the model in this way seems likely to improve performance by reducing the number of parameters in the model and incorporating useful linguistic knowledge.","Both of the older systems discussed by Christodoulopoulos et al.(2010), i.e., Clark (2003) and Brown et al.(1992), included this constraint and achieved very good performance relative to token-based systems.",Single Citance
150,D10-1083.xml,D11-1059.xml,"Since this is much better than the performance of current unsupervised syntactic class induction systems, constraining the model in this way seems likely to improve performance by reducing the number of parameters in the model and incorporating useful linguistic knowledge.","Both of the older systems discussed by Christodoulopoulos et al.(2010), i.e., Clark (2003) and Brown et al.(1992), included this constraint and achieved very good performance relative to token-based systems.","More recently, Lee et al.(2010) presented a new type-based model, and also reported very good results.","A second property of our model, which distinguishes it from the type-based Bayesian model of Lee et al.(2010), is that the underlying probabilistic model is a clustering model, (specifically, a multinomial mixture model) rather than a sequence model (HMM).","In this sense, our model is more closely re 638 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 638–647, Edinburgh, Scotland, UK, July 27–31, 2011.",Single Citance
151,D10-1083.xml,D11-1059.xml,"Previous work suggests that using parallel text can improve performance on various unsupervised NLP tasks (Naseem et al., 2009; Snyder and Barzilay, 2008).",We evaluate our model on 25 corpora in 20 languages that vary substantially in both syntax and morphology.,"As in previous work (Lee et al., 2010), we find that the one-class-per-type restriction boosts performance considerably over a comparable token- based model and yields results that are comparable to state-of-the-art even without the use of morphology or alignment features.",Including morphology features yields the best published results on 14 or 15 of our 25 corpora (depending on the measure) and alignment features can improve results further.,Our model is a multinomial mixture model with Bayesian priors over the mixing weights θ and α θ z β φ f Z nj M Figure 1: Plate diagram of the basic model with a single feature per token (the observed variable f ).,Single Citance
152,D10-1083.xml,D11-1059.xml,These features are extracted for language ℓ by word aligning ℓ to another language ℓ′ (details of the alignment procedure are described in Section 3.1).,"The features used for each token e in ℓ are the left and right context words of the word token that is aligned to e (if there is one).As with the monolingual context features, we use only the F most fre quent words in ℓ′ as possible features.",2 One could approximate this likelihood term by assuming independence between all nj feature tokens of word type j. This is the approach taken by Lee et al.(2010).,"3 We use the word kind here to avoid confusion with type, which we reserve for the type-token distinction, which can apply to features as well as words.","greater effect for infrequent words (as appropriate, since there is less evidence from context and alignments).",Single Citance
153,D10-1083.xml,D11-1059.xml,"For further testing, we used the remaining MUL- TEXT languages, as well as the languages of the CONNL-X (Buchholz and Marsi, 2006) shared task.","This dataset contains 13 languages, 4 of which are freely available (Danish, Dutch, Portuguese and Swedish) and 9 that are used with permission from the creators of the corpora ( Arabic7, Bulgarian8, Czech9, German10, Chinese11, Japanese12, Slovene13, Spanish14, Turkish15 ).",Following Lee et al.(2010) we used only the training sections for each language.,"Finally, to widen the scope of our system, we generated two more corpora in French16 and Ancient Greek17, extracting the gold standard parts of speech from the respective dependency treebanks.",3.3 Baselines.,Single Citance
154,D10-1083.xml,D12-1086.xml,Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many- to-one accuracy.,"However Christodoulopoulos et al.(2010) show that the older one-tag-per-word models such as (Brown et al., 1992) outperform the more sophisticated sparse prior and posterior regularization methods both in speed and accuracy (the Brown model gets .68 many-to-one accuracy with a 1M word corpus).","Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010)",2.2 Word-feature models.,One problem with the algorithms in the previous section is the poverty of their input features.,Single Citance
155,D10-1083.xml,D12-1125.xml,The first column gives the ID of the feature as listed in WALS.,The second column describes the feature and the last column enumerates the allowable values for each feature; besides these values each feature can also have a value of ‘No dominant order’.,"vised POS induction algorithm (Lee et al., 2010)","For initialization, we perform multiple random restarts and select the one with the lowest final objective score.",We first present the results of our model using the gold POS tags for the target language.,Single Citance
156,D10-1083.xml,D12-1127.xml,"Furthermore, much of the annotated text is of limited genre, normally focusing on newswire or literary text.","Performance of treebank-trained systems degrades significantly when applied to new domains (Blitzer et al., 2006).","Unsupervised induction of POS taggers offers the possibility of avoiding costly annotation, but despite recent progress, the accuracy of unsupervised POS taggers still falls far behind supervised systems, and is not suitable for most applications (Berg- Kirkpatrick et al., 2010; GracÂ¸a et al., 2011; Lee et al., 2010).","Using additional information, in the form of tag dictionaries or parallel text, seems unavoidable at present.","Early work on using tag dictionaries used a labeled corpus to extract all allowed word-tag pairs (Merialdo, 1994), which is quite an unrealistic scenario.",Multi Citance
157,D10-1083.xml,D13-1004.xml,"(See Hammarstro¨ m and Borin (2011) for an overview of unsupervised morphology learning, likewise Christodoulopoulos et al.(2010) for a comparison of part of speech/syntactic category induction systems.)","However, given the tightly coupled nature of these two tasks, there has been surprisingly little work in joint learning of morphology and syntactic categories.","Systems for inducing syntactic categories often make use of morpheme-like features, such as word-final characters (Smith and Eisner, 2005; Haghighi and Klein, 2006; Berg-Kirkpatrick et al., 2010; Lee et al., 2010)","Other systems (Dasgupta and Ng, 2007; Christodoulopoulos et al., 2011) use morphological segmentations learned by a separate morphology model as features in a pipeline approach.","Models of morphology induction generally operate over a lexicon, i.e. a list of word types, rather than token corpora (Goldsmith, 2006; Creutz and Lagus, 2007; Kurimo et al., 2010).",Multi Citance
158,D10-1083.xml,N12-1045.xml,"Here we are interested in learning concatenative morphology of words, meaning the substrings of the word corresponding to morphemes that, when concatenated, will give the lexical representation of the word type.",For the rest of the paper we will refer to this task as (morphological) segmentation.,"Several unsupervised POS induction systems make use of morphological features (Blunsom and Cohn, 2011; Lee et al., 2010; Berg-Kirkpatrick et al., 2010; Clark, 2003; Christodoulopoulos et al., 2011)",In a similar fashion one could think that knowing POS tags could be useful for learning morphological segmentations and in this paper we will study this hypothesis.,In this paper we will build a model that combines POS induction and morphological segmentation into one learning problem.,Multi Citance
159,D10-1083.xml,P11-1087.xml,These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag.,"Conversely, Ganchev et al.(2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags.","Recently Lee et al.(2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity.","Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint.","This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers distributions over tag trigrams, bigrams and uni- grams and whole words and their character level representation.",Single Citance
160,D10-1083.xml,P11-1087.xml,If we restrict the model to bigrams we see a considerable drop in performance.,Note that the bigram PYPHMM outperforms the closely related BHMM (the main difference being that we smooth tag bigrams with unigrams).,It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).,Figures 4 and 5 provide insight into the behavior of the sampling algorithms.,The former shows that both our models and mkcls induce a more uniform distribution over tags than specified by the treebank.,Single Citance
161,D10-1083.xml,P13-1150.xml,"Finally, we note some similarities of our model to some ideas proposed in other contexts.",We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel).,"Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011)",Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1 We note that similar ideas were simultaneously proposed.,"For example, the cluster Poisson parameter over vowel observation types might be λ = 9 (indicating 9 vowel letters on average for the cluster), while the parameter over consonant observation types might be λ = 20 (indicating 20 consonant letters on average).These priors will be distinct In contrast, the transition Dirichlet parameters may be asymmetric, and thus very specific and informative.",Multi Citance
162,D10-1083.xml,W11-0301.xml,"dependencies be Θ−|T ∼ Dirichlet(α− , L−) tween affixes and the POS tag are much stronger than those be Θ0 ∼ Dirichlet(α0, L0) Θ+|T ∼ Dirichlet(α+, L+) tween the stems and tags.","In our preliminary experiments, when stems are also generated conditioned on the tag, spurious stems are easily created and associated with garbage-collecting tags.","Here, W t refers to the set of word types that are generated by tag t. In other words, conditioned on tag t, we can only generate word w from the set of word types in W t which is generated earlier (Lee et al., 2010).",Token-Seg Model The model captures the morphological agreement between adjacent segmentations using a first-order Markov chain.,"The probability of drawing a sequence of segmentations s is given by all latent variables, including the segmentations s. P (s, t, S, T , L|w, W , γ, α, β) r ∝ P (w, s, t, W , S, T , L, Θ, θ|γ, α, β)dΘdθ We want to sample from the above distribution using collapsed Gibbs sampling (Θ and θ integrated out.)",Single Citance
163,D10-1083.xml,W12-1914.xml,Unsupervised induction of word categories has been approached from three broad perspectives.,"First, it is of interest to cognitive scientists who model syntactic category acquisition by children (Redington et al. 1998, Mintz 2003, Parisien et al. 2008, Chrupała and Alishahi 2010), where the primary concern is matching human performance patterns and satisfying cog- nitively motivated constraints such as incremental learning.","Second, learning categories has been cast as unsupervised part-of-speech tagging task (recent work includes Ravi and Knight (2009), Lee et al.(2010), Lamar et al.","(2010), Christodoulopoulos et al.(2011)), and primarily motivated as useful for tagging under-resourced languages.","Finally, learning categories has also been researched from the point of view of feature learning, 100 where the induced categories provide an intermediate level of representation, abstracting away and generalizing over word form features in an NLP application (Brown et al. 1992, Miller et al. 2004, Lin and Wu 2009, Turian et al. 2010, Chrupala 2011, Ta¨ckstro¨ m et al. 2012).",Multi Citance
164,E03-1020.xml,W11-1104.xml,"aWe consider graph-based approaches to WSI, which typically construct a graph from word occurrences or collocations.",The core problem is how to identify sense-specific information within the graph in order to perform sense induction.,"Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.","We reinterpret the challenge of identifying sense- specific information in a co-occurrence graph as one of community detection, where a community is de fined as a group of connected nodes that are more connected to each other than to the rest of the graph (Fortunato, 2010).","Within the co occurrence graph, we hypothesize that communities identify sense- specific contexts for each of the terms.",Multi Citance
165,E03-1020.xml,W11-2214.xml,"For example, “earth” may refer to the planet Earth, dirt, or solid ground, depending on the context.",The goal of Word Sense Induction (WSI) is to automatically discover the different senses by examining how a word is used.,"This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003).","Furthermore, these discovered senses can be used to automatically expand lexical resources such as WordNet or FrameNet (Klapaftis and Manandhar, 2010).",Discovering the multiple senses is frequently confounded by the relationships between a word’s senses.,Single Citance
166,E03-1020.xml,W11-2214.xml,These links refine co- occurrence based contexts by utilizing syntactic indications of how words are related.,"Dependency parsed features have proven highly effective for word representations in many NLP applications, e.g., (Pado´ and Lapata, 2007; Baroni et al., 2010).","We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.","We note that recently Kern et al.(2010) achieved good WSI performance with only a small, manually-tuned subset of all relations as context.","Word Ordering Word ordering can provide a mild form of syntactic information (Jones et al., 2006; Sahlgren et al., 2008).",Multi Citance
167,E09-2008.xml,N13-1140.xml,"Kay, 1994).","It is now widely accepted that finite- state transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach.","Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser.","Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajicˇ et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009).","Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000).",Single Citance
168,E09-2008.xml,W11-2605.xml,"Evaluating and comparing both approaches is motivated because the first method may produce much higher recall by virtue of generating a large number of input-output candidates during application, and the question is whether the corresponding loss in precision may be mitigated by judicious application of post-processing filters.","Both of the methods we have evaluated involve learning a set of string-transformation rules to convert words, morphemes, or individual letters (graphemes) in the dialectal forms to the standard variant.","The rules that are learned are in the format of so-called phonological replacement rules (Beesley and Karttunen, 2002) which we have later converted into equivalent finite-state transducers using the freely available foma toolkit (Hulden, 2009a).","The reason for the ultimate conversion of the rule set to finite-state transducers is twofold: first, the transducers are easy to apply rapidly to input data using available tools, and secondly, the transducers can further be modified and combined with the standard morphology already available to us as a finite transducer.","In its simplest form, a replacement rule is of the format A → B || C D (1) where the arguments A,B,C ,D are all single symbols or strings.",Multi Citance
169,E09-2008.xml,W12-1003.xml,3.2 Structure checking.,"After writing the verse, the system can evaluate if it is technically correct, i.e. if the overall structure is correct and if each line in the form abides by the required syllable count and rhyming scheme.","The syllable counter is implemented using the foma software (Hulden, 2009), and the implementation (Hulden, 2006) can be found on the homepage of Figure 1: A verse written in the BAD web application.","foma.3 Separately, we have also developed a rhyme checker, which extracts special patterns in the lines that must rhyme and checks their conformity.",These patterns are extracted using foma (see section 3.4) after which some phonological rules are applied.,Multi Citance
170,E09-2008.xml,W12-6202.xml,".o. ConsN) .o. ConsN .o. \%* -> 0; Now, this yields a transducer that maps every underlying form to n asterisks, n being the number of violations with respect to ConsN in the candidates that have successfully survived ConsN.","If this transducer represents a function (is single-valued), then we know that two candidates with a different number of violations have not survived ConsN, and that the worsening yielded the correct answer.","Since the question of transducer functionality is known to be decidable (Blattner and Head, 1977), and an efficient algorithm is given in Hulden (2009a), which is included in foma (with the command test functional) we can address this question by calculating the above for each constraint, if necessary, and then permute the violation markers until the above transducer is functional.",3.4 Equivalence testing.,"In many cases, the purpose of an OT grammar is to capture accurately some linguistic phenomenon through the interaction of constraints rather than by other formalisms.",Single Citance
171,E09-2008.xml,W12-6211.xml,2.1 Finite-state models.,Foma toolkit was used to build the FS machines and code the evaluation sets.,"Foma (Hulden, 2009) is a freely available2 toolkit that allows to both build and parse FS automata and transducers.","Foma offers a versatile layout that supports imports/exports from/to other tools such as: Xerox XFST (Beesley and Karttunen, 2003), AT&T (Mehryar Mohri and Riley, 2003), OpenFST (Riley et al., 2009).","There are, as well, outstanding alternatives such as HFST (Linde´n et al., 2010).",Single Citance
172,E09-2008.xml,W12-6212.xml,Qc 2012 Association for Computational Linguistics fer at the verb chunk level is carried out.,"The verbal chunk transfer is a very complex module because of the nature of Spanish and Basque auxiliary verb constructions, and is the main subject of this paper.","This verb chain transfer module is implemented as a series of ordered replacement rules (Beesley and Karttunen, 2003) using the foma finite-state toolkit (Hulden, 2009).","In total, the system consists of 166 separate replacement rules that together perform the verb chunk translation.","In practice, the input is given to the first transducer, after which its output is passed to the second, and so forth, in a cascade.",Multi Citance
173,E09-2008.xml,W12-6212.xml,"Since Matxin was designed to be open source, we built a simple compiler that converted the XFST rules into regular expressions that could then be applied without FST technology, at the cost of execution speed.",This verbal chunk transfer module read and applied these regular expressions at a speed of 50 verbal chunks per second.,"In the work presented here, we have reimplemented and expanded the original rules written for XFST with the foma2 toolkit (Hulden, 2009).","After adapting the grammar and compiling it, the 55 separate transducers occupy 607 kB and operate at roughly 2,000 complete verb chains per second.3 Passing the strings from one transducer to the next in the chain of 55 transducers in accomplished by the depth-first-search transducer chaining functionality available in the foma API.",2 http://foma.sourceforge.net 3 On a 2.8MHz Intel Core 2 Duo..,Single Citance
174,E09-2008.xml,W12-6213.xml,4.3 Surface lexicon extraction.,"Having the BAMA represented as a FST also allows us to extract the output projection of the grammar, producing an automaton that only accepts legitimate words in Arabic.","This can be then be used in spell checking applications, for example, by integrating the lexicon with weighted transduc ers reflecting frequency information and error models (Hulden, 2009a; Pirinen et al., 2010).",4.4 Constraint analysis.,"Interestingly, the BAMA itself contains a vast amount of redundant information in the co- occurrence constraints.",Multi Citance
175,H05-1115.xml,P09-1083.xml,"In order to rank sentence for opinion question answering, two aspects should be taken into account.","First, the answer candidate is relevant to the question topic; second, the answer candidate is suitable for question sentiment.","Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005).","Given the set Vs = {vi} containing all the sentences to be ranked, we construct a graph where each node represents a sentence and each edge weight between sentence vi and sentence vj is induced from sentence similarity measure as fol Figure 1: Opinion PageRank first layer contains all the sentiment words from a lexicon to represent the opinion information, and the second layer denotes the sentence relationship lows: ( ) = f (i→j) k=1 f (i→k) , where f (i → j) in the topic sensitive Markov Random Walk model discussed above.","The dashed lines between these represents the similarity between sentence vi and sentence vj , here is cosine similarity (BaezaYates and RibeiroNeto, 1999).",Single Citance
176,H05-1115.xml,P09-1083.xml,"word oj , otherwise 0.",Est denotes the relationship between sentence and topic word.,"Its weight twij is calculated by tf · idf (Otterbacher et al., 2005).",The word’s sentiment score is fixed in Opinion PageRank.,"This may encounter problem when We define two matrixes O = (Oij ) and the sentiment score definition is not suitable for |Vs|×|Vt| as follows, for Oij = owij , the specific question.",Single Citance
177,H05-1115.xml,P09-1083.xml,"Relevant answers are further processed by focus detection, opinion scope identification and polarity detection.",Some works on opinion mining are motivated by opinion question answering.,"includingfact-based QA and text summarization (Erkan andRadev, 2004; Mihalcea and Tarau, 2004; Otter-bacher et al., 2005; Wan and Yang, 2008).","(SooMin and Hovy, 2005) addresses another important component of opinion question answering: finding opinion holders.","More recently, TAC 2008 QA track (evolved from TREC) focuses on finding answers to opinion questions (Dang, 2008).",Multi Citance
178,H05-1115.xml,C10-2049.xml,"Informative sentences, on the other hand, are better extracted from the source articles themselves, rescoreS , scoreS , and scoreS are the compo nent scores of the sentence S with respect to the ancestor, current or other remaining nodes.","We give positive credit to a sentence that contains keywords from an ancestor node, but penalize sentences with keywords from other topics (as such sentences would be better descriptors for those other topics).","Component relevance scores are calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring a specific strategy.","As informative sen tences contain more content, our strategy with scoreQ = rel(S, Q) Q′ rel(S, Q′) S Q (2) GCSum is to attempt to locate informative sentences to describe the internal nodes, failing which GCSum falls back to using predefined templates to generate an indicative placeholder.","= w∈Q log(tfw + 1) × log(tfw + 1) × isfw Nor m where rel(S, Q) is the relevance of S with respect to topic Q, Norm is a normalization factor of rel(S, Q) over all input sentences, tf S and tf Q w w To implement GCSum’s informative extractor, we use ",Single Citance
179,H05-1115.xml,P08-2003.xml,"In Natural Language Processing (NLP), this information synthesis can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information.","Recently, the graph-based method (LexRank) is applied successfully to generic, multi document summarization (Erkan and Radev, 2004).","A topic- sensitive LexRank is proposed in (Otterbacher et al., 2005).","In this method, a sentence is mapped to a vector in which each element represents the occurrence frequency (TF*IDF) of a word.","However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information thus cannot distinguish between “The hero killed the villain” and “The villain killed the hero”.",Single Citance
180,H05-1115.xml,P08-2003.xml,Each sentence is represented as a vector of term specific weights.,The term specific weights in the sentence vectors are products of term frequency (tf) and inverse document frequency (idf).,"To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005).","The score of a sentence is determined by a mixture model: rel(s|q) Figure 1: Example of semantic trees Where TREESIM(s,q) is the normalized syntactic (and/or semantic) similarity between the query (q) and the document sentence (s) and C is the set of all sentences in the collection.","In cases where the query is composed of two or more sentences, we compute the similarity between the document sentence (s) and each of the query-sentences (qi) then we take the average of the scores.",Single Citance
181,H05-1115.xml,P10-2055.xml,selects all the synonyms of the title phrases in the document as key phrases.,"Afterwards, our approach is evaluated against two existing approaches, which rely on the conventional semantic network and are able to capture binary relations only.","The other one is based on topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here.",The title-sensitive PageRank approach makes use of title phrases to re-weight the transitions between vertices and picks up 10% top ranked phrases as key phrases.,"Approach Precision Recall F1 Title-sensitive Pa- geRank (d=0.15) 34.8% 39.5% 37.0% Title-community 29.8% 56.9% 39.1% Our approach (=1.8, =0.5) 39.4% 44.6% 41.8% WordNet expansion (baseline) 7.9% 32.9% 12.5% Table 1.",Pre Contiguous
182,H89-2014.xml,W93-0111.xml,Iterating the method in this way should be able to refine the classes until a fixed point is reached at which no further improvement in classification occurs.,The major challenge in using this approach will be to keep it computationally tractable.,"This approach is similar in spirit to the iterative computational approaches of the Hidden Markov Models (Kupiec, 1989",l l VBNP 4 JJ.,31 'I 9 VBD 46 VBN.,Single Citance
183,H89-2014.xml,J93-2006.xml,One can similarly use probabilities for assigning semantic structure (Section 4).,We report in Section 2 on our experiments on the assignment of part of speech to words in text.,The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985),"Our work is an incremental improvement on these models in three ways: (1) Much less training data than theoretically required proved adequate; (2) we inte­ grated a probabilistic model of word features to handle unknown words uniformly within the probabilistic model and measured its contribution; and (3) we have applied the forward-backward algorithm to accurately compute the most likely tag set.In Section 3, we demonstrate that probability models can improve the performance of knowledge-based syntactic and semantic processing in dealing with structural am­ biguity and with unknown words.","Though the probability model employed is not new, our empirical findings are novel.",Pre Contiguous
184,H89-2014.xml,C00-1081.xml,2.3 Selecting Words to be Lexicalized.,"Generally speaking, a word-based n-gram model is better than a POS-based n-gram model in terms ofpredictive power; however lexicalization of some in frequent words may be harmful because it may cause a data-sparseness problem.","In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.","Also, in a state-of-the-art English parser (Collins, 1997) only the words that occur more than 4 times in training data are lexicalized.","For this reason, our parser selects the words to belexicalized at the time of learning.",Single Citance
185,H89-2014.xml,H92-1022.xml,obtained a high degree of accuracy without performing any syntactic analysis on the input.,These stochastic part of speech taggers make use of a Markov model which captures lexical and contextual information.,"The parameters of the model can be estimated from tagged (1, 3, 4, 6, 12] or untagged [2, 9, 11] text.","Once the parameters of the model are estimated, a sentence can then be automatically tagged by assigning it the tag se­ quence which is assigned the highest probability by the model.",Performance is often enhanced with the aid of various higher level pre- and postprocessing procedures or by manually tuning the model.,Single Citance
186,H89-2014.xml,C92-1060.xml,Forpedagogical reasons individual words were shown as el ements of this matrix.,"A vocabulary exeeeding 220,000words is actually used by the program so it is not practi cal to try to reliably estimate the B matrix probabilities for each word individually.","Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989)","Thus &quot;type&quot;, &quot;store&quot; and &quot;dog&quot; would all be labelled as singular-noun-or-nninflected-verb.","Forthe category set that is used, only 250 different equiva lence elasees are necessary to cover the dictionary.It is important that the initial guesses for param eter values are well-informed.",Single Citance
187,I05-5011.xml,C08-1107.xml,One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet).,Supervised learning of broad coverage rule-sets is an arduous task.,"This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).","Most unsupervised entailment rule acquisition methods learn binary rules, rules between templates with two variables, ignoring unary rules, rules between unary templates (templates with only one variable).However, a predicate quite often appears in the text with just a single variable (e.g. intransitive verbs or passives), where infer ence requires unary rules, e.g. ‘X take a nap → X sleep’ (further motivations in Section 3.1).",In this paper we focus on unsupervised learning of unary entailment rules.,Multi Citance
188,I05-5011.xml,C10-2017.xml,Paraphrases can also be used to improve natural language processing (NLP) systems.,"In this direction, (CallisonBurch et al., 2006) tried to improve machine translations by enlarging the coverage of patterns that can be translated.","In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.",Most of these applications need a n-best set of solutions in order to rerank them according to a task-specific criterion.,"In order to produce the paraphrases, a promising approach is to see the paraphrase generation problem as a statistical translation problem.",Multi Citance
189,I05-5011.xml,D12-1094.xml,"Formally, a relation phrase ctx is polysemous if there exist 2 different relations < ��1 , ��, ��2 > and < 𝐶 ′, ��′, 𝐶 ′ > where 𝑐𝑡𝑥 ∈ 𝑃 ∩ ��′.",In the previ ly related tasks: paraphrase discovery and recog1 2 nizing textual entailment.,"Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.",The Recognizing Textual Entailment algorithms (Berant et al. 2011) can also be used to find related phrases since they find pairs of phrases in which one entails the other.,"To efficiently cluster high-dimensional datasets, canopy clustering (McCallum et al., 2000) uses a cheap, approximate distance measure to divide data into smaller subsets, and then cluster each subset using an exact distance measure.",Multi Citance
190,I05-5011.xml,E09-1025.xml,"The experimental results will be presented in Section 5, followed by an error analysis and discussions in Section 6.",Finally Section 7 will conclude the paper and point out future work directions.,"A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).","In our work we use the DIRT collection because it is the largest one available and it has a relatively good accuracy (in the 50% range for top generated paraphrases, (Szpektor et al., 2007)).","In this section, we describe the DIRT algorithm for acquiring inference rules.",Multi Citance
191,I05-5011.xml,I08-1070.xml,2.3 Acquiring paraphrase instances.,"As reviewed in Section 1, paraphrase rules generate incorrect paraphrases, because their applicability conditions are not specified.","To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).","Although these clues restrict phenomena to those appearing in particular domain or those describing coordinated events, they have enabled us to collect 1 See http://nlp.cs.nyu.edu/WTEP/.",paraphrases accurately.,Multi Citance
192,I05-5011.xml,J10-3003.xml,"If the variables in the patterns link to the same or comparable named entities (based on the entity text and type), then consider the patterns to be paraphrases of each other.","At the end, the output is a list of generalized paraphrase patterns with named entity types as variables.","As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.",The idea of enlisting named entities as proxies for detecting semantic equivalence is interesting and has certainly been explored before (see the discussion regarding Pas¸ca and Dienes [2005] in Section 3.2).,"However, it has some obvious disadvantages.",Single Citance
193,I05-5011.xml,P06-2094.xml,"For example, (Barzilay 01) proposed a method to extract paraphrases from parallel translations derived from one original document.","We proposed to find paraphrases from multiple newspapers reporting the same event, using shared Named Entities to align the phrases (Shinyama et al. 02).",We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).,The phrases connecting two NEs are grouped based on two types of evidence.,"One is the identity of the NE instance pairs, as multiple instances of the same NE pair (e.g. Yahoo!",Single Citance
194,I05-5011.xml,P07-1058.xml,"However, it shouldn’t be applied for “Students acquired a new language”.","In the same manner, the rule ‘X acquire Y → X learn Y ’ should be applied only when Y corresponds to some sort of knowledge, as in the latter example.","Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.","For example, when answering the question “Which companies did IBM buy?” a QA system would apply the rule ‘X acquire Y → X buy Y ’ correctly, since the phrase “IBM acquire X ” is likely to be found mostly in relevant economic contexts.",We thus expect that an evaluation methodology should consider context relevance for entailment rules.,Single Citance
195,I05-5011.xml,P07-1058.xml,"For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y → X own Y ’, which is clearly not a paraphrase.",2.2 Evaluation of Acquisition Algorithms.,"Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).","However, there is still no common accepted framework for their evaluation.","Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directional- ity.",Multi Citance
196,I05-5011.xml,P07-1058.xml,"Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components.","Thus, as in many other NLP learning settings, a direct evaluation is needed.","Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).","In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not.",The criterion for correctness is not explicitly described in most previous works.,Multi Citance
197,I05-5011.xml,P09-2063.xml,Paraphrases can also be used to improve natural language processing (NLP) systems.,"(CallisonBurch et al., 2006) improved machine translations by augmenting the coverage of patterns that can be translated.","Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.","In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and CallisonBurch, 2005).",A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT.,Single Citance
198,I05-5011.xml,P11-1109.xml,"However, there are few paraphrases involving metaphors or idioms in the outputs due to the nature of definition sentences.","In this regard, we do not claim that our method is almighty.",We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.,"In graphs (a) and (b), the precision of the SMT method goes up as rank goes down.","This strange behavior is due to the scoring by Moses that worked poorly for the data; it gave 1.0 to 82.5% of all the samples, 38.8% of which were incorrect.",Single Citance
199,I05-5011.xml,P12-2031.xml,"For example, to answer the question ‘Where was Reagan raised?’ a QA system can use the rule ‘X brought up in Y→X raised in Y’ to extract the answer from ‘Reagan was brought up in Dixon’.","Similarly, an IE system can use the rule ‘X work as Y→X hired as Y’ to extract the PERSON and ROLE entities in the “hiring” event from ‘Bob worked as an analyst for Dell’.","The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.","However, despite their potential, utilization of inference rule resources is currently somewhat limited.",This is largely due to the fact that these algorithms often produce invalid rules.,Multi Citance
200,I05-5011.xml,P12-2031.xml,These issues have also been noted by Sammons et al.(2010) and LoBue and Yates (2011).,A complementary application-independent evaluation method is hence necessary.,"Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).","However, Szpektor et al.(2007) observed that directly judging rules out of context often results in low inter-annotator agreement.","To remedy that, Szpektor et al.(2007) and 156 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156–160, Jeju, Republic of Korea, 814 July 2012.",Multi Citance
201,I05-5011.xml,P13-1131.xml,"Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek- tor and Dagan, 2008).","To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al., 2011), a large scale publicly available web- based open extractions data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X , and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y ’.","Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.","Next, we applied some cleanup preprocessing to the ReVerb extractions.","This includes discarding stop words, rare words and non-alphabetical words instantiating either the X or the Y arguments.",Single Citance
202,I05-5011.xml,W12-4006.xml,"Both kinds of rules are typically acquired either from structured sources (e.g. WordNet (Fellbaum, 1998)), or from unstructured sources according for instance to distributional properties (e.g. DIRT (Lin and Pantel, 2001)).","Entailment rules should typically be applied only in specific contexts, defined in (Szpektor et al., 2007) as relevant contexts.","Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.(Sekine, 2005), (CallisonBurch, 2008)), but most do not.","Because of a lack of an adequate representation of the linguistic context in which the 34 Proceedings of the 3rd Workshop on the People’s Web Meets NLP, ACL 2012, pages 34–43, Jeju, Republic of Korea, 814 July 2012.","Qc 2012 Association for Computational Linguistics rules can be successfully applied, their concrete use reflects this limitation.",Multi Citance
203,I05-5011.xml,W12-4006.xml,"However, so far it has only been used as source of factual knowledge, while in our work the focus is on the acquisition of more complex rules, concerning for instance spatial or temporal expressions.","The interest of the research community in producing specific methods to collect inference and paraphrase pairs is proven by a number of works in the field, which are relevant to the proposed approach.","As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.","In the Microsoft Research Paraphrase Corpus2, pairs of sentences are extracted from news sources on the web, and manually annotated.","As for rule repositories collected using dis- tributional properties, DIRT (Discovery of Inference 3quisition methodology.",Single Citance
204,J00-3003.xml,W13-4047.xml,"In addition, we compare the recognition accuracy over time and find that high accuracy can be obtained with as little context as one system dialogue act, so that there is often no need to take a larger context into account.",Approaches to dialogue act recognition from spoken input have explored a wide range of methods.,"(Stolcke et al., 2000) use HMMs for dialogue modelling, where sequences of observations correspond to sequences of dialogue act types.","(Zimmermann et al., 2005) also use HMMs in a joint segmentation and classification model.","(Grau et al., 2004) use a combination of Naive Bayes and n-grams with different smoothing methods.",Single Citance
205,J00-3003.xml,W12-1634.xml,Dialogue act classification is concerned with understanding users’ communicative intentions as reflected in their utterances.,It is an important first step toward building automated dialogue systems.,"To date, the majority of work on dialogue act modeling has addressed spoken dialogue (Samuel et al., 1998; Stolcke et al., 2000; Surendran and Levow, 2006; Bangalore et al., 2008; Sridhar et al., 2009; Di Eugenio et al., 2010).","However, with the increasing popularity of computer-mediated means of conversation, such as instant messaging and social networking services, automated analysis of textual dialogue holds much appeal.Dialogue act modeling for textual conversations has many practical application areas, which include web- based intelligent tutoring systems (Boyer et al., 2010a), chat-based online customer service (Kim et al., 2010), and social media analysis (Joty et al., 2011).",Human interaction involves not only verbal communication but also nonverbal communication.,Multi Citance
206,J00-3003.xml,W12-1634.xml,"Research on nonverbal communication (Knapp and Hall, 2006; Mehrabian, 2007; Russell et al., 2003) has identified a range of nonverbal cues, such as posture, gestures, eye gaze, and facial and vocal expressions.","However, the utility of these nonverbal cues has not been fully explored within the context of dialogue act classification research.","Previous research has leveraged prosodic cues (Sridhar et al., 2009; Stolcke et al., 2000) and facial expressions (Boyer et al., 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored.","As a first step toward a dialogue system that learns its behavior from a human corpus, this paper proposes a novel approach to dialogue act classification that leverages information about users’ posture.","Posture has been found to be a significant indicator of a broad range of emotions (D’Mello and Graesser, 2010; Kapoor et al., 2007; Woolf et al., 2009).",Multi Citance
207,J00-3003.xml,N06-2021.xml,This is a simple first order HMM.,The HMM has been widely used in many tagging problems.,"Stolcke et al.(Stolcke et al., 2000) used it for dialog act classification, where each utterance (or dialog act) is used as the observation.","In speaker role detection, the observation is composed of a much longer word sequence, i.e., the entire speech from one speaker.","Figure 1 shows the graphical representation of the HMM for speaker role identification, in which the states are the speaker roles, and the observation associated with a state consists of the utterances from a speaker.",Pre Contiguous
208,J96-3004.xml,A00-2032.xml,"The work of Ito and Kohda (1995) similarly relies on high-frequency character n-grams, but again, is more concerned with using these frequent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon.","Our algorithm, on the hand, is fundamen­ tally different in that it incorporates no explicit no­ tion of word, but only ""sees"" locations between characters.","Chinese According to Sproat et al.(1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously pubÂ­ lished instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical apÂ­ proach.","In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data.","To our knowledge, the Chinese segmenter most similar to ours is that of Sun et al.(1998).",Multi Citance
209,J96-3004.xml,C00-2095.xml,Ambiguity is a central issue when talking about segmentation.,"The absence or ambiguity of word separators can lead to multiple segmen­ tations, and more than one of them can have a. meaning.","As (Sproat ct a.l., 1996) testify, several native Chinese speakers do not always agree on one unique tokeniza.tion for a. given sentence.","Thanks to the use of item graphs, Sumo can handle ambiguity efficiently.",Why try to fully disambiguate a tokeniza.tion when there is no agreement on a single best solution?,Single Citance
210,J96-3004.xml,C02-1049.xml,"It resolves ambiguous and false extractions based on the morphological validity, syntactic validity, and statistical validity.",3 Unknown Word Detection.,"Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (Chen &amp; Liu, 1992, Sproat et al, 1996).",Hence after segmentation process the unknown words in the text would be incorrectly segmented into pieces of single character word or shorter words.,"If all occurrences of monosyllabic words are considered as morphemes of unknown words, the recall rate of the detection will be about 99%, but the precision is as low as 13.4% (Chen & Bai, 1998).",Multi Citance
211,J96-3004.xml,C02-1049.xml,msi + 2 (?),"combin e(i, i + 1, i + 2) word.","Mutu al infor matio n-like statist ics are very often adopt ed in meas uring assoc iation stren gth msi (?)dsi +1 () combine (i, i + 1) 1993, Sproat et al, 1996)",Howe ver such kind of msi (?)msi +1 (?),psi + 2 () msi (?)msi +1 (?),Multi Citance
212,J96-3004.xml,C02-1080.xml,A similar system is also reported in Luo & Song (01).,Chinese NE recognition is much more difficult than that in English due to two major problems.,"The first is the word segmentation problem (Sproat et al. 96, Palmer 97).","In Chinese, there is no white space to delimit the words, where a word is defined as consisting of one or more characters representing a linguistic token.","Word is a vague concept in Chinese, and Palmer (97) showed that even native speakers could only achieve about 75% agreement on “correct” segmentation.",Pre Contiguous
213,J96-3004.xml,C02-1143.xml,About 200 sentences for each word were selected randomly from PDN and sense- tagged as with the CTB.,We automatically annotated the PDN data to yield the same types of annotation that had been available in the CTB.,"We used a maximum- matching algorithm and a dictionary compiled from the CTB (Sproat et al., 1996; Xue, 2001) to do segmentation ","The system performance is much lower for the PDN than for the CTB, for several reasons.","First, the PDN corpus is more balanced than the CTB, which contains primarily financial articles.",Multi Citance
214,J96-3004.xml,E09-1063.xml,The intrinsic quality of word segmentation is normally evaluated against a manually segmented gold-standard corpus using F-score.,"While this approach can give a direct evaluation of the quality of the word segmentation, it is faced with several limitations.","First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996).","Second, an increase in F-score does not necessarily imply an improvement in translation quality.","It has been shown that F-score has a very weak correlation with SMT translation quality in terms of BLEU score (Zhang et al., 2008).",Single Citance
215,J96-3004.xml,I05-3031.xml,"Experimental results show that our scheme in Integrated Mode performs the best in terms of accuracy, where Separated Mode is more suitable under limited computational resources.",The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.,"As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).","In this paper, we propose a statistical approach based on the works of (Xue and Shen, 2003), in which the Chinese word segmentation problem is first transformed into a tagging problem, then the Maximum Entropy classifier is applied to solve the problem.",We further improve the scheme by introducing correctional treatments after first round tagging.,Pre Contiguous
216,J96-3004.xml,J00-3004.xml,Word segmentation is an important prerequisite for such applications.,"However, it is a difficult and ill-defined task.","According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the &quot;correct&quot; segmentation, and the figure reduces as more people become involved.","This paper describes a general scheme for segmenting text by inferring the position of word boundaries, thus supplying a necessary preprocessing step for applications like those mentioned above.","Unlike other approaches, which involve a dictionary of legal words and are therefore language-specific, it works by using a corpus of already­ segmented text for training and thus can easily be retargeted for any language for which a suitable corpus of segmented material is available.",Multi Citance
217,J96-3004.xml,J00-3004.xml,"in words-for example, compare the size of the T section of a telephone directory with the size of the T section of a dictionary-but such differences are far more pronounced in Chinese.",three-character names have arisen in recent years to ensure uniqueness when the fam­ ily name is popular-such as Smith or Jones in English.,"Sproat et al.(1996) implement special recognizers not only for Chinese names and transliterated foreign names, but for components of morphologically obtained words as well.","The approach we present is not specially tailored for name recognition, but because it is fully adaptive it is likely that it would yield good performance on names if lists of names were provided as supplementary training text.",This has not yet been tested.,Single Citance
218,J96-3004.xml,J04-1004.xml,"The other is the inductive strategy, which identifies words through the compositional process of morpho-lexical rules.","This strategy represents words with common characteristics (e.g., numeric compounds) by rules.","In Chinese text segmentation there are three basic approaches (Sproat et al. 1996): pure heuristic, pure statistical, and a hybrid of the two.",The heuristic approach identifies words by applying prior knowledge or morpho-lexical rules governing the derivation of new words.,The statistical approach identifies words based on the distribution of their components in a large corpus.,Single Citance
219,J96-3004.xml,J04-1004.xml,"In Chinese text, each substring of a whole sentence can potentially form a word, but only some substrings carry clear meanings and thus form a correct word.","For example, the sentence has 21 substrings, but only four substrings, , , , and , can be considered words (we do not consider single-character words here).In some implementations, the segmentation method is used to extract those words (recent reviews on Chinese word segmentation include Wang, Su, and Mo [1990] and Wu and Tseng [1993]).","There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching(Teahan et al. 2000; Dai, Loh, and Khoo 1999; Sproat et al. 1996).","If the dictionary includes the words , , and , then forward maximum matching will ex tract two words, and , after segmenting the sentence.","If is deleted 78 from the dictionary, then the sentence will be segmented into , , , and , and two words, and , are obtained.",Multi Citance
220,J96-3004.xml,J04-1004.xml,"In the following, we tested our method on the large corpus and all strings with lengths from two to seven characters.","In the end, we extracted the numeric-type compounds from each corpus.","In addition, there is no commonly accepted standard for evaluating the performance of word extraction methods, and it is very hard to decide whether a word is meaningful or not (Sproat et al. 1996).","We define precision as the number of extracted words that would be meaningful in a Chinese native speaker ’s opinion, divided by the total number of extracted compounds.","As it is very hard to find all of the words in the original corpus that would be found meaningful by a Chinese person, it is very hard to count recall in the traditional way, that is, the number of meaningful words extracted divided by the number of all meaningful words in the original data.",Single Citance
221,J96-3004.xml,J04-1004.xml,"For example, when the threshold was set to three, the precision and partial recall with the small corpus were 83.8% and 66.5%, respectively, whereas with the large corpus they were 58.3% and 87.2%, respectively.","When the threshold was set to nine, the corresponding numbers were 97.7% and 41.5% versus 73.4% and 80.4%.","As even human judges differ when facing the task of segmenting a text into words and test corpora differ from system to system (Sproat et al. 1996), it is very difficult to compare two methods.","To convincingly illustrate the efficiency of our method, we chose one of the most direct ways: We implemented Chang and Su’s (1997) method and our own method on a corpus, the size of which was similar to the one that was used in their paper.","We chose Chang and Su’s paper as reference for two reasons: Their approach was unsupervised, just like ours, and it was a complicated iterative method that integrated several commonly used word-filtering techniques (including Viterbi training, mutual information, entropy, and joint Gaussian mixture density function) to improve their result.",Single Citance
222,J96-3004.xml,J05-4005.xml,We believe that the identification of OOV words should not be treated as a problem separate from word segmentation.,We propose a unified approach that solves both problems simultaneously.,"A previous work along this line is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).",Our approach is similarly motivated but is based on a different mechanism: linear mixture models.,"As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information.",Single Citance
223,J96-3004.xml,J05-4005.xml,"First, the “correct” segmentation is not clearly defined.",It is common that for a given sentence there are multiple plausible word segmentations.,"As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%.","To deal with this problem, Fung and Wu (1994) suggest a procedure called nk-blind that uses n blind judges’ standards.","If we set k = 1, it is sufficient for a segmentation to be considered correct if it agrees with at least one of the n judges.",Single Citance
224,J96-3004.xml,J05-4005.xml,"If k = n, all judges must agree.","Therefore, nk-blind gives a more representative performance measure by taking into account multiple judges.","Similarly, Sproat et al.(1996) also uses multiple human judges.","In Section 8.2, we will present our method for cross-system comparison.",We do not use multiple human judges.,Single Citance
225,J96-3004.xml,J05-4005.xml,"Class models are defined as generative models that are estimated on their corresponding named entity lists using MLE, together with a backoff smoothing schema, as described in Section 4.1.1.",We will describe briefly the constraints and the class models here.,The Chinese person-name model is a modified version of that described in Sproat et al.(1996).,"Other NE models are novel, though they share some similarities with the Chinese person-name model.",5.3.1 Chinese Person Names.,Single Citance
226,J96-3004.xml,J11-1005.xml,"Human readers often use semantics, contextual informa tion about the document, and world knowledge to resolve segmentation ambiguities.",There is no fixed standard for Chinese word segmentation.,Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996).,"Also, specific NLP tasks may require different segmentation criteria.","For example, could be treated as a single word (Bank of Beijing) for machine translation, although it is more naturally segmented into (Beijing) (bank) for tasks such as text-to-speech synthesis.",Single Citance
227,J96-3004.xml,J11-3001.xml,The unsupervised approaches are not comparable with supervised.,"ones in general, because the conventional criteria are the manual segmentations known as gold standards.","Gold standards, however, 435 cannot be uniﬁed into a single standard (Fung and Wu 1994; Sproat et al. 1996).",2,The unsupervised approaches are not comparable with each other to.,Multi Citance
228,J96-3004.xml,J97-4004.xml,"Words, and tokens in general, are the primary building blocks in almost all linguistic theories (e.g., Gazdar, Klein, Pullum, and Sag 1985; Hudson 1984) and language pro­ cessing systems (e.g., Allen 1995; Grosz, Jones, and Webber 1986).","Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992).","Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al. 1994; Chen and Liu 1992; Chiang et al. 1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al. 1992; Li et al. 1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al. 1996;","* Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; email: guojin@iss.nns.sg © 1997 Association for Computational Linguistics The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al. 1992), and English (Garside, Leech, and Sampson 1987), in various media, such as continuous speech and cursive handwriting, and in numerous appli­ cations, such as translation, recognition, indexing, and proofreading.","For Chinese, sentence tokenization is still an unsolved problem, which is in part due to its overall complexity but also due to the lack of a good mathematical de­ scription and understanding of the problem.",Multi Citance
229,J96-3004.xml,J97-4004.xml,"For instance, in Ma (1996), words in a tokenization dictionary are represented as pro­ duction rules and character strings are modeled as derivatives of these rules under a string concatenation operation.","Although not stated explicitly in his thesis, this is obviously a finite-state model, as evidenced from his employment of (finite-) state diagrams for representing both the tokenization dictionary and character strings.",The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.,They both stop at merely representing possible tokenizations as a single large finite-state diagram (word graph).,The focus is then shifted to the problem of defining scores for evaluating each possible tokenization and to the associated problem of searching for the best-path in the word graph.,Single Citance
230,J96-3004.xml,J97-4004.xml,"While tokenization evaluation is important, it would be more effective if employed at a later stage.","On the one hand, critical tokenization can help greatly in developing tokenization knowledge and heuristics, especially those tokenization specific understandings, such as the observation of""one tokenization per source"" and the trick of highlighting hidden ambiguities by contrasting competing critical tokenizations (Guo 1997).","While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apÂ­ parently employed neither in Sproat et al.(1996) nor in Ma (1996).","Further, what has been implemented in the two systems is basically a token unigram function, which has been shown to be practically irrelevant to hidden ambiguity resolution and not to be much better than some simple maximum tokenization approaches such as shortest tokenization (Guo 1997).","On the other hand, critical tokenization can help significantly in boosting tokeniza­ tion efficiency.",Multi Citance
231,J96-3004.xml,N10-1068.xml,"We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches.","In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs).","Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and AlOnaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation.",• Complex problems can be broken down into a cascade of simple WFSTs.,• Input- and output-epsilon transitions allow compact designs.,Multi Citance
232,J96-3004.xml,P03-1035.xml,We believe that the identification of unknown words should not be defined as a separate problem from word segmentation.,These two problems are better solved simultaneously in a unified approach.,"One example of such approaches is Sproat et al.(1996), which is based on weighted finite-state transducers (FSTs).","Our approach is motivated by the same inspiration, but is based on a different mechanism: the improved source-channel models.","As we shall see, these models provide a more flexible framework to incorporate various kinds of lexical and statistical information.",Single Citance
233,J96-3004.xml,P03-1035.xml,Consider the earlier example.,"Assuming that C* = LN/国际/航空/公司, where 中国 is tagged as a LN, the probability P(S’|ON) would be estimated using a word class bigram model as: P(中国国际航空公司 |ON) ≈ P(LN/国际/航空/公司|ON) P(中国|LN) = P(LN|<ON>)P(国际|LN)P(航空|国际)P(公司|航空) P(</ON>|公司)P(中国 |LN), where P(中国|LN) is the class model probability of 中国 given that it is a LN, <ON> and </ON> are symbols denoting the beginning and the end of a ON, respectively.",5.2.4 Transliterations of foreign names As described in Sproat et al.(1996): FNs are usually transliterated using Chinese character strings whose sequential pronunciation mimics the source language pronunciation of the name.,"Since FNs can be of any length and their original pronunciation is effectively unlimited, the recognition of such names is tricky.","Fortunately, there are only a few hundred Chinese characters that are particularly common in transliterations.",Single Citance
234,J96-3004.xml,P06-1010.xml,The English named entity candidate selection process was already described above.,Candidate Chinese transliterations are generated by consulting a list of characters that are frequently used for transliterating foreign names.,"As discussed elsewhere (Sproat et al., 1996), a subset of a few hundred characters (out of several thousand) tends to be used overwhelmingly for transliterating foreign names into Chinese.","We use a list of 495 such characters, derived from various online dictionaries.",A sequence of three or more characters from the list is taken as a possible name.,Pre Contiguous
235,J96-3004.xml,P06-1126.xml,"At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%.",Correlation between language model perplexity and word segmentation performance is also discussed.,"Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).","In Gao et al.(2003), an approach based on source-channel model for Chinese word segmentation was proposed.",Gao et al.(2005) further developed it to a linear mixture model.,Multi Citance
236,J96-3004.xml,P07-1015.xml,"Since the goal is to use minimal knowledge or data from the target language, using supervised methods is inappropriate for our approach.","Therefore, Chinese sentences were not segmented.","Using the 495 characters that are frequently used for transliterating foreign names (Sproat et al., 1996), a sequence of three of more characters from the list was taken as a possible candidate for Chinese.","4) For the given 200 English named entities and target language candidate lists, all the possible pairings of English and target-language name were considered as possible transliteration pairs.",The number of candidates for each target language is presented in Table 3.,Single Citance
237,J96-3004.xml,P07-1016.xml,Different Chinese characters may render into the same syllable and form a range of homonyms.,"Among the homonyms, those arousing positive meanings can be used for personal names.","As discussed elsewhere (Sproat et al., 1996), out of several thousand common Chinese characters, a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese, e.g. only 731 Chinese characters are adopted in the E-C corpus.","Although the character sets are shared across languages and genders, the statistics in Table 2 show that each semantic attribute is associated with some unique characters.","In the C-C corpus, out of the total of 4,507 characters, only 776 of them are for surnames.",Single Citance
238,J96-3004.xml,P12-1110.xml,"Note that P07– P09 and P18–P21 (look-ahead features) require the look-ahead information of the next word form and POS tags, which cannot be incorporated straightforwardly in an incremental framework.","Although we have found that these features can be incorporated using the delayed features proposed by Hatori et al.(2011), we did not use them in our current model because it results in the significant increase of computational time.","3.3.1 Dictionary features Because segmentation using a dictionary alone can serve as a strong baseline in Chinese word segmentation (Sproat et al., 1996), the use of dictionaries is expected to make our joint model more robust and enables us to investigate the contribution of the syntactic dependency in a more realistic setting.","Therefore, we optionally use four features D01–D04 associated with external dictionaries.","These features distinguish each dictionary source, reflecting the fact that different dictionaries have different characteristics.",Single Citance
239,J96-3004.xml,P12-1111.xml,"Take n = 2 for example, i.e. c = c1c2, the set of possible words is {c1, c2, c1c2}, i.e. s = |x| = 3.","There are only two possible so wˆ = arg max ) score(wi) . lutions subject to constraints (1) and (2), x = 110 w∈segGEN(c) i=1 where function segGEN maps character sequence c to the set of all possible segmentations of c. For example, w = (c1..cl1 )...(cn−lk +1...cn) represents a segmentation of k words and the lengths of the first and last word are l1 and lk respectively.","In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).","Exact search is possible with a Viterbi-style algorithm, but beam- search decoding is more popular as used in (Zhang and Clark, 2007) and (Jiang et al., 2008a).","We propose an Integer Linear Programming (ILP) formulation of word segmentation, which is naturally viewed as a word-based model for CWS.",Single Citance
240,J96-3004.xml,P97-1041.xml,"1 A major difficulty in evaluating segmentation al­ gorithms is that there are no widely-accepted guide­ lines as to what constitutes a word, and there is therefore no agreement on how to ""correctly"" seg­ ment a text in an unsegmented language.",It is 1Most published segmentation work has been done for Chinese.,"For a discussion of recent Chinese segmentation work, see Sproat et al. {1996).","frequently mentioned in segmentation papers that native speakers of a language do not always agree about the ""correct"" segmentation and that the same text could be segmented into several very different (and equally correct) sets of words by different na­ tive speakers.",Sproat et al.(l996) and Wu and Fung (1994) give empirical results showing that an agree­ ment rate between native speakers as low as 75% is common.,Single Citance
241,J96-3004.xml,P98-1076.xml,"Consequently, Zheng and Liu (1997) themselves merely took the apparent regularity as a special case, and focused on the development of local­ context-oriented disambiguation rules.","Moreover, while they constructed for tokenization disambiguation an annotated ""phrase base"" of all ambiguous fragments in the large corpus, they still concluded that good results can not come solely from corpus but have to rely on the utilization of syntactic, semantic, pragmatic and other information.",The actual implementation of the weighted finiteÂ­ state transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.,"While the primary strength of such a transducer is its effectiveness in representing and 6 Roughly a three-character fragment abc where a, b, c,.","ab, and be are all tokens in the tokenization dictionary.",Single Citance
242,J96-3004.xml,P98-1076.xml,"ab, and be are all tokens in the tokenization dictionary.","c, d, ab, be, and cd are all tokens in the tokenization dictionary.","utilizing local and sentential constraints, what Sproat et al.( 1996) implemented was simply a token unigram scoring function.","Under this setting, no critical fragment can realize different tokenizations in different local sentential context, since no local constraints other than the identity of a token together with its associated token score can be utilized.","That is, the requirement of one tokenization per source has actually been implicitly obeyed.",Single Citance
243,J96-3004.xml,P99-1036.xml,Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model.,The model can achieve 96.6% tag­ ging accuracy if unknown words are correctly seg­ mented.,"In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).",But there has been relatively little improve­ ment in recent years because most of the remaining errors are due to unknown words.,"There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).",Multi Citance
244,J96-3004.xml,P99-1036.xml,"About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996).",But there has been relatively little improve­ ment in recent years because most of the remaining errors are due to unknown words.,"There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).",We take the latter approach.,"To improve word segmenta­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.",Multi Citance
245,J96-3004.xml,P99-1036.xml,"There are two approaches to solve this problem: to increase the coverage of the dictionary (Fung and Wu, 1994; Chang et al., 1995; Mori and Nagao, 1996) and to design a better model for unknown words (Nagata, 1996; Sproat et al., 1996).",We take the latter approach.,"To improve word segmentaÂ­ tion accuracy, (Nagata, 1996) used a single general purpose unknown word model, while (Sproat et al., 1996) used a set of specific word models such as for plurals, personal names, and transliterated foreign words.",' The goal of our research is to assign a correct part of speech to unknown word as well as identifying it correctly.,"In this paper, we present a novel statistical model for Japanese unknown words.",Multi Citance
246,J96-3004.xml,P99-1036.xml,"As Figure 3 shows, word type information improves the prediction accuracy significantly.",4.4 Word Segmentation Accuracy.,"Word segmentation accuracy is expressed in terms of recall and precision as is done in the previous research (Sproat et al., 1996).","Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys.","Since it is inconvenient to use both recall and precision all the time, we also use the F-measure to indicate the overall performance.",Single Citance
247,J96-3004.xml,W00-0803.xml,Segmentation and morphological analysis are tedious tasks and the accuracy of the automatic segmentation and morphological analysis dniStically vary in different domains.,The word based indexing of CJK texts is therefore computationally expensive.,"Segmentation rutd morphological analysis related issues of both Chinese and Japanese are intensively addressed elsewhere (Sproat et al., 1996; MatsUIIt(ltO et al., 1997 and many others).",The n-gram (n >1) character based indexing is computationally expensive as well.,The number of in?exing terms (n-grams) i111creases drastically as n mcreases.,Multi Citance
248,J96-3004.xml,W00-1207.xml,"In languages like Chinese, where no word boundary exists in written texts, this is by no means an easy job.",In many cases the machine will not even realize that there is an unfound word in the sentence since most single Chinese characters can be words by themselves.,"Purely statistical methods of word segmentation (e.g. de Marcken 1996, Sproat et al 1996, Tung and Lee 1994, Lin et al (1993), Chiang et al (1992), Lua, Huang et al, etc.) often fail to identify those words because of the sparse data problem, as the likelihood for those words to appear in the training texts is extremely low.",There are also hybrid approaches such as (Nie dt al 1995) where statistical approaches and heuristic rules are combined to identify new words.,"They generally perform better than purely statistical segmenters, but the new words they are able to recognize are usually proper names and other relatively frequent words.",Multi Citance
249,J96-3004.xml,W01-0513.xml,A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere.,"In such cases, these larger units may be MWUs.","The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et.al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).","Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression.","Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not.",Multi Citance
250,J96-3004.xml,W02-1117.xml,"But the existing algorithms haven’t a universal data structure, each algorithm can resolve a problem, and correspond to a concrete data structure specifically.","In process of the difficulties, the first step is identification of all possible candidates of Chinese words segmentation.","For examples: these words should be obtained: The ambiguous string is .There are some methods to resolve this problem: the one is the method forward maximum matching, backward maximum matching and minimum matching are used to find out the possible word strings from the character string [Guo 1997; Sproat et al. 1996; Gu and Mao 1994; Li et al. 1991; Wang et al. 1991b; Wang et al. 1990].",The second is The words finding automaton based on the AhoCorasick Algorithm [Hong-I and Lua].,The former requires three scans of the input character string.,Multi Citance
251,J96-3004.xml,W02-1808.xml,"Liang 1986) Tokenization has been recognized as a widespread problem rather than being unique to Chinese and other oriental languages It is an initial or prerequisite phase of NLP for all languages although the obscurity of the problem varies from language to language (Webster and Kit 1992a Palmer 2 ) Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen 1994 Grefenstette 1999 Grefenstette et al 2 ) adopt ing a finite-state approach However identification of multi-word units such as proper names and technical terms in these languages is highlyof literature on both linguistic and compu tational sides as listed in (Liu et al 1994 Guo 1997) among many others In general these strategies can be divided into two camps namely dictionary-based and statistical-based approaches Nevertheless the former can be understood as a restricted instance of the latter with an equi-probability for each word in a given dictionaryl Most if not all dictionary-based strategies are built upon a few basic ""mechanical"" segmentation methods based on string matching (Kit et al 1989) among which the most applicable thus widely used since the very beginning are the two max mum ma ch ng methods (MMs) one scanning forward (FMM) and the other backward (BMM) Interestingly their performance frequently used as the baseline for evaluation is never too far away from the state- of-the-art approaches in terms of segmentation accuracy Although performing little statistical computation the MMs comply in general with the essential principle of the statistical-based approaches select a segmentation as probable as possible among all choices This ad hoc way of choosing the segmentation with fewest words usually leads to by coincidence a more probable output than most other choices with more words2 A dictionary is actually a restricted form of language model, in this sense.","The coincidence of fewer words with a greater probability can be illustrated as follows: given a string s, the probability of its most probable segmentation seg(s) in terms of a given language model isII comparable to that of multi-character Chinese prob(seg(s)) = max s== w prob(w ) words there are no delimiters available So far a great variety of segmentation strategies for Chinese with various linguistic resources have been explored yielding a large volume wlw2 n where prob(w ) is some conditional probability in the model.",Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996),compilation and automatic detection of high-tech terms and unknown words like names to complement a never-big-enough dictionary (Chang et al 1995 Pont and Croft 1996 Chang and Su 1997),of ambiguities (Sun and Zhou 1998),Multi Citance
252,J96-3004.xml,W03-1025.xml,"The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter.",It is widely known that Chinese word-segmentation is a hard problem.,"There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower","The reason is that human subjects may differ in segmenting things like personal names (whether family and given names should be one or two words), number and measure units and compound words, although these ambiguities do not change a human being’s understanding of a sentence.","Low agreement between humans affects directly evaluation of machines’ performance (Wu and Fung, 1994) as it is hard to define a gold standard.",Multi Citance
253,J96-3004.xml,W03-1025.xml,"Bikel and Chiang (2000) and Xu et al.(2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study.","Bikel and Chiang (2000) in fact contains two parsers: one is a lexicalized probabilistic context- free grammar (PCFG) similar to (Collins, 1997); the other is based on statistical TAG (Chiang, 2000).","Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low.","Without knowing and controlling testing conditions, it is nearly impossible to compare results in a meaningful way.","Therefore, we will compare our approach with some related work only without commenting on segmentation accuracy.",Multi Citance
254,J96-3004.xml,W03-1025.xml,Our method is supervised in that the training data is manually labeled.,Palmer (1997) uses transform-based learning (TBL) to correct an initial segmentation.,Sproat et al.(1996) employs stochastic finite state machines to find word boundaries.,Luo and Roukos (1996) proposes to use a language model to select from ambiguous word- segmentations.,All these work assume that a lexicon or some manually segmented data or both are available.,Single Citance
255,J96-3004.xml,W03-1728.xml,"Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are represented as strings of Chinese characters or hanzi without similar natural delimiters.","Therefore, the first step in a Chinese language processing task is to identify the sequence of words in a sentence and mark boundaries in appropriate places.","This may sound simple enough but in reality identifying words in Chinese is a nontrivial problem that has drawn a large body of research in the Chinese language processing community (Fan and Tsai, 1988; Gan et al., 1996; Sproat et al., 1996; Wu, 2003; Xue, 2003).",The key to accurate automatic word identification in Chinese lies in the successful resolution of ambiguities and a proper way to handle out-of-vocabulary words.,"The ambiguities in Chinese word segmentation is due to the fact that a hanzi can occur in different word-internal positions (Xue, 2003).",Multi Citance
256,J96-3004.xml,W05-0709.xml,"The final machine is a trigram language model, specifically a KneserNey (Chen and Goodman, 1998) based back- off language model.","Differing from (Lee et al., 2003), we have also introduced an explicit model for un 1 As an example, we do not chain mentions with different gender, number, etc.Figure 1: Illustration of dictionary based segmenta tion finite state transducer 3.1 Bootstrapping.","In addition to the model based upon a dictionary of stems and words, we also experimented with models based upon character n-grams, similar to those used for Chinese segmentation (Sproat et al., 1996).","For these models, both arabic characters and spaces, and the inserted prefix and suffix markers appear on the arcs of the finite state machine.","Here, the language model is conditioned to insert prefix and suffix markers based upon the frequency of their appearance in n-gram character contexts that appear in the training data.",Single Citance
257,J96-3004.xml,W06-1630.xml,"one can either only use the phonetic model, which does not depend on the sample size; or else one must expand the data set and hope for more occurrence.","To generate the Hindi and Arabic candidates, all words from the same seven days were extracted.","The words were stemmed all possible ways using simple hand-developed affix lists: for example, given a Hindi word c1 c2 c3 , if both c3 and c2 c3 are in our suffix and ending list, then this single word generates three possible candidates: c1 , c1 c2 , and c1c2 c3 . In contrast, Chinese candidates were extracted using a list of 495 characters that are frequently used for foreign names (Sproat et al., 1996).",A sequence of three or more such characters from the list is taken as a possible name.,The number of candidates for each target language is presented in the last column of Table 5.,Single Citance
258,J96-3004.xml,W10-3212.xml,This word segmentation model for Urdu OCR system takes input in the form of a sequence of ligatures recognized by an OCR to construct a sequence of words from them.,"Many languages, e.g., English, French, Hindi, Nepali, Sinhala, Bengali, Greek, Russian, etc. segment text into a sequence of words using delimiters such as space, comma and semi colon etc., but on the other hand many Asian languages like Urdu, Persian, Arabic, Chinese, Dzongkha, Lao and Thai have no explicit word boundaries.","In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches.",These techniques segment text using the lexicon.,"Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 2122 August 2010.",Multi Citance
259,J96-3004.xml,W10-3708.xml,They further argued that IR often benefits from splitting compound words that are annotated as single units by manual segmentation.,The essence of the problem is that there is no clear definition of word in Chinese.,"Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).","While units such as “ ” (peanut) and “ ” (match maker) should clearly be considered as a single term in Chinese IR, compounds such as “JLRq:>J” (machine learning) are more controversial.1 Xu et al.(2009) proposed a “continuum hypothesis” that rejects a clean binary classification of Chinese semantic units as either compositional or non-compositional.Instead, they introduced the notion of a tightness measure, which quantifies the degree of compositionality.","On this tightness continuum, at one extreme are non 1 This issue is also present to a certain degree in languages that do use explicit delimiters, including English (Halpern, 2000; McCarthy et al., 2003; Guenthner and Blanco, 2004).",Single Citance
260,J96-3004.xml,W11-0823.xml,"Obviously, white space makes the task much easier than it would be otherwise.",There is a considerable literature on word breaking in Chinese and Japanese which is considerably more challenging than English largely because there is no white space in Chinese and Japanese.,There are a number of popular dictionary-based solutions such as Cha Sen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information.,The situation may not be all that different in English.,English is full of multiword expressions.,Single Citance
261,J96-3004.xml,W12-1011.xml,"As mentioned in Section 2, a common test for word segmentation is “compositionality of meaning”.","While there are clear-cut cases like sha men, many cases fall in the grey area.","Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996).","It is not surprising, then, that a myriad of guidelines for word segmentation have been proposed for various corpora of Modern Chinese (Liu et al., 1994; Chinese Knowledge Information Processing Group, 1996; Yu et al., 1998; Xia 2000; Sproat and Emerson, 2003).","In the rest of this section, we first review the approaches taken in three classical Chinese corpora, developed respectively at Jiaotong University (Huang et al., 2006), University of Sheffield (Hu et al., 2005) and the Academia Sinica (Wei et al., 1997).",Single Citance
262,J96-3004.xml,W12-1011.xml,"Among these, 75% consists of two characters.",Disagreement rate on the presence of word boundary between characters was only 1.7%.,"No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996).",This may be explained by the relatively small number of types of strings (see Table 2) that are considered to be multi-character words in our corpus.,POS tagging on strings without internal structures.,Single Citance
263,J96-3004.xml,W12-2303.xml,"In order to utilize increasingly available language resources such as user contributed annotations and web lexicon and/or to dynamically construct models for new domains, we have to either frequently rebuild models or rely on techniques such as incremental learning and transfer learning, which are unsolved problems themselves.","In the case of frequent model rebuilding, the most efficient approach is the tokenization model (using the terminology in Huang et al., 2007), in which the retraining is just the update of the dictionary and the segmentation is a greedy string matching procedure using the dictionary and some disambiguation heuristics, e.g. Liang (1986) and Wang et al.(1991).","An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al.(1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.","However, all the methods mentioned above are mostly based on the knowledge of in-vocabulary words and usually suffer from poor performance, as the out of-vocabulary words (OOV) rather than segmentation ambiguities turn out to the dominant error source for word segmentation on real corpora (Huang and Zhao, 2007).",This fact has led to a shift of the research focus to modeling the roles of individual characters in the word formation process to tackle the OOV problem.,Multi Citance
264,J96-3004.xml,W12-2303.xml,"And the probability of the label assignment for a character token is mostly determined by its features, which are usually local contexts in the form of character co-occurrences.",There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field.,"For example, the Sproat et al.(1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.","Another typical example is Ma and Chen (2003), which proposed context free grammar like rules together with a recursive bottom-up merge algorithm that merges possible morphemes after an initial segmentation using maximum matching.","It would be fairer to compare the OOV recognition performance of our approach with these methods, rather than maximum matching.",Pre Contiguous
265,J96-3004.xml,W97-0120.xml,"[Chang et al., 1991 combined a small seed segmented corpus and a large unsegmented corpus to build a word unigraJ model using the Viterbi re-estimation.","[Luo and Roukos, 1996] proposed are-estimation procedw which alternates word segmentation and word frequency re-estimation on each half of the tra.inil:J text divided into halves.","One of the major problems in unsupervised word segmentation is the treatment of unseen word [Sproat et al., 1996] wrote lexical rules for each productive morphological process, such as plur noun formation, Chinese personal names, and transliterations of foreign words.","[Chang et al., 1991 used a statistical method called ""Two-Class Classifier'', which decided whether the string is actual1 a word based on the features derived from character N-gram.","In this paper, we present a self-organized method to build a Japanese word segmenter frOJ a small number of basic words and a large amount of unsegmented training text using a noV re-estimation procedure.",Single Citance
266,J96-3004.xml,W97-0120.xml,"In both Japanese and Chinese, one of the most popular non-stochastic dictionary-based approaches is the longest match method 1.","There are many variations of the longest match method, possibly augmented with further heuristics.","We used a simple greedy algorithm described in [Sproat et al., 1996].","It starts at the beginning of the sentence, finds the longest word starting at that point, and then repeats the process starting at the next character until the end of the sentence is reached.",We chose the greedy algorithm because it is easy to implement and guaranteed to produce only one segmentation.,Single Citance
267,J96-3004.xml,W97-0120.xml,We chose the greedy algorithm because it is easy to implement and guaranteed to produce only one segmentation.,3.2 String Frequency.,"[Sproat et al., 1996] also proposed another method to estimate a set of initial word frequencies without segmenting the corpus.",It derives the initial estimates from the frequencies in the corpus of the strings of character making up each word in the dictionary whether or not each string is actually an instance of the word in question.,The total number of words in the corpus is derived simply by summing the string frequency of each word in the dictionary.,Single Citance
268,J96-3004.xml,W97-0120.xml,"The longest match string frequency (lsf) method considers all possible longest matches in the text, while the greedy longest match (lm) algorithm considers only one possibility.",It is obvious that the longest match string frequency method remedies the problem that the string frequency (sf) method consistently and inappropriately favors short words.,"The problem of the longest match string frequency method is that if a word W1 is a substring of other word w2 and if wl always appears as a substring of w2 in the training text, just like m 1Although (Sproat et al., 1996] calls it &quot;maximum matching&quot;, we call this method &quot;longest match&quot; according to a review on Chinese word segmentation [Wu and Tseng, 1993) and the literal translation of the Japanese name of the method Hi!:.","longest match (1m-)----+- --+ 9t ?' C: :: L'""Lm o--------- string frequency (sf) longest match string frequency (lsf) II II IJ IJ II II IJ II' . Figure 3: Comparison of the initial word frequency estimation methods and in the above example, the frequency estimate of W1 becomes 0.","Although this rarely happens for a large training text, we have to smooth the word frequencies.",Multi Citance
269,J96-3004.xml,W97-0120.xml,This is an example of the Zipf law.,5.2 Evaluation Measures.,"Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996).","Let the number of words in the manually segmented corpus be Std, the number of words in the output of the word segmenter be Sys, and the number of matched words be M. Recall is defined as M/Std, and precision is defined as M/Sys.",Since it is inconvenient to use both recall and precision all the we also use the F-mea.sure to indicate the overall performance.,Multi Citance
270,J96-3004.xml,W97-0316.xml,"In Chinese, the characters are called ideographs.",Automatic methods for correctly isolating words in a sentence -- a process called word segmentation -- is therefore an important and necessary first step to be taken before other analysis can begin.,"Many researchers have proposed practical methods to resolve this problem such as (Nie et al., 1995, Wu and Tsang, 1995, Jin &amp; Chen, 1996, Ponte &amp; Croft, 1996, Sproat et al., 1996, Sun et al., 1997).",Information retrieval (IR) deals with the problem of selecting relevant documents for a user need that is expressed in free text.,"The document collection is usually huge, of gigabyte size, and both queries and documents are domain unrestricted and unpredictable.",Pre Contiguous
271,J98-2005.xml,J01-2004.xml,The left­ factorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).,See that paper for more discussion of the benefits of 3 A PCFG is consistent or tight if there is no probability mass reserved for infinite trees.,Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.,All of the PCFGs that are used in this paper are estimated using the relative frequency estimator.,4 A leftmost derivation is a derivation in which the leftmost nonterminal is always expanded.,Single Citance
272,N01-1011.xml,J02-2003.xml,"Dunning (1993) argues for the use of G2 rather than X2, based on the claim that the sampling distribution of G2 approaches the true chi-square distribution quicker than the sampling distribution of X2.","However, Agresti (1996, page 34) makes the opposite claim: “The sampling distributions of X2 and G2 get closer to chi-squared as the sample size n increases....","In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic.","Finally, the results of the pseudo-disambiguation experiments presented in Section 7 are at least as good, if not better, when using X2 rather than G2, and so we conclude that the question of which statistic to use should be answered on a per application basis.","The procedure for finding a suitable class, cl, to generalize concept c in position r of verb v works as follows.",Multi Citance
273,N04-1038.xml,E12-1054.xml,"Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations.","Finally, we show that statistical and knowledge- based methods can be combined for increased performance.","Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and DeÂ´silets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004)","The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar.","We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010).",Multi Citance
274,N04-1038.xml,H05-1003.xml,(Soon et al. 2001) use WordNet to test the semantic compatibility of individual noun phrase pairs.,Ingeneral these approaches do not explore the possi bility of exploiting the global semantic context provided by the document as a whole.,"Recently Bean and Riloff (2004) have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.",Our central goal is to model semantic and coreference structures in such a way that we can take advantage of a semantic context larger than the individual noun phrase when making coreference decisions.,"Ideally, this model should make it possible to pick out important features in the context and to distinguish useful signals from background noise.",Single Citance
275,N04-1038.xml,N13-1104.xml,Inference Figure 1: Graphical representation of our model.,"Hyper- parameters, the stickiness factor, and the frame and event initial and transition distributions are not shown for clarity.","the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004).",3.3 Full generative story.,"To summarize, the distributions that are learned by our model are the default distributions PBKG(B), PF−INIT(F ), PE−INIT(E); the transition distri butions PF−TRAN(Fi+1|Fi), PE−TRAN(Ei+1|Ei); and the emission distributions PSLOT(S|E, A, B), PE−HEAD(e|E, B), PA−HEAD(a|S), PA−DEP(dep|S).We used additive smoothing with uniform Dirich let priors for all the multinomials.",Single Citance
276,N04-1038.xml,N13-1110.xml,"However, some paraphrases that might not be considered to be valid (e.g., under $200 and around $200) can be acceptable coreference relations.","Unlike Wang and CallisonBurch (2011), we do not work on document pairs but on sets of at least five (comparable) documents, and we do not require sentence alignment, but just verb alignment.",Another source of inspiration is the work by Bean and Riloff (2004).,"They use contextual roles (i.e., the role that an NP plays in an event) for extracting patterns that can be used in coreference resolution, showing the relevance of verbs in deciding on coreference between their arguments.","However, they use a very small corpus (two domains) and do not aim to build a dictionary.",Post Contiguous
277,N04-1038.xml,P05-1020.xml,"Previous attempts on bootstrapping coreference classifiers have only been mildly successful (e.g., Mu¨ ller et al.(2002)), and this is also an area that deserves further research.","To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., Harabagiu et al.","(2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).","Of course, we can also expand our pre-selected set of coreference systems via incorporating additional learning algorithms, clustering algorithms, and feature sets.","Once again, we may use previous work to guide our choices.",Multi Citance
278,N04-1038.xml,P06-1005.xml,"With such a set, we could extract fine-grained features, perhaps tied to individual words or paths.","For example, we could estimate the likelihood each noun belongs to a particular gender/number class by the proportion of times this noun was labelled as the antecedent for a pronoun of this particular gender/number.","Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005)",We address the drawbacks of these approaches Table 3: Gender classification performance (%) Cl as sif ierF Sc or e Be rg sm a (2 00 5) Co rp us ba se d 8 5 . 4 Be rg sm a (2 00 5) W eb ba se d 9 0 . 4 Be rg sm a (2 00 5) Co m bi ne d 9 2 . 2 D up lic ate d Co rp us ba se d 8 8 . 0 Co ref er en t Path ba se d 9 0 . 3 by using coreferent paths as the assumed resolutions in the bootstrapping.,"Because we can vary the threshold for defining a coreferent path, we can trade-off coverage for precision.",Multi Citance
279,N04-1038.xml,P06-1005.xml,"Kehler et al.(2004) saw no apparent gain from using semantic compatibility information, while Yang et al.(2005) saw about a 3% improvement with compatibility data acquired by searching on the world wide web.",Section 6 analyzes the contribution of MI to our system.,"Bean and Riloff (2004) used bootstrapping to extend their semantic compatibility model, which they called contextual-role knowledge, by identifying certain cases of easily-resolved anaphors and antecedents.",They give the example â€œMr.,"Bush disclosed the policy by reading it.â€ Once we identify that it and policy are coreferent, we include read:obj:policy as part of the compatibility model.",Post Contiguous
280,N04-1038.xml,P07-1067.xml,Their model assumes that a definite NP and its hypernym words usually co-occur in texts.,"Thus, for a definite-NP anaphor, a preceding NP that has a high co-occurrence statistics in a large corpus is preferred for the antecedent.",Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution.,They apply an IE component to unannotated texts to generate a set of extraction caseframes.,"Each caseframe represents a linguistic expression and a syntactic position, e.g. â€œmurder of &lt;NP&gt;â€, â€œkilled &lt;patient&gt;â€.",Post Contiguous
281,N04-1038.xml,P07-1068.xml,"In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks 536 and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.","As a result, researchers have re-adopted the once-popular knowledge-rich approach, investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al.(2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al.","(2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)).","Another type of semantic knowledge that has been employed by coreference resolvers is the semantic class (SC) of an NP, which can be used to disallow coreference between semantically incompatible NPs.","However, learning-based resolvers have not been able to benefit from having an SC agreement feature, presumably because the method used to compute the SC of an NP is too simplistic: while the SC of a proper name is computed fairly accurately using a named entity (NE) recognizer, many resolvers simply assign to a common noun the first (i.e., most frequent) WordNet sense as its SC (e.g., Soon et al.(2001), Markert and Nissim (2005)).",Multi Citance
282,N04-1038.xml,P08-1090.xml,They are extracted from hand-sorted (by topic) sets of documents using log-likelihood ratios.,"These terms can capture some narrative relations, but the model requires topic-sorted training data.",Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.,"Our work can be seen as an attempt to generalize the intuition of caseframes (finding an entire set of events 1 We analyzed FrameNet (Baker et al., 1998) for insight, but found that very few of the frames are event sequences of the type characterizing narratives and scripts.",rather than just pairs of related frames) and apply it to a different task (finding a coherent structured narrative in non-topic-specific text).,Single Citance
283,N04-1038.xml,P09-1068.xml,"Our previous work (Chambers and Jurafsky, 2008) relied on the intuition that in a coherent text, any two events that are about the same participants are likely to be part of the same story or narrative.","The model learned simple aspects of narrative structure (‘narrative chains’) by extracting events that share a single participant, the protagonist.","In this paper we extend this work to represent sets of situation-specific events not unlike scripts, caseframes (Bean and Riloff, 2004)","This paper shows that verbs in distinct narrative chains can be merged into an improved single narrative schema, while the shared arguments across verbs can provide rich information for inducing semantic roles.","602 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 602–610, Suntec, Singapore, 27 August 2009.",Single Citance
284,N04-1038.xml,P09-1074.xml,Conclusion #3: Assuming the availability of CEs unrealistically simplifies the coreference resolution task.,3.5 Anaphoricity Determination.,"Finally, several coreference systems have successfully incorporated anaphoricity determination modules (e.g. Ng and Cardie (2002a) and Bean and Riloff (2004)).",The goal of the module is to determine whether or not an NP is anaphoric.,"For example, pleonastic pronouns (e.g. it is raining) are special cases that do not require coreference resolution.",Single Citance
285,N04-1038.xml,P10-1142.xml,above which a pair of NPs is considered coreferent.,lae (2006)).,"The DempsterShafer rule (Dempster, 1968), which combines the positive and negative pairwise decisions to score a partition, is used by Kehler (1997) and Bean and Riloff (2004) to identify the most probable NP partition.",Some clustering algorithms bear a closer resemblance to the way a human creates coreference clusters.,"In these algorithms, not only are the NPs in a text processed in a left-to-right manner, the later coreference decisions are dependent on the earlier ones (Cardie and Wagstaff, 1999; Klenner and Ailloud, 2008).5 For example, to resolve an NP, N Pk , Cardie and Wagstaff’s algorithm considers each preceding NP, N Pj , as a candidate antecedent in a right-to-left order.",Multi Citance
286,N04-1038.xml,P11-1082.xml,"If this assumption fails, we will not create any features based on FrameNet for these two NPs.","To our knowledge, FrameNet has not been exploited for coreference resolution.","However, the use of related verbs is similar in spirit to Bean and Riloffâ€™s (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006).",3.2 World Knowledge from Annotated Data.,"Since world knowledge is needed for coreference resolution, a human annotator must have employed world knowledge when coreference-annotating a document.",Multi Citance
287,N04-1038.xml,P13-1121.xml,"Degree (kill, dobj) (wound, dobj) 0.82 Causal (kill, dobj) (die, nsubj) 0.80 Type (rise, dobj) (drop, prep to) 0.81 Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model.","In particular, they are (gov, role) pairs, where gov is a proposition-bearing element, and role is an approximation of a semantic role with gov as its head (See Figure 1 for examples).","Caseframes do not consider the dependents of the semantic role approximations.The use of caseframes is well grounded in a va riety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004)","Related semantic representations are popular in Case Grammar and its derivative formalisms such as frame semantics (Fillmore, 1982).",We use the following algorithm to extract case- frames from dependency parses.,Single Citance
288,N04-1038.xml,P13-2015.xml,"Ng (2007) used lexical information to assess the likelihood of a noun phrase being anaphoric, but this did not show clear improvements on ACE data.","There has been previous work on domain- specific coreference resolution for several domains, including biological literature (Castan˜ o et al., 2002; Liang and Lin, 2005; Gasperin and Briscoe, 2008; Kim et al., 2011; BatistaNavarro and Ananiadou, 2011), clinical medicine (He, 2007; Zheng et al., 2011; Glinos, 2011; Gooch and Roudsari, 2012) and legal documents (BouayadAgha et al., 2009).","In addition, BABAR (Bean and Riloff, 2004) used contextual role knowledge for coreference resolution in the domains of terrorism and natural disasters.","To the best of our knowledge, our work is the first to examine the impact of lexicalized features for domain-specific coreference resolution.","Table 1 shows the performance of a learning-based coreference resolver, Reconcile (Stoyanov et al., 2010), with its default feature set using different combinations of training and testing data.",Single Citance
289,N04-1038.xml,W05-0612.xml,Cardie and Wagstaff developed an unsupervised approach that partitions noun phrases into coreferent groups through clustering (1999).,"However, the partitions they generate for a particular document are not useful for processing new documents, while our approach learns distributions that can be used on unseen data.","There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al., 1998), or contextual role-knowledge (Bean and Riloff, 2004).","Co-training can also leverage unlabeled data through weakly-supervised reference resolution learning (Mu¨ ller et al., 2002).","As an alternative to co-training, Ng and Cardie (2003) use EM to augment a supervised coreference system with unlabeled data.",Multi Citance
290,N04-1038.xml,W06-0106.xml,"In addition, several unsupervised approaches have been proposed.",Cardie and Wagstaff (1999) recast the problem as a clustering task which applied a set of incompatibility functions and weights in the distance metric.,Bean and Riloff (2004) used information extraction patterns to identify contextual clues that would determine the compatibility between NPs.,All of the previously mentioned work has been for English.,There has been relatively little work in Chinese: Florian et al.(2004) provides results using a language-independent framework on the Entity Detection and Tracking task (EDT).,Single Citance
291,N04-1038.xml,W06-0206.xml,This problem is often referred to as semi-supervised learning.,It significantly reduces the effort needed to develop a training set.,"It has shown promise in improving the performance of many tasks such as name tagging (Miller et al., 2004), semantic class extraction (Lin et al., 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004)","However, it is not clear, when semi-supervised learning is applied to improve a learner, how the system should effectively select unlabeled data, and how the size and relevance of data impact the performance.",In this paper we apply two semi-supervised learning algorithms to improve a state-of-the-art name tagger.,Multi Citance
292,N04-1038.xml,W10-3909.xml,"People can still resolve these correctly because “terrorist” is more likely to be arrested than “boy”, and because the one shooting is more likely to be arrested than the one being shot.","In such cases, semantic constraints and preferences are required for correct coreference resolution.","Methods for acquiring and using such knowledge are receiving increasing attention in 60 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 60â€“68, Beijing, August 2010 recent work on anaphora resolution.","However, this task is difficult because it requires the acquisition of a large amount of semantic information.","Furthermore, there is not universal agreement on the value of these semantic preferences for pronoun coreference.",Multi Citance
293,N04-1038.xml,W10-3909.xml,Contextual compatibility features have long been studied for pronoun coreference: Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution.,It determined the preference of candidates based on predicate-argument frequencies.,"Bean and Riloff (2004) present a system, which uses contextual role knowledge to aid coreference resolution.",They used lexical and syntactic heuristics to identify high-confidence coreference relations and used them as training data for learning contextual role knowledge.,"They got substantial gains on articles in two specific domains, terrorism and natural disasters.",Post Contiguous
294,N04-1038.xml,W10-3909.xml,"Chambers and Jurafsky (2008) built narrative event chains, which are partially ordered sets of events related by a common protagonist.","They use high-precision hand-coded rules to get coreference information, extract predicate arguments that link the mentions to verbs, and link the arguments of the coreferred mentions to build a verb entailment model.","Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.",Their result raises the natural question as to whether the approach (which may capture domain- specific pairs such as “kidnap—release” in the terrorism domain) can be successfully extended to a general news corpus.,We address this question in the experiments reported here.,Single Citance
295,N06-2049.xml,D13-1031.xml,Table 10 listed the results.,Best05 represents the best system reported on the Second International Chinese Word Segmentation Bakeoﬀ.,CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006).,Other three systems all represent the methods using their cor responding model in the corresponding papers.,"Note that these state-of-art systems are eitherusing complicated models with semi-Markov relaxations or latent variables, or modifying mod els to ﬁt special conditions.",Single Citance
296,N06-2049.xml,I08-4009.xml,"Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006.","Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test.","Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).","Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).",The basic assumption of such combination is that DS method performs better on IV words and Zhang derives this belief from the fact that DS achieves higher IV recall rate as Table 1 shows.,Multi Citance
297,N06-2049.xml,I08-4009.xml,"Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test.","Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).","Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).",The basic assumption of such combination is that DS method performs better on IV words and Zhang derives this belief from the fact that DS achieves higher IV recall rate as Table 1 shows.,"In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail).",Single Citance
298,N06-2049.xml,I08-4015.xml,"Thus, we use the trigram language model to select top B (B is a constant predefined before search and in our experiment 3 is used) best candidates with highest probability at each stage so that the search algorithm can work in practice.","Finally, when the whole sentence has been read, the best candidate with the highest probability will be selected as the segmentation result.","After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).","Since word-based segmentation result also corresponds to a tag sequence according to the 6-tag set, we now have two tags for each character, word-based tag (WT) and CRF tag (CT).","Which tag will be kept as the final result depends on Marginal Probability (MP) of the CT. Here, we give a short explanation about what is the MP of the CT. Suppose there is a sentence C  c0c1...cM , where ciis the character this sen word, else F0 = 0 and F1 = 1 if and only if C0 C1 tence containing.",Single Citance
299,N06-2049.xml,I08-4030.xml,"In CRF models, a margin of each character can be gotten, and the margin could be considered as the confidence of that character.","For the Segmentation task, we performed the Maximum Probability Segmentation first, through which each character is assigned a BIO tag (B represents the Beginning of a word, I represents In a word and O represents Out of a word).","If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).","Conditional Random Fields (CRFs) are a class of undirected graphical models with exponent distribution (Lafferty et al., 2001).","A common used special case of CRFs is linear chain, which has a distribution of: T lems.",Single Citance
300,N06-2049.xml,I08-4030.xml,"If the probability of y with the largest probability is lower than 0.75, which is decided according to the experiment results, the tag given by Maximum Probability Seg mentation will be used instead of tag given by CRF.",The motivation of this method is to use the Maximum Probability method to enhance the F-measure of In-Vocabulary (IV) Words.,"According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.",One simplest way to combine them is the method we described.,"Besides, there are some complex methods, such as estimation using Support Vector Machine (SVM) for CRF, CRF combining boosting and combining Margin Infused Relaxed Algorithm (MIRA) with CRF, that might perform better.",Single Citance
301,N06-2049.xml,J11-1005.xml,"Different encodings were provided, and the UTF8 data for all four corpora were used in our experiments.","Following the format of Table 4, the results for this bakeoff are shown in Table 5.","We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.","Row “Zh-a” and “Zh-b” represent the pure sub-word CRF model and the confidence-based combination of the CRF and rule-based models, respectively.","Again, our model achieved better overall accuracy than all the other models.",Multi Citance
302,N06-2049.xml,N09-1007.xml,"For each group, therows marked by ∗ represent our models with hy brid word/character information.","Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006)10, which was also used in Gao et al.","Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);","(2005); C05 represents the system in Chen et al. 10 It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models (Andrew, 2006).","However, in general, as discussed in Andrew (2006), it is essentially still a semi-CRF model.",Single Citance
303,N06-2049.xml,P07-1106.xml,"Wang et al.(2006) incorporates an N-gram language model in M E tagging, making use of word information to improve the character tagging model.",The key difference between our model and the above models is the word- based nature of our system.,"One existing method that is based on sub-word information, Zhang et al.(2006), combines a C R F and a rule-based model.","Unlike the character-tagging models, the C R F submodel assigns tags to sub- words, which include single-character words and the most frequent multiple-character words from the training corpus.",Thus it can be seen as a step towards a word-based model.,Single Citance
304,N06-2049.xml,P07-1106.xml,"Different encodings were provided, and the UTF8 data for all four corpora were used in this experiment.","Following the format of Table 5, the results for this bakeoff are shown in Table 6.","We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.(2006) for comparison.","Row “Zh-a” and “Zh-b” represent the pure sub-word C R F model and the confidence-based combination of the C R F and rule-based models, respectively.","Again, our model achieved better overall accuracy than the majority of the other models.",Multi Citance
305,N06-2049.xml,P12-1027.xml,of-the-art systems reported in the previous papers.,The statistics are listed in Table 3.,"Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.(2006).",Prob.,indicates whether or not the system can provide probabilistic information.,Single Citance
306,N06-2049.xml,W06-0118.xml,The sections below provide a detailed description of the system and our experimental results.,"In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate a segmentation result.","Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)",The maximum matching algorithm is a greedy segmentation approach.,"It proceeds through the sentence, mapping the longest word at each point with an entry in the dictionary.",Single Citance
307,N06-2049.xml,W06-0118.xml,"In this approach, most existing methods use the character-based IOB tagging.","For example, “;g(all) 3'::KJ!!�(extremely important)” is labeled as “;g(all)/O 3'(until)/B ::K(close)/I J!!(heavy)/I �(demand)/I”.","Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.","In this method, all single-hanzi words and the top frequently occurring multihanzi words are extracted from the training corpus to form the lexicon subset.Then, each word in the training corpus is segmented for IOB tagging, with the forward maximum matching algorithm, using the formed lexicon subset as the dictionary.","In the above example, the tagging labels become “;g(all)/O3'(until)/B ::K(close)/I J!!",Single Citance
308,N06-2049.xml,W08-0335.xml,This enabled us to examine the effect of CWS specifications on SMT.,"We used a Chinese word segmentation tool, Achilles, to implement word segmentation.","Part of the work using this tool was described by (Zhang et al., 2006).","Moreover, this tool meets our need to test the effect of the two kinds of CWS approaches for SMT.",We can easily train a dictionary-based and a CRF-based CWS by using this tool.,Single Citance
309,N06-2049.xml,W08-0335.xml,"This CWS was then used to segment the SMT training corpus and then we extracted a lexicon of 100,000 from the top frequent words of the segmented SMT corpus.",This lexicon was used as the lexicon of the “dict-hybrid.” The LM of “dict- hybrid” was also trained on the segmented corpus.,"Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.","This segmentation generated 49,546,231 tokens, 112,072 unique words for the training data and 693 OOVs for the test data.",The segmentation data were used for training a new SMT model.,Single Citance
310,N06-2049.xml,W10-4128.xml,Chinese Word Segmentation (CWS) has witnessed a prominent progress in the first four SIGHAN Bakeoffs.,"Since Xue (2003) used character-based tagging, this method has attracted more and more attention.","Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.",Because the word-based models can capture the word-level contextual information and IV knowledge.,"Besides, many strategies are proposed to balance the IV and OOV performance (Wang et al., 2008).",Multi Citance
311,N06-2049.xml,W10-4135.xml,"In this word segmentation competition, unfortunately, only a small size of unlabeled target domain data is available.",Thus we focus on handling out-of-vocabulary (OOV) words.,"For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).","In more detail, we adopted and extended subword-based method.",Subword list is augmented with new- word list recognized by accessor variety method.,Multi Citance
312,N06-2049.xml,W10-4135.xml,"Moreover, single- character words 1, if they are not contained in the subword list, are also added.","Such proce 1 By single-character word, we refer to words that consist solely of a Chinese character.","Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).","To enhance the effect of subwords, we go one step further to build a list, named confident-word list here and below, which contains words that are not a portion of other words and are never segmented in the training data.","In the competition, 400 most frequent words in the confident-word list are used.",Single Citance
313,N06-2049.xml,W10-4135.xml,"With subword list and confident-word list, both training and test data are segmented with forward maximum match method by using the union of subword list and confident-word list.",Each segmentation unit (single-character or multi- character unit) in the segmentation results are regarded as “pseudo character” and thus can be represented with the basic features in Table 1 and two additional features as shown in Table 2.,"See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)","Accessor variety (AV) (Feng et al., 2004) is a simple and effective unsupervised method for extraction of new Chinese words.","Given a unsegmented text, each substring (candidate word) in the text can be assigned a value according to the following equation: AV (s) = min{Lav (s), Rav (s)} (1) where the left and right AV values, Lav (s) and Rav (s) are defined to be the number of distinct character types appearing on the left and right, respectively.",Single Citance
314,N06-2049.xml,W10-4138.xml,"Both of them occur 73 times, which is a large number for such a small corpus.","Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.","This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.(2006) were proposed.","We uses suffix array algorithm to calculate exact boundaries of phrase and their frequencies (Sung et al., 2008), called term contributed boundaries (TCB) and term contributed fre quencies (TCF), respectively, to analogize similarities and differences with the term frequencies (TF).","For example, in Vodis Corpus, the original TF of the term “RAIL ENQUIRIES” is 73.",Pre Contiguous
315,N09-1001.xml,N09_csl2013.xml,"We should also note that while improvement is experienced for both the objective and subjective classes, a major gain is observed for the latter.","We are not aware of any other work that considered the task of word sense subjectivity labeling in a cross-lingual setting, and thus no direct comparison with previous work can be performed.","The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al.			(2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words.","These results are however not directly comparable to ours, as they are applied on different datasets, with different levels of difficulty.","0.8 EnRo-ML-overall E n C L o v e r a l l R o C L o v e r a l l 0.7 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of iterations Figure 5: Macro-accuracy for multilingual bootstrapping (versus cross-lingual framework) 0.9 0.8 E n R o M L o b j E n R o M L s u b j E n C L o b j E n C L s u b j R o C L o b j R o C L s u b j 0.7 0.6 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Number of iterations Figure 6: F-measure for the objective and subjective classes for multilingual bootstrapping (versus cross-lingual framework) Monolingual Features Multilingual Features English Romanian English & Romanian feeling state quality mental feel emotion emotional pain no hit good mind great self regard important judgment lack true suffering lacking opinion statement trait disposition concern extreme felt distress social pleasure intense belief danger feelings argument personal attitude sentiment (En: feeling) stare (En: state) lips˘a (En: lack) atitudine (En: attitude) suferin¸ta˘ (En: suffering) boal˘a (En: disease) sim¸ti (En: feel) idee (En: idea) anumit (En, adj.",Multi Citance
316,N09-1001.xml,N09prod.xml,"Pang[2] used Naive Bayes (NB), Max entropy (ME) and SVM separately as supervised learning algorithms for binary classification of reviews.","Besides these, Pang[3] exploited regression and One-Vs-All SVMs to predict the score for classification, namely the multi-classification, which can also be realized by combining binary classifiers in a two-stage manner.",Pang[4] and Su[5] both optimized their multi-classification results using the Mincut model.,"For sentiment classification in microblogs, Go[6] was the first to classify Twitter data into two classes (positive and negative).Barbosa[7] classified Twitter data into three classes in a two-stage manner.",Jiang[8] analyzed topic related sentiment for Twitter text.,Multi Citance
317,N09-1001.xml,N09prod.xml,Do binary classification on the test data which is correctly divided into subject class in Step 1 for positive and negative detection.,4.3 Minimum Cut Model Optimization.,"Inspired by work of Pang[4] and Su[5], we also use Minimum cut (Mincut) model to optimize the Two-stage SVM result.",Binary classification with Mincut in graph is based on the idea that similar items should be split in the same cut.,"We build an undirected graph G with vertices {s, t, v1, v2, …, vn}; s is the source and t is the sink, and all items in the test data are seen as vertices vi.",Multi Citance
318,N09-1001.xml,N15-1071.xml,"Some of the neutral-to-positive or neutral-to- negative transitions don’t seem well-motivated and may be regarded as artifacts from the crowdsourcing, as does n’t, is n’t and are n’t are negative in SSTb whereas ’s not, do n’t and did n’t get a neutral label.","In HeiST, nicht immer (not always) as well as nicht ganz (not quite) are negative, whereas auch nicht (neither) and nicht so (not as) or nicht unbedingt (not necessarily) are neutral.","The MWE productions seem to overlap with well- known linguistic phenomena  consider Fahrni and Klenner (2008) and their claim that most adjectives have a polarity that is dependent on the target they modify instead of having a prior polarity that holds independently of the target, or the observation of Su and Markert (2009) that sentiment should be dependent on word senses instead of word forms (which would capture a large number of examples within the expression strengthening category).","Yet, others may be idiosyncracies introduced by the crowdsourcing process, and powerful learners such as RNTN or the approach of Hall et al.(2014) will gain performance from simply memorizing the idiosyncracies of the data when there is Ty pe Total S V M C o rr Prec C L S A C o rr Prec R N T N C o rr Prec +r ep lace go ld C or r Prec AV G 6341 M W E 1638 IN T 612 ID 392 IN V 259 34 08 0.546 3 6 9 0.225 3 7 0 0.605 2 8 3 0.722 7 6 0.293 31 58 0.506 5 3 8 0.328 3 6 2 0.592 2 6 9 0.686 6 5 0.251 36 04 0.577 5 3 8 0.359 4 1 3 0.675 2 8 6 0.730 9 3 0.359 43 09 0.690 5 4 6 0.333 4 7 0 0.768 3 2 3 0.824 7 9 0.305 Table 7: Precision of rules with non-neutral parent label (ID: daughters and parent have identical labels) enough of it – because of the way the Stanford Sentiment Treebank is constructed, phrases always have the same (context-independent) label, while we may get a more accurate (and possibly different) picture from introducing additional means of quality control (which in turn increases the necessary investment for such a sentiment treebank).","In table 7, we tabulated the classification accuracy for the parent node in different types of productions in HeiST.",Multi Citance
319,N09-1001.xml,P101167.xml,"Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or objective, utilizing the MPQA corpus.",They show that subjectivity information for Word- Net senses can improve word sense disambiguation tasks for subjectivity ambiguous words.,Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information.,Their method requires less training data other than the sense definitions and relational structure of WordNet.2.5 Word Polarity Classification for Foreign Languages.,Word sentiment and subjectivity has also been studied for languages other than English.,Single Citance
320,N09-1001.xml,PEAAI_n09.xml,"Features used were high- frequency words, content words, sentence length and punctuation.","Results on the Twitter dataset are better than those obtained on the Amazon dataset, with accuracy of 0.947 with a k-nearest neighbors implementation.",Semi-supervised techniques on text mining were applied by Fangzhong and Markert (2009).,"Their approach involves Wordnet, like us, and they propose a subjectivity measure of each Wordnet entry.",They suggest a semi-supervised minimum-cut framework that makes use of both WordNet deﬁnitions and its relation structure.,Single Citance
321,N09-1001.xml,Pproc2014_n09.xml,"The creation, gain, and benefit classes are +effect events.","For example, baking a cake has a positive effect on the cake because it is created;3 increasing the tax rate has a positive effect on the tax rate; and comforting the child has a positive effect on the child.","The antonymous classes of each are -effect events: destroying the building has a negative effect on the building; demand decreasing has a negative effect on demand; and killing Bill has a negative effect on Bill.4 While sentiment (Esuli and Sebastiani, 2006; Wilson et al., 2005; Su and Markert, 2009) and connotation lexicons (Feng et al., 2011; Kang et al., 2014) are related, sentiment, connotation, and +/-effects are not the same; a single event may have different sentiment and +/-effect polarities, for example.","Consider the following example: perpetrate: S: (v) perpetrate, commit, pull (perform an act, usually with a negative connotation) “perpetrate a crime”; “pull a bank robbery” This sense of perpetuate has a negative connotation, and is an objective term in SentiWordNet.However, it has a positive effect on the object, a crime, since performing a crime brings it into existence.",3 Deng et al.(2013) point out that +/-effect objects are not equivalent to benefactive/malefactive semantic roles.,Multi Citance
322,N09-1001.xml,Pproc2014_n09.xml,"Also, the similarity of glosses in WordNet is considered.","Even though they investigated the hierarchical structure by LCS values, WordNet relations are not exploited directly.",Su and Markert (2009) adopt a semi-supervised mincut method to recognize the subjectivity of word senses.,"To construct a graph, each node corresponds to one WordNet sense and is connected to two classification nodes (one for subjectivity and another for objectivity) via a weighted edge that is assigned by a classifier.","For this classifier, WordNet glosses, relations, and monosemous features are considered.",Single Citance
323,N09-1001.xml,W11-0311.xml,This indicates that non-expert MTurk annotations can replace expert annotations for our end-goal – improving contextual opinion analysis – while reducing time and cost requirements by a large margin.,"Moreover, we see that the improvements in (Akkaya et al., 2009) scale up to new subjectivity clues.","One related line of research is to automatically assign subjectivity and/or polarity labels to word senses in a dictionary (Valitutti et al., 2004; An- dreevskaia and Bergler, 2006; Wiebe and Mihalcea, 2006; Esuli and Sebastiani, 2007; Su and Markert, 2009).","In contrast, the task in our paper is to automatically assign labels to word instances in a corpus.","Recently, some researchers have exploited full word sense disambiguation in methods for opinion- related tasks.",Multi Citance
324,N09-1025.xml,C10-2052.xml,"While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions.",We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative.,"We explore the different options for context and feature se 1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity.","Using the resulting parameters in a Maximum Entropy classifier, we are able to improve significantly over existing results.",The general outline we present can potentially be extended to other word classes and improve WSD in general.Rudzicz and Mokhov (2003) use syntactic and lexical features from the governor and the preposition itself in coarse-grained PP classification with decision heuristics.,Multi Citance
325,N09-1025.xml,D11-1081.xml,"With millions of features trained on 519K sentences in 0.03 second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST ChineseEnglish test sets.","Discriminative model (Och and Ney, 2002) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.","Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al., 2006; Tillmann and Zhang, 2006; Watanabe et al., 2007; Blunsom et al., 2008; Chiang et al., 2009).","However, the training of the large number of features was always restricted in fairly small data sets.","Some systems limit the number of training examples, while others use short sentences to maintain efficiency.",Multi Citance
326,N09-1025.xml,D11-1081.xml,"However, the training of the large number of features was always restricted in fairly small data sets.","Some systems limit the number of training examples, while others use short sentences to maintain efficiency.","Overfitting problem often comes when training many features on a small data (Watanabe et al., 880 2007; Chiang et al., 2009).","Obviously, using much more data can alleviate such problem.","Furthermore, large data also enables us to globally train millions of sparse lexical features which offer accurate clues for SMT.",Multi Citance
327,N09-1025.xml,D11-1081.xml,(1) Solid hyperedges denote a “reference” derivation tree t1 which exactly yields the reference translation.,"(2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2 , which fails to swap the order of X3,4 .","(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3 . Generally, this is done by deleting a node X0,1 . ing to constructing the oracle reference (Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2009), which is nontrivial for SMT and needs to be determined experimentally.","Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5).","Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data.",Multi Citance
328,N09-1025.xml,D11-1081.xml,Perceptron.,We also learn thousands of context word features together with the 8 traditional features on a small data using perceptron.,"Following (Chiang et al., 2009), we only use 100 most frequent words for word context feature.","This setting use CKY de T R A I N R T R AI N D EV T E ST # S e n t . # W or d A v g . L e n . L o n . L e n . 51 9, 35 9 8 . 6 M 1 6 . 5 9 9 1 8 6, 8 1 0 1 . 3 M 7 . 3 9 5 8 7 8 2 3 K 2 6.","4 7 7 3, 7 8 9 1 0 5 K 2 8 . 0 1 1 6 Table 1: Corpus statistics of Chinese side, where Sent., Avg., Lon., and Len.",Single Citance
329,N09-1025.xml,D11-1081.xml,"For simplicity we do not compress the generated derivations into forests, therefore the size of resulting derivations is fairly small, which is about 265.8 for each sentence on average, where 6.1 of them are reference derivations.","Furthermore, we use lexical- ize operator more often than generalize operator (the ration between them is 1.5 to 1).","Lexicalize operator is used more frequently mainly dues to that the reference derivations are initialized with reusable (thus Chiang et al., 2009), gradient descent (Blunsom et al., 2008; Blunsom and Osborne, 2008).",The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data.,"For efficiency, we only use neighboring derivations for training.",Multi Citance
330,N09-1025.xml,D11-1125.xml,Several researchers have attempted to address this weakness.,"Recently, Watanabe et al.(2007) and Chiang et al.(2008b) have developed tuning methods using the MIRA algorithm (Crammer and Singer, 2003) as a nucleus.",The MIRA technique of Chiang et al. has been shown to perform well on large-scale tasks with hundreds or thousands of features (2009).,"However, the technique is complex and architecturally quite different from MERT.","Tellingly, in the entire proceedings of ACL 2010 (Hajicˇ et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT.1 Here we propose a simpler approach to tuning that scales similarly to high-dimensional feature spaces.",Single Citance
331,N09-1025.xml,D11-1125.xml,"If MERT cannot scale in this simple scenario, it has little hope of succeeding in a high-dimensionality deployment scenario.",We would like to modify MERT so that it scales well to high-dimensionality candidate spaces.,The most prominent example of a tuning method that performs well on high-dimensionality candidate spaces is the MIRA-based approach used by Watanabe et al.(2007) and Chiang et al.(2008b; 2009).,"Unfortunately, this approach requires a complex architecture that diverges significantly from the MERT approach, and consequently has not been widely adopted.",Our goal is to achieve the same performance with minimal modification to MERT.,Single Citance
332,N09-1025.xml,D11-1125.xml,"Specifically, we use 15 baseline features for PBMT, similar to the baseline features described by Watanabe et al.(2007).","We use 19 baseline features for SBMT, similar to the baseline features described by Chiang et al.(2008b).","We used the following feature classes in SBMT and PBMT extended scenarios:  Discount features for rule frequency bins (cf. Chiang et al. (2009), Section 4.1)  Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.","Chiang et al.(2009), Section 4.1):10 • Rule overlap features • Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.","minal set than that used for the other two SBMT systems, accounting for the wide disparity in feature count for these feature classes.",Single Citance
333,N09-1025.xml,D11-1125.xml,"We use 19 baseline features for SBMT, similar to the baseline features described by Chiang et al.(2008b).","We used the following feature classes in SBMT and PBMT extended scenarios: • Discount features for rule frequency bins (cf.Chiang et al.(2009), Section 4.1) • Target word insertion features9 We used the following feature classes in SBMT extended scenarios only (cf.","Chiang et al. (2009), Section 4.1):10  Rule overlap features  Node count features 9 For ChineseEnglish and UrduEnglish SBMT these features only fired when the inserted target word was unaligned to any source word.","minal set than that used for the other two SBMT systems, accounting for the wide disparity in feature count for these feature classes.",UrduEnglish SBMT baseline feature tuning 26 25 MERT.,Single Citance
334,N09-1025.xml,D11-1125.xml,Each of the three approaches we compare in this study has various details associated with it that may prove useful to those wishing to reproduce our results.,"We list choices made for the various tuning methods here, and note that all our decisions were made in keeping with best practices for each algorithm.","5.4.1 MERT We used David Chiangs CMERT implementation We for the most part follow the MIRA algorithm for machine translation as described by Chiang et al. (2009)12 but instead of using the 10-best of each of the best hw , hw +g, and hw -g, we use the 30-best according to hw .13 We use the same sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al. (2008b; 2009). ",We ran MIRA for 30 iterations.,5.4.3 PRO We used the MegaM classifier and sampled as described in Section 4.2.,Single Citance
335,N09-1025.xml,N12-1006.xml,"We used GIZA++ (Och and Ney, 2003) to align sentences and extract hierarchical rules.","The decoder used a log-linear model that combines the scores of multiple feature scores, including translation probabilities, smoothed lexical probabilities, a dependency tree language model, in addition to a trigram English language model.","Additionally, we used 50,000 sparse, binary-valued source and target features based on Chiang et al. (2009) ",The English language model was trained on 7 billion words from the Gigaword and from a web crawl.,"The feature weights were tuned to maximize the BLEU score on a tuning set using the Expected- BLEU optimization procedure (Devlin, 2009).",Single Citance
336,N09-1025.xml,P12-1001.xml,General PMO Approach: The strategy we outlined in Section 3.2 can be easily applied to other MT optimization techniques.,"For example, by replacing the optimization subroutine (line 10, Algorithm 2) with a Powell search (Och, 2003), one can get PMOMERT4.","Alternatively, by using the large- margin optimizer in (Chiang et al., 2009) and moving it into the for-each loop (lines 49), one can get an online algorithm such PMOMIRA.",Virtually all MT optimization algorithms have a place where metric scores feedback into the optimization procedure; the idea of PMO is to replace these raw scores with labels derived from Pareto optimality.,4.1 Evaluation Methodology.,Single Citance
337,N09-1025.xml,P13-1110.xml,RAMPION aims to address the disconnect between MT and machine learning by optimizing a structured ramp loss with a concave-convex procedure.,2.1 Large-Margin Learning.,"Online large-margin algorithms, such as MIRA, have also gained prominence in SMT, thanks to their ability to learn models in high-dimensional feature spaces (Watanabe et al., 2007; Chiang et al., 2009).",The usual presentation of MIRA’s optimization problem is given as a quadratic program: 1 https://github.com/veidel/cdec 2 We may omit d in some equations.,for clarity.,Multi Citance
338,N09-1025.xml,P13-1110.xml,The vertical gray dotted line depicts the bound B. White arrows indicate updates triggered by constraint violations.,Squares are data points in the k-best list not selected for update in this round.,"task Corpus Sentences Tokens En Zh/Ar MT05 1082 35k 33k training 1M 23.7M 22.8M tune (MT06) 1797 55k 49k baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and RAMPION (Gimpel and Smith, 2012).","The size of the k-best list was set to 500 for RAMPION, MIRA and RM, and 1500 for PRO, with both PRO and RAMPION utilizing k best aggregation across iterations.","RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers.",Multi Citance
339,N09-1025.xml,P13-1110.xml,"We set C to 0.01, and M axI ter to 100.","We selected the bound step size D, based on performance on a held-out dev set, to be 0.01 for the basic feature set and 0.1 for the sparse feature set.","The bound constraint B was set to 1.4 The approximate sentence-level BLEU cost i is computed in a manner similar to (Chiang et al., 2009), namely, in the context of previous 1-best translations of the tuning set.",All results are averaged over 3 runs.,4.2 Feature.,Single Citance
340,N09-1025.xml,P13-1110.xml,4.2 Feature.,"Sets We experimented with a small (basic) feature set, and a large (sparse) feature set.For the small feature set, we use 14 features, including a language model, 5 translation model features, penalties for unknown words, the glue rule, and rule arity.","For experiments with a larger feature set, we introduced additional lexical and non-lexical sparse Boolean features of the form commonly found in the literature (Chiang et al., 2009; Watan 4 We also conducted an investigation into the setting of the B parameter.","We explored alternative values for B, as well as scaling it by the current candidate’s cost, and found that the optimizer is fairly insensitive to these changes, resulting in only minor differences in BLEU.","Optimizer Zh Ar MIRA 35k 37k PRO 95k 115k RAMPION 22k 24k RM 30k 32k Active+Inactive 3.4M 4.9M Table 2: Active sparse feature templates abe et al., 2007; Simianer et al., 2012).",Single Citance
341,N09-1025.xml,Pmert_n09.xml,"Secondly, it offers a globally optimal line search.","Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization.","These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.","In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.","On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good as MERT on small feature sets, and also scales better as the number of features increases.",Multi Citance
342,N09-1025.xml,Pmert_n09.xml,"Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization.","These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization.","In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation.","On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good as MERT on small feature sets, and also scales better as the number of features increases.","In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search.",Single Citance
343,N09-1025.xml,PMTS_n09.xml,"Since MERT directly optimizes the log- linear weights on the devset BLEU scores, as compared to the linear weights which were learnt by optimizing the maximum likelihood on the target side of the devset, we expected the former to provide better results in terms of BLEU.","However, in the tuning phase, MERT was observed to iterate to the maximum allowable iteration limit (25) in order to complete, rather than converging automatically based on the evaluation metric criterion.","This observation conrms previous ndings (Chiang et al., 2009) regarding the inability of the MERT algorithm to converge on an optimal set of weights for a reasonably large number of parameters.",Linear mixture adaptation caused the translation scores to improve by 1.06 absolute BLEU points (4.08% relative) for En–De and 2.56 absolute points (7.55% relative) for En–Fr over the baseline.,For DeEn and FrEn the improvements were 0.23 absolute BLEU points (0.65% relative) and 0.5 absolute BLEU points (1.37% relative) respectively.,Single Citance
344,N09-1025.xml,PMTS_n09.xml,"The BLEU score improvements over the baseline for this adaptation model were 1.72 absolute (6.62% relative) points for En–De, 2.58 absolute (7.61% relative) points for En–Fr, 0.17 absolute (0.48% relative) points for DeEn and 0.1 absolute (0.28% relative) points for FrEn.","As in the previous phases, the improvements are statistically signiﬁcant for translations from English.","The MERT algorithm is known to be unable to learn optimal weights for large parameter settings (Chiang et al., 2009).","In the current scenario, two phrase tables, two reordering models and three language models resulted in a con siderable number of parameters, causing the algorithm to learn sub-optimal mixture weights leading to poorer performance.",The overall trends of the results emphasize the importance of linear mixture adaptation for both language and translation models.,Single Citance
345,N09-1025.xml,PMTS_n09.xml,Here the weights for linear combination of multiple phrase tables were estimated using language models.,Directly learning linear weights by optimizing translation quality in terms of the development set would be the prime direction in future.,"We would also like to look into alternative tuning techniques, especially ones based on the MIRA algorithm to improve the quality of log-linear mixture adaptation in large parameter settings (Chiang et al., 2009).",Enhancing the translation quality further with third party forum data would also be another objective in this direction.,Finally we would also like to investigate further on different ranking schemes and empirical threshold selection for selecting relevant datasets to supplement training data for improving translation quality.,Single Citance
346,N09-1025.xml,PSMPT_n09.xml,"In practice, the number of NTs on the right hand side is constrained to at most two, which must be separated by lexical items in α.","Each rule is associated with a score that is derived using the log-linear model (Och and Ney, 2002) as in (2): features seem to be effective in Arabic-to-English and Chinese-to-English translation.","Chiang et w(X  (, ,  )) = ii (2) i al. 2009) define new translational features using neighbouring word contexts of the source phrase, which are directly integrated into the translation model of Hiero system.","In order to limit the the size of their model, they restrict words to being among the 100 most frequently occurring words from the training data; all other words are replaced with a special token.","One final paper in this strand of research is that of (He et al., 2008), who despite not mentioning the link between the two pieces of work, show that the low-level source-language features used by (Stroppa et al., 2007) are also of benefit when used with the HPB decoder (Chiang, 2007).",Single Citance
347,N09-1025.xml,PTASL_n09.xml,The algorithm using the lexicalized monolingual TSG as a language model is just a special case of our generative model.,Many researchers are also concerned with the overgeneralization problem of STSG rules.,"For example, Chiang et al. [3] designed many target-side syntax features to improve the string-to-tree translation.","Rather than using a rule lexicalization model, binary features such as rule overlap features, bad-rewrite features and insertion features are employed in the algorithm.","They argue that certain nonterminals are more reliable than others for the rule overlap feature, and they created a binary feature for each root nonterminal of a specified rule.",Single Citance
348,N09-1025.xml,PTASL_n09.xml,"The features proposed by Chiang et al. are also language-dependent, based on a thorough analysis of the tuning set.In contrast to Chiang et al., we argue that in any language pair, the newly generated nonterminal in the target parse tree depends strongly on the underlying source- and target-side lexical strings.","We therefore propose the language-independent generative and discriminative lexicalized STSG models, and regard these models as additional strong features to be integrated into the log-linear model.","Moreover, to establish a strong baseline, we also include the discount feature used by Chiang et al. [3] in our baseline string-to-tree model.",Other related work focuses on grammar or rule lexicalization.,"An example of grammar lexicalization is the proposal of Zhang and Gildea [31], [32] to improve the word alignment using lexicalized inversion transduction grammar.",Single Citance
349,N09-1025.xml,W10-1757.xml,ability (e.g. many different word reorder- ings).,"Larger N may improve reranking performance, but may also increase feature sparsity.","When the number of features is too large, even popular reranking algorithms such as SVM (Shen et al., 2004) and MIRA (Watanabe et al., 2007; Chiang et al., 2009) may fail.",Our goal here is to address this situation.,3 Proposed Reranking Framework.,Multi Citance
350,N09-1025.xml,W10-1757.xml,Combinations of multitask features with high frequency features also give significant improvements over the high frequency features alone.,method.,"Recent work by (Chiang et al., 2009) describes new features for hierarchical phrase-based MT, while (Collins and Koo, 2005) describes features for parsing.","Evaluation campaigns like WMT (CallisonBurch et al., 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks.",2,Multi Citance
351,N09-1025.xml,W10-1761.xml,Chiang et al.(2008) rectified this deficiency by using the MIRA to tune all feature function weights in combination.,"However, the translation model continues to be hierarchical.","Chiang et al. (2009) added thousands of linguistically-motivated features to hierarchical and syntax systems, however, the source syntax features are derived from the research above.",The translation model remain constant but the parameterization changes.,Shen et al.(2009) discusses soft syntax constraints and context features in a dependency tree translation model.,Single Citance
352,N09-1025.xml,W10-1761.xml,Initialization Rules are also extracted which contains a mixture of decorated and undecorated non-terminals.,These rules can also be lexicalized or unlexicalized.,"A non-exhaustive sample is given below: [X  Ls, i, i] Terminal Symbol (X  Ls)  G X  ADJ A1 Akt # X1 Act N P  N E1 X2 # X1 X2 T OP  N E1 letzter X2 # X1 Last X2 [X    Fj,k Ls, i, j] [X  Fj,k  Ls, i, j + 1] Non-Terminal Symbol [X    Fj,k Ls, i, j] [X, j, Rj,k ] [X  Fj,k  Ls, i, Rj,k ] [X    Ls, i, Ri,j ] [Fi,j = Ls] the log function (hm = log hm) [X  Ls, i, Ri,j ] log p(tPIPEs) = m hm(t, s) (3) m Goal [X  Ls, 0, PIPEV PIPE  1] An advanta ge of our model over (Marto n and Resnik, 2008; Chiang et al., 2008, 2009) is the number of feature function s remains the same,This model allows translation rules to take ad vantage of both syntactic label and word context.","The presence of default label edges between every node allows undecorated non-terminals to be applied to any span, allowing flexibility in the translation model.","This contrasts with the approach by (Zollmann and Venugopal, 2006) in attempting to improve the coverage of syntactic translation.",Multi Citance
353,N09-1025.xml,W10-1761.xml,"Rather than creating ad-hoc schemes to categories non-terminals with syntactic labels when they do not span syntactic constituencies, we only use labels that are presented by the parser or shallow tagger.","Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al., 2008), but we build ambiguity into the translation rule.","The model also differs from (Marton and Resnik, 2008; Chiang et al., 2008, 2009) by adding informative labels to rule non-terminals and requiring them to match the source span label.","The soft constraint in our model pertain not to a additional feature functions based on syntactic information, but to the availability of syntactic and non- syntactic informed rules.","In common with most current SMT systems, the decoding goal of finding the most probable target language sentence ˆt, given a source language sentence s ˆt = argmaxt p(t|s) (1) The argmax function defines the search objective of the decoder.",Multi Citance
354,P00-1025.xml,J06-1004.xml,"reduplication, but no complexity analysis of the model is given.","Moreover, this technique does not seem to be able to describe interdigitation.","Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.","The compile-replace algorithm facilitates a compact definition of non- concatenative morphological processes, but since such expressions compile to the na¨ıve networks, no space is saved.","Furthermore, this is a compile-time mechanism rather than a theoretical and mathematically founded solution.",Single Citance
355,P00-1025.xml,P9852_p00.xml,"As discussed above in Sections 2 and 4, Indonesian morphology includes the non-concatenative proc­ ess of reduplication.",Handling this with pure regu­ lar grammars as implemented by finite state auto­ mata is very difficult.,"Thus, we employ the com pile-replace feature in xfst (Beesley &amp; Karttunen, 2000).","The right bracket is also augmented with A2 to indicate duplication; thus, the full anno­ tation is nA["" and nA2A1"" . Given this network defmition, xfst compiles and post-processes these annotations to produce a new network where the appropriate reduplications have been carried out.","For example, nA [bukuA2AJ"" would eventually be compiled to become bukubuku.",Single Citance
356,P00-1025.xml,PE2006_p00.xml,"reduplication, but no complexity analysis of the model is given.","Moreover, this technique does not seem to be able to describe interdigitation.","Beesley and Karttunen (2000) describe a technique, called compile-replace, for constructing FSTs, which involves reapplying the regular-expression compiler to its own output.","The compile-replace algorithm facilitates a compact definition of non- concatenative morphological processes, but since such expressions compile to the na¨ıve networks, no space is saved.","Furthermore, this is a compile-time mechanism rather than a theoretical and mathematically founded solution.",Single Citance
357,P00-1025.xml,Pmorph_p00.xml,A system of realization rules expressed as a datr theory can be used to generate an inﬂected surface form from its lexical description but such a system is not directly usable for recognition.,"In contrast, ﬁnite-state transducers are bidirectional.The same transducer can generate an inﬂected form from its underlying representation or analyze it into a lexical stem or stems and the associated feature bundles.",In this section we will show in detail how Realizational Morphology can be expressed in terms of the parc/xrce regular expression calculus as dened in Beesley and Karttunen [4].,The regular expressions given in this section constitute a script that can be directly compiled with the xfst tool.,"The data and the analysis of Lingala come from Chapter 5 in Stump’s book, from a short monograph on Lingala by Meeuwis [16], and from Michael Gasser’s course notes at http://www.indiana.edu/~gasser/L103/hw10.html.",Single Citance
358,P00-1025.xml,Pstat_p00.xml,"The first step in the modification of such strings is to interdigitate the roots and patterns to form stems, but only on the lower side of the relation.","The interdigitation is formalized in finite-state terms as intersection (Beesley, 1998b; Beesley, 1998a), but it in fact represents a special case of intersection that is performed much more efficiently by a finite-state algorithm called MERGE.","The application of the merge algorithm to the lower-side of the relation is performed by the COMPILER EPL ACE algorithm (Beesley and Karttunen, 2000),11 and the result is shown in Figure 7.","Once compile-replace has been performed on the lower side, the necessary alternation rules can 11 Before merge and compile-replace became available, a much less efficient algorithm produced the same result by automatically generating alternation rules, compiling them, and applying them to the lower side of the transducer.","be compiled and applied, via composition, in the usual way shown in Figure 4.",Single Citance
359,P00-1025.xml,W02-0503-parscit130908.xml,Anne Roeck and Waleed Al-Fares (2000) developed a clustering algorithm for Arabic words sharing the same verbal root.,They used root-based clusters to substitute for dictionaries in indexing for information retrieval.,Beesley and Karttunen (2000) described a new technique for constructing finite-state transducers that involves reapplying a regular-expression compiler to its own output.,They implementedthe system in an algorithm called compile replace.,"This technique has proved useful for handling non-concatenate phenomena, and they demonstrate it on Malay full-stem reduplication and Arabic stem inter-digitations.",Single Citance
360,P00-1025.xml,W07-0802-parscit130908.xml,The remaining case of State = Const takes care of the id. ¯afah genitive constructions.,"Thus, after applying all the “correction” functions above, we get the following implementation of the noun determination rule: oper agrP3 : Species -&gt; Gender -&gt; Number -&gt; PerGenNum= \h,g,n -&gt; case &lt;h,n&gt; of { &lt;NoHum,Pl&gt; =&gt; Per3 Fem Sg; _ =&gt; Per3 g n }; The agrP3 helping function tests for the case when the species and number are nonhuman andplural.This case is treated in agreement as the fem inine singular.","A large-scale implementation of the Arabic morphological system is the Xerox Arabic Morphologi cal Analyzer and Generator (Beesley and Karttunen,2000; Beesley, 2001).","This system is developed us ing only the Xerox Finite State Technology tools(Beesley and Karttunen, 2003) from which an Arabic Finite State Lexical Transducer is written.","A re search version is available for online testing, and an expanded and updated version can be obtained witha commercial license.",Multi Citance
361,P00-1025.xml,W08-0703.xml,"However, the effects of autosegmental phonology and other long-range dependencies (like vowel harmony) cannot be easily Bayesianized.",1.1 Related Work.,"In the last decade, finite-state approaches to phonology (Gildea and Jurafsky, 1996; Beesley and Karttunen, 2000) have effectively brought theoretical linguistic work on rewrite rules into the computational realm.","A finite-state approximation of optimality theory (Karttunen, 1998) was later refined into a compact treatment of gradient constraints (Gerdemann and van Noord, 2000).","Recent work on Bayesian models of morphological segmentation (Johnson et al., 2007) could be combined with phonological rule induction (Goldwater and Johnson, 2004) in a variety of ways, some of which will be explored in our discussion of future work.",Multi Citance
362,P00-1025.xml,W09-0802.xml,"The most popular system is probably the Xerox Finite State Tool (Beesley and Karttunen, 2003).","It has been used, among others, for the description of Arabic morphology (Beesley, 1998).","The interdigitation is handled using a compile-replace process using the replace operator (Karttunen and Beesley, 2000) (Karttunen, 1995).","The computational model is a sequential one, where two-tape transducers are merged using the 2 There is a Akkadian verb with 3 weak consonants as root..",composition operation.,Multi Citance
363,P05-1004.xml,C10-2101.xml,"In fact, Saito et al.(2007) report that a majority of unknown named entities (those never appear in a training corpus) contain unknown morphemes as their constituents and that NER models perform poorly on them.",A straightforward solution to this problem would be to acquire unknown morphemes and to assign semantic labels to them.,"Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).","A supersense corresponds to one of the 26 broad categories defined by WordNet (Fellbaum, 1998).",Each noun synset is associated with a supersense.,Multi Citance
364,P05-1004.xml,J07-4005.xml,"Automatic acquisition of sense inventories is an important endeavor, and we hope to look at ways of combining our method for detecting predominance with automatically induced inventories such as those produced by CBC.","Evaluation of induced inventories should be done in the context of an application, because the senses will be keyed to the acquisition corpus and not to WordNet.Induction of senses allows coverage of senses appearing in the data that are not present in a predefined inventory.","Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.",9 We used the demonstration at http://www.isi.edu/~pantel/Content/Demos/LexSem/cbc.htm with the.,"option to include all corpora (TREC2002, TREC9, and COSMOS).",Multi Citance
365,P05-1004.xml,J09-3004.xml,"One direction is to explore better vector comparison methods that will utilize the improved feature weighting, as shown in Geffet and Dagan (2005).","Another direction is to integrate distributional similarity and pattern-based acquisition approaches, which were shown to provide largely complementary information (Mirkin, Dagan, and Geffet 2006).","An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.","As a parallel direction, future research should explore in detail the impact of different lexical-semantic acquisition methods on text understanding applications.","Finally, our proposed bootstrapping scheme seems to have a general appeal for improving feature vector quality in additional unsupervised settings.",Single Citance
366,P05-1004.xml,N06-1017.xml,"Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet.In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item.","To our knowledge, there exists no other approach to date to the problem of detecting unknown senses.","There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.",Plan of the paper.,"After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3.",Multi Citance
367,P05-1004.xml,N06-1017.xml,"Our immediate goal is to use unknown sense detection in combination with WSD, to filter out items that the WSD system cannot handle due to missing senses.","Once items have been identified as unknown, they are available for further processing: If possible one would like to assign some measure of sense information even to these items.","Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.",$,$,Multi Citance
368,P05-1004.xml,N07-1024.xml,The knowledge-based models described above classify unknown words using information about the syntactic and semantic categories of their component characters.,Another useful source of information is the context in which unknown words occur.,"While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)",Part of the goal of this study is to investi gate whether and how these two different sources of information can be combined to improve performance on semantic classification of Chinese unknown words.,"To this end, we first use the knowledge-based models to propose a list of five candidate categories for the target word, then extract a generalized context for each category in Cilin from a corpus, and finally compute the similarity between the context of the target word and the generalized context of each of its candidate categories.",Multi Citance
369,P05-1004.xml,P12-2050.xml,"entries, but here we have repurposed them as target labels for direct human annotation.","Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semanticfields (Miller, 1990; Fellbaum, 1990).","More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.","SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g.,3Note that work in supersense tagging used text with fine grained sense annotations that were then coarsened to SSTs.",4The noun/verb distinction might prove problematic in some languages.,Multi Citance
370,P05-1004.xml,S07-1032.xml,It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples.,"Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach.","Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).","However, most of the later approaches used the original Lexico- graphical Files of WN (more recently called Super 1 http://www.illc.uva.nl/EuroWordNet/ 2 http://www.ceid.upatras.gr/Balkanet 3 http://www.lsi.upc.es/ nlp/meaning 157 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval2007), pages 157–160, Prague, June 2007.",Qc 2007 Association for Computational Linguistics senses) as very coarse–grained sense distinctions.,Multi Citance
371,P05-1004.xml,S10-1090.xml,"Thus, some research has been focused on deriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Schu¨ tze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007).","That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.","In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).","That is, grouping senses of different words into the same explicit and comprehensive semantic class.",Most of the later approaches used the original Lexico- graphical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions.,Multi Citance
372,P05-1004.xml,S12-1011.xml,"We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning.",Developing models of the meanings of words and phrases is a key challenge for computational linguistics.,"Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).","However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011).",It is in this area that our paper makes its contribution.,Multi Citance
373,P05-1004.xml,S12-1011.xml,Classes are marginalised over every 10th iteration.,4.1 Supersense Tagging.,"Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.","The task of this eval Ni ni | Ψn mi | Ψm Mi ∀k: ck,i | Ψc Ni ∼ Multi(Ψn ) ∼ Multi(Ψm ) Mi ∼ Multi(Ψc ) uation is to determine the WORDNET supersenses of a given list of nouns.","We report results on the WN1.6 test set as defined by Ciaramita and Johnson (2003), who used 755 randomly selected nouns with a unique supersense from the WORDNET 1.6 corpus.",Multi Citance
374,P05-1004.xml,S12-1023.xml,"In contrast, we are interested in relations within words, namely between word senses.","We cannot expect two different senses of the same noun to co-occur in the same sentence, as this is discouraged for pragmatic reasons (Gale et al., 1992).","A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.","However, our emphasis lies rather on modeling polysemy across words (meta alternations), something that is absent in WSD, class-based or not.","The only exception, to our knowledge, is Ando (2006), who pools the labeled examples for all words from a dataset for learning, implicitly exploiting regularities in sense alternations.",Multi Citance
375,P05-1004.xml,W06-1670.xml,"From a similar perspective, de Loupy et al.(de Loupy et al., 1998) also investigated the potential advantages of using HMMs for disambiguation.","More recently, variants of the generative HMM have been applied to WSD (Molina et al., 2002; Molina et al., 2004) and evaluated also on Senseval data, showing performance comparable to the first sense baseline.","Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.","As far as applications are concerned, it has been shown that supersense information can support supervised WSD, by providing a partial disambiguation step (Ciaramita et al., 2003).","In syntactic parse re-ranking supersenses have been used to build useful latent semantic features (Koo and Collins, 2005).",Multi Citance
376,P05-1053.xml,C08-1088.xml,"In the literature, feature-based methods have dominated the research in semantic relation extraction.","Featured-based methods achieve promising performance and competitive efficiency by transforming a relation example into a set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information.","However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy.","Therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly.","From prior work (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) to current research (Zhang et al., 2006; Zhou et al., 2007), kernel methods have been showing more and more potential in relation extraction.",Single Citance
377,P05-1053.xml,C08-1088.xml,"Improvements of different tree setups over SPT on the ACE RDC 2004 corpus Finally, Table 4 compares our system with other state-of-the-art kernel-based systems on the 7 relation types of the ACE RDC 2004 corpus.","It shows that our UPST outperforms all previous tree setups using one single kernel, and even better than two previous composite kernels (Zhang et al., 2006; Zhao and Grishman, 2005).","Furthermore, when the UPST (FPT) kernel is com bined with a linear state-of-the-state feature- based kernel (Zhou et al., 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al.","(2007) (i.e. polynomial degree d=2 and coefficient Į=0.3), we get the so far best performance of 77.1 in F-measure for 7 relation types on the ACE RDC 2004 data set.Systems P(%) R(%) F Ours: composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao and Grishman, (2005):4 composite kernel 69.2 70.5 70.4 Ours: CTK with UPST 80.1 70.7 75.1Zhou et al., (2007): context sensitive CTK with CS-SPT 81.1 66.7 73.2 Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 4.",Comparison of different systems on the ACE RDC 2004 corpus In Table 3 we summarize the improvements of different tree setups over SPT.,Single Citance
378,P05-1053.xml,C10-1018.xml,"Since relations are usually asymmetric in nature, hence in all of our experiments, unless otherwise stated, we distinguish between the argument ordering of the two mentions.","For instance, we consider m1:emporg:m2 and m2:emporg:m1 to be distinct relation types.","Most of the features used in our system are based on the work in (Zhou et al., 2005).","In this paper, we propose some new collocation features inspired by word sense disambiguation (WSD).",We give an overview of the features in Table 1.,Single Citance
379,P05-1053.xml,C10-1018.xml,"In this paper, we propose some new collocation features inspired by word sense disambiguation (WSD).",We give an overview of the features in Table 1.,"Due to space limitations, we only describe the collocation features and refer the reader to (Zhou et al., 2005) for the rest of the features.",2.1 Collocation Features.,"Following (Zhou et al., 2005), we use a single word to represent the head word of a mention.",Single Citance
380,P05-1053.xml,D07-1076.xml,"For example, the sentence “Bill Gates is the chairman and chief software architect of Microsoft Corporation.” c onveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (person name) and “Microsoft Corporation” (organization name).Extraction of semantic relations between entities can be very useful in many applic a- tions such as question answering, e.g. to answer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the pres ident of the United States”via his relationship with “the United States”.",Many researches have been done in relation extraction.,"Among them, feature-based methods (Kambhatla 2004; Zhou et al., 2005) achieve certain success by employing a large amount of diverse linguistic features, varying from lexical knowledge, entity- related information to syntactic parse trees, dependency trees and semantic information","As an alternative to feature-based methods, tree kernel-based methods provide an elegant solution to explore implic itly structured features by directly computing the simila rity between two trees.","Although earlier researches (Zelenko et al 2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a) only achieve success on simple tasks and fail on complex tasks, such as the ACE RDC task, tree kernel-based methods achieve much progress recently.",Multi Citance
381,P05-1053.xml,D07-1076.xml,"For example, the sentence “Bill Gates is the chairman and chief software architect of Microsoft Corporation.” c onveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (person name) and “Microsoft Corporation” (organization name).Extraction of semantic relations between entities can be very useful in many applic a- tions such as question answering, e.g. to answer the query “Who is the president of the United States?”, and information retrieval, e.g. to expand the query “George W. Bush”with “the pres ident of the United States”via his relationship with “the United States”.",Many researches have been done in relation extraction.,"How ever, it is difficult for them to effectively capture struc tured parse tree information (Zhou et al 2005), which is critical for further performance improvement in relation extraction.","As an alternative to feature-based methods, tree kernel-based methods provide an elegant solution to explore implic itly structured features by directly computing the simila rity between two trees.","Although earlier researches (Zelenko et al 2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a) only achieve success on simple tasks and fail on complex tasks, such as the ACE RDC task, tree kernel-based methods achieve much progress recently.",Single Citance
382,P05-1053.xml,D07-1076.xml,It shows that the dynamic tree span can futher improve theTable 2: Comparison of dynamic contextsensitive tree span with SPT using our context sensitive convolution tree kernel on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora.,18% of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category.,"Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al (2006), is applied to integrate the proposed c ontextsensitive convolution tree kernel with a state -of the-art linear kernel (Zhou et al 2005)","In the Here, K L ( , ) and K C ( , )indicates the normal future work, we will further explore expanding the ized linear kernel and context -sensitive convolution dynamic tree span beyond SPT for the remaining tree tree kernel respectively whileK p ( , ) is the poly span categories.","nomial expansion of K ( , ) with degree d=2, i.e. 6 Significance test shows that the dynamic tree span per-.",Single Citance
383,P05-1053.xml,D07-1076.xml,"K p ( , ) ( K( , ) 1)2 and a is the coefficient (a is set to 0.3 using cross -validation).","7 Here, we use the same set of flat features (i.e. word,.","entity type, mention level, overlap, base phrase chunk- ing, dependency tree, parse tree and semantic information) as Zhou et al (20 05).",Table 3 evaluates the performance of the ACE RDC 2004 P(%) R(%) F composite kernel.,It shows that the composite kernel much further improves the performance beyond that of either the state-of-the-art linear kernel or our tree kernel and achieves the F-measures of 74.1 and 75.8 on the major relation types of the ACE RDC 2003 and 2004 corpora respectively.,Pre Contiguous
384,P05-1053.xml,D07-1076.xml,They also show that our tree kernel-based system outperforms the state-of-the-art feature-based system.,This proves the great potential inherent in the parse tree structure for relation extraction and our tree kernel takes a big stride towards the right direction.,dependency kernel Zhou et al.(2005),5 C o n c l u s i o n Structured parse tree information holds great potential for relation extraction.,This paper proposes a context- sensitive convolution tree kernel to resolve two critical problems in previous tree kernels for relation extraction by first automatically determining a dynamic context-sensitive tree span and then applying a context-sensitive convolution tree kernel.,Single Citance
385,P05-1053.xml,D09-1149.xml,"In this paper, we select Support Vector Machines (SVMs) as the underlying supervised classifier since it represents the state-of-the-art in the machine learning research community, and there are good implementations of the algorithm available.","Specifically, we use LIBSVM (Chang et al., 2001), an effective tool for support vector classification, since it supports multi-class classification and provides probability estimation as well.","For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).","(1) Words: According to their positions, four categories of words are considered: a) the words of both the mentions; b) the words between the two mentions; c) the words before M1; and d) the words after M2.",straightforwardly trained from previously available labeled data as follows: Algorithm self-bootstrapping Require: labeled seed set L Require: unlabeled data set U Require: batch size S Repeat Train a single classifier on L Run the classifier on U Find at most S instances in U that the classifier has the highest prediction confidence Add them into L Until: no data points available or the stoppage condition is reached Figure 1.,Single Citance
386,P05-1053.xml,D09-1149.xml,"Generally, the sample size is normally proportional to the relative size of the strata.",The main motivation for using a stratified sampling design is to ensure that particular groups within a population are adequately represented in the sample.,"It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.","When the relation instances for a specific relation type occurs frequently in the initial seed set, the classifier will achieve good performance on this type, otherwise the classifier can hardly recognize them from the test set.In order for every type of relations to be properly represented, the stratified sampling strategy is applied to the seed selection procedure.",Table 1.,Multi Citance
387,P05-1053.xml,D12-1074.xml,"• Baseline, simply the arc-factored model consisting only of Rel and corresponding Label variables for each entity.","Features on the relation factors, which are common to all model configurations, are combinations of lexical information (i.e., the words that form the entity, the pos-tags of the entities, etc.) as well as the distance between the relation.","This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information.","• Baseline-Ent, a variant of Baseline with additional features which include combinations of • Oracle D-Parse, in which we also instantiate a full set of latent dependency syntax variables, and connect them to the baseline model using D-CONNECT factors.",Syntax variables are clamped to their true values.,Single Citance
388,P05-1053.xml,E06-2012.xml,3.3 Statistical training.,"Because we had no existing methods to address financial events or relations, we took this opportunity to develop a trainable approach.","Recent work has begun to address relation and event extraction through trainable means, chiefly SVM classification (Zelenko et al, 2003, Zhou et al, 2005).","The approach we’ve used here is classifier-based as well, but relies on maximum entropy modeling instead.","Most trainable approaches to event extraction are entity-anchored: given a pair of relevant entities (e.g., a pair of companies), the object of the endeavor is to identify the relation that holds between them (e.g., acquisition or subsidiary).",Multi Citance
389,P05-1053.xml,E12-1020.xml,"For experiments on both relation detection and relation classification, we use SVM4 (Vapnik 1998) as the learning algorithm since it can be extended to support transductive inference as discussed in section 4.3.","However, for the analysis in section 3.2 and the purification preprocess steps in section 4.2, we use a MaxEnt5 model since it outputs probabilities6 for its predictions.","For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011).",2.2 ACE 2005 annotation.,The ACE 2005 training data contains 599 articles 4 SVM-Light is used.,Single Citance
390,P05-1053.xml,E12-1020.xml,"To understand what causes the missing annotations and the spurious ones, we need methods to find how similar/different the false positives are to true positives and also how similar/different the false negatives (missing annotations) are to true negatives.","If we adopt a good similarity metric, which captures the structural, lexical and semantic similarity between relation mentions, this analysis will help us to understand the similarity/difference from an extraction perspective.","We use a state-of-the-art feature space (Zhou et al., 2005) to represent examples (including all correct examples, erroneous ones and untagged examples) and use MaxEnt as the weight learning model since it shows competitive performance in relation extraction (Jiang and Zhai, 2007) and outputs probabilities associated with each prediction.","We train a MaxEnt model for relation detection on true positives and true negatives, which respectively are the subset of correct examples annotated by fp1 (and adjudicated as correct ones) and negative Figure 3: cumulative distribution of frequency (CDF) of the relative ranking of model-predicted probability of being positive for false negatives in a pool mixed of false negatives and true negatives; and the CDF of the relative ranking of model-predicted probability of being negative for false positives in a pool mixed of false positives and true positives.","For false negatives, it shows a highly skewed distribution in which around 75% of the false negatives are ranked within the top 10%.",Multi Citance
391,P05-1053.xml,E12-1020.xml,"Experiments were conducted over the same set of documents on which we did analysis: the 511 documents which have completed annotation in all of the fp1, fp2 and adj from the ACE 2005 Multilingual Training Data V3.0.","To reemphasize, we apply the hierarchical learning scheme and we focus on improving relation detection while keeping relation classification unchanged (results show that its performance is improved because of the improved detection).",We use SVM as our learning algorithm with the full feature set from Zhou et al.(2005).,Baseline algorithm: The relation detector is unchanged.,"We follow the common practice, which is to use annotated examples as positive ones and all possible untagged relation mentions as negative ones.",Single Citance
392,P05-1053.xml,I08-1004.xml,Much research work has been done in this direction.,"Prior researches apply feature-based methods to select and define a set of flat features, which can be mined from the parse trees, to represent particular structured information in the parse tree, such as the grammatical role (e.g. subject or object), according to the particular application.","Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).","The major problem with feature-based methods on exploring structured information is that they may fail to well capture complex structured information, which is critical for further performance improvement.","The current trend is to explore kernel-based methods (Haussler, 1999) which can implicitly explore features in a high dimensional space by employing a kernel to calculate the similarity between two objects directly.",Multi Citance
393,P05-1053.xml,N06-1037.xml,"Prior feature-based methods for this task (Kambhatla 2004; Zhou et al., 2005) employed a large amount of diverse linguistic features, varyingfrom lexical knowledge, entity mention informa tion to syntactic parse trees, dependency trees and semantic features.","Since a parse tree contains rich syntactic structure information, in principle, the features extracted from a parse tree should contribute much more to performance improvement for relation extraction.","However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.",This may be mainly due to the fact that the syntactic structure information in a parse tree is hard to explicitly describe by a vector of linear features.,"As an alternative, kernel methods (Collins and Duffy, 2001) provide an elegant solution to implicitly explore tree structure features by directly computing the similarity between two trees.",Multi Citance
394,P05-1053.xml,N06-1037.xml,Their results essentially depend on the entire full parse tree.,"Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction.",Zhou et al.(2005) explore various features in relation extraction using SVM.,They conduct exhaustive experiments to investigate the incorporation and the individual contribution of diverse features.,They report that chunking information contributes to most of the performance improvement from the syntactic aspect.,Single Citance
395,P05-1053.xml,N06-1037.xml,They conduct exhaustive experiments to investigate the incorporation and the individual contribution of diverse features.,They report that chunking information contributes to most of the performance improvement from the syntactic aspect.,The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.,Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.,"Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.",Multi Citance
396,P05-1053.xml,N06-1037.xml,The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually.,Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.,"Besides, Zhou et al.(2005) introduce additional chunking features to enhance the parse tree features.","However, the hierarchical structured information in the parse trees is not well preserved in their parse tree- related features.","As an alternative to the feature-based methods, kernel methods (Haussler, 1999) have been proposed to implicitly explore features in a high dimensional space by employing a kernel function to calculate the similarity between two objects directly.",Single Citance
397,P05-1053.xml,N06-1037.xml,"The ACE corpus is gathered from various newspaper, news- wire and broadcasts.","The same as previous work 2 For the convenience of discussion, without losing generality,.",we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.,"(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.",The training set consists of 674 annotated text documents and 9683 relation instances.,Multi Citance
398,P05-1053.xml,N06-1037.xml,"The same as previous work 2 For the convenience of discussion, without losing generality,.",we call the features used in Zhou et al.(2005) and Kambhatla (2004) flat feature set.,"(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.",The training set consists of 674 annotated text documents and 9683 relation instances.,The test set consists of 97 documents and 1386 relation instances.,Single Citance
399,P05-1053.xml,N07-1015.xml,Recent studies on relation extraction have shown the advantages of discriminative model-based statistical machine learning approach to this problem.,There are generally two lines of work following this approach.,"The first utilizes a set of carefully selected features obtained from different levels of text analysis, from part-of-speech (POS) tagging to full parsing and dependency parsing (Kambhatla, 2004; Zhao and Grishman, 2005; Zhou et al., 2005)1.","The second line of work designs kernel functions on some structured representation (sequences or trees) of the relation instances to capture the similarity between two relation instances (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhang et al., 2006a; Zhang et al., 2006b).","Of particular interest among the various kernels proposed are the convolution kernels (Bunescu and Mooney, 2005b; Zhang et al., 2006a), because they can efficiently compute the similarity between two instances in a huge feature space due to their recursive nature.",Multi Citance
400,P05-1053.xml,N07-1015.xml,"Task-oriented heuristics can be used to prune the feature space, and when appropriately done, can improve the performance.","A combination of features of different levels of complexity and from different sentence representations, coupled with task-oriented feature pruning, gives the best performance.",Zhao and Grishman (2005) and Zhou et al.(2005) explored a large set of features that are potentially useful for relation extraction.,"However, the feature space was defined and explored in a somewhat ad hoc manner.",We study a broader scope of features and perform a more systematic study of different feature subspaces.,Multi Citance
401,P05-1053.xml,N07-1015.xml,"To represent a conjunctive feature such as “arg 1 is a Person entity and arg 2 is a Bounded-Area entity”, we can take a subgraph that contains two nodes, one for each argument, and each labeled with an entity attribute.","Note that in this case, the subgraph contains two disconnected components, which is al lowed by our definition.",Bag-of-Words: These features have also been explore by Zhao and Grishman (2005) and Zhou et.al.(2005).,"To represent a bag-of-word feature, we can simply take a subgraph that contains a single node labeled with the token.","Because the node also has an argument tag, we can distinguish between argument word and non-argument word.",Multi Citance
402,P05-1053.xml,N07-1015.xml,"Grammar Productions: The features in convolution tree kernels for relation extraction (Zhang et al., 2006a; Zhang et al., 2006b) are sequences of grammar productions, that is, complete subtrees of the syntactic parse tree.","Therefore, these features can naturally be represented by subgraphs of the relation instance graphs.","Dependency Relations and Dependency Paths: These features have been explored by Bunescu and Mooney (2005a), Zhao and Grishman (2005), and Zhou et.al.(2005).",A dependency relation can be represented as an edge connecting two nodes from the dependency tree.,The dependency path between the two arguments can also be easily represented as a path in the dependency tree connecting the two nodes that represent the two arguments.,Multi Citance
403,P05-1053.xml,N09-3012.xml,"In (Bunescu and Mooney, 2005b) sequence of words features are utilized using a sub-sequence kernel.","In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al., 2006a) syntactic features are employed for relation extraction.","Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.","66 Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 66–71, Boulder, Colorado, June 2009.","Qc 2009 Association for Computational Linguistics In CD’01 (Collins and Duffy, 2001) a convolution syntactic tree kernel is proposed that generally measures the syntactic similarity between parse trees.",Single Citance
404,P05-1053.xml,N13-1093.xml,All the DDIs (i.e. positive RE instances) of the incorrectly identified sentences in Stage 1 (i.e. the sentences which are incorrectly labelled as not having any DDI and filtered) are automatically considered as false negatives during the calculation of DDI extraction results in Stage 2.,"To verify whether our proposed hybrid kernel achieves state-of-the-art results without taking benefits of the output of Stage 1, we did some experiments without discarding any sentence.","These experiments are done using Zhou et al.(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.","Table 1 shows the results of 5-fold cross-validation experiments (hyper-parameters are tuned for obtaining maximum F-score).As the results show, there is a gain +0.9 points in F-score (mainly due to the boost in recall) after the addition of features related to negation scope.",There is also some minor improvement due to the proposed non-target entity specific features.,Single Citance
405,P05-1053.xml,N13-1093.xml,"Table 1 shows the results of 5-fold cross-validation experiments (hyper-parameters are tuned for obtaining maximum F-score).As the results show, there is a gain +0.9 points in F-score (mainly due to the boost in recall) after the addition of features related to negation scope.",There is also some minor improvement due to the proposed non-target entity specific features.,We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.(2005),"In each case, there were improvements in precision, recall and F- score.",The gain in F-score ranged from 1.0 to 1.4 points.,Single Citance
406,P05-1053.xml,P06-1016.xml,"For example, we want to determine whether a person is at a location, based on the evidence in the context.","Extraction of semantic relationships between entities can be very useful for applications such as question answering, e.g. to answer the query “Who is the president of the United States?”.",One major challenge in relation extraction is due to the data sparseness problem (Zhou et al 2005).,"As the largest annotated corpus in relation extraction, the ACE RDC 2003 corpus shows that different subtypes/types of relations are much unevenly distributed and a few relation subtypes, such as the subtype “Founder” under the type “ROLE”, suffers from a small amount of annotated data.",Further experimentation in this paper (please see Figure 2) shows that most relation subtypes suffer from the lack of the training data and fail to achieve steady performance given the current corpus size.,Single Citance
407,P05-1053.xml,P06-1016.xml,"Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance.","Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor sub- types given the much unevenly distribution among different relation subtypes.","While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations.",This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes.,"Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a top- down way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector.",Multi Citance
408,P05-1053.xml,P06-1016.xml,"Roth and Yih (2002) used the SNoW classifier to incorporate various features such as word, part-of-speech and semantic information from WordNet, and proposed a probabilistic reasoning approach to integrate named entity recognition and relation extraction.","Kambhatla (2004) employed maximum entropy models with features derived from word, entity type, mention level, overlap, dependency tree, parse tree and achieved F- measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus.","Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively.","To overcome the data sparseness problem, feature-based approaches normally incorporate various scales of contextsinto the feature vector extensively.",These ap proaches then depend on adopted learning algorithms to weight and combine each feature effectively.,Single Citance
409,P05-1053.xml,P06-1016.xml,"In this way, the bi nary class hierarchy can be built iteratively in a bottom-up way.",4 Experimentation This paper uses the ACE RDC 2003 corpus provided by LDC to train and evaluate the hierarchical learning strategy.,"Same as Zhou et al (2005), we only model explicit relations and explicitly model the argument order of the two mentions involved.","4.1 Experimental.Setting Type Subtype Freq Bin Type AT Based-In 347 Medium Located 2126 Large Residence 308 Medium Other 6 Small ROLE Affiliate-Partner 204 Medium Citizen-Of 328 Medium Client 144 Small Founder 26 Small General-Staff 1331 Large Management 1242 Large Member 1091 Large Owner 232 Medium Other 158 Small Table 1: Statistics of relation types and subtypes in the training data of the ACE RDC 2003 corpus (Note: According to frequency, all the subtypes are divided into three bins: large/ middle/ small, with 400 as the lower threshold for the large bin and 200 as the upper threshold for the small bin).",The training data consists of 674 documents (~300k words) with 9683 relation examples while the held-out testing data consists of 97 documents (~50k words) with 1386 relation examples.,Single Citance
410,P05-1053.xml,P06-1017.xml,"Compared with Zhou et al.’s method, the performance of LP algorithm is slightly lower.",It may be due to that we used a much simpler feature set.Our work in this paper focuses on the investigation of a graph based semi-supervised learning algorithm for relation extraction.,"In the future, we would like to use more effective feature sets Zhou et al.(2005)",5 Conclusion and Future Work.,This paper approaches the problem of semi- supervised relation extraction using a label propagation algorithm.,Single Citance
411,P05-1053.xml,P06-1104.xml,Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns.,"Miller et al.(2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model.","Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052 ) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.",These methods are very effective for relation extraction and show the best- reported performance on the ACE corpus.,"However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks.",Multi Citance
412,P05-1053.xml,P08-2023.xml,"Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans.","In the feature-based framework, Kambhatla (2004) employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree.","Based on his work, Zhou et al (2005)","Jiang and Zhai (2007) then systematically explored a large space of features and evaluated the effectiveness of different feature subspaces corresponding to sequence, syntactic parse tree and dependency parse tree.","Their experiments showed that using only the basic unit features within each feature subspace can already achieve state-of-art performance, while over-inclusion of complex features might hurt the performance.",Single Citance
413,P05-1053.xml,P09-1113.xml,Table 1 shows examples of relation instances extracted by our system.,We also use this system to investigate the value of syntactic versus lexi cal (word sequence) features in relation extraction.,"While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.","Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature.","Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations.",Multi Citance
414,P05-1053.xml,P09-1114.xml,"Empirical evaluation on the ACE 2004 data set shows that our proposed method largely outperforms two baseline methods, improving the average F1 measure from 0.1532 to 0.4132 when only 10 seed instances of the new relation type are used.",Recent work on relation extraction has been dominated by feature-based and kernel-based supervised learning methods.,Zhou et al.(2005) and Zhao and Grishman (2005) studied various features and feature combinations for relation extraction.,"We systematically explored the feature space for relation extraction (Jiang and Zhai, 2007b) . Kernel methods allow a large set of features to be used without being explicitly extracted.","A number of relation extraction kernels have been proposed, including dependency tree kernels (Culotta and Sorensen, 2004), shortest dependency path kernels (Bunescu and Mooney, 2005) and more recently convolution tree kernels (Zhang et al., 2006; Qian et al., 2008).",Pre Contiguous
415,P05-1053.xml,P09-1114.xml,Empirical evaluation on the ACE 2004 data set shows that the proposed method substantially improves over two baseline methods.,Relation extraction is the task of detecting and characterizing semantic relations between entities from free text.,"Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008).","However, supervised learning heavily relies on a sufficient amount of labeled data for training, which is not always available in practice due to the labor-intensive nature of human annotation.",This problem is especially serious for relation ex traction because the types of relations to be extracted are highly dependent on the application domain.,Multi Citance
416,P05-1053.xml,P11-1053.xml,"Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based.","Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing.","Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007).","In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008).","Both lines of work depend on effective features, either explicitly or implicitly.",Multi Citance
417,P05-1053.xml,P11-1053.xml,We now describe a supervised baseline system with a very large set of features and its learning strategy.,4.1 Baseline Feature Set.,"We first adopted the full feature set from Zhou et al.(2005), a state-of-the-art feature based relation extraction system.","For space reasons, we only show the lexical features as in Table 3 and refer the reader to the paper for the rest of the features.","At the lexical level, a relation instance can be seen as a sequence of tokens which form a five tuple <Before, M1, Between, M2, After>.",Single Citance
418,P05-1053.xml,P11-1053.xml,The clusters are available at http://www.cs.nyu.edu/~asun/data/TDT5_BrownW C.tar.gz.,"For relation extraction, we used the benchmark ACE 2004 training data.",Zhou et al.(2005) tested their system on the ACE 2003 data;.,Zhou et al.(2007) tested their system on the ACE 2004 data.,"10 The paper gives a recall value of 70.5, which is not.",Single Citance
419,P05-1053.xml,P11-1056.xml,"”, one would like to extract the relation that “the Seattle zoo” is located-at “Seattle”.","RE has been frequently studied over the last few years as a supervised learning task, learning from spans of text that are annotated with a set of semantic relations of interest.","However, most approaches to RE have assumed that the relationsâ€™ arguments are given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Jiang, 2009; Zhou et al., 2005), and therefore offer only a partial solution to the problem.","Conceptually, this is a rather simple approach as all spans of texts are treated uniformly and are being mapped to one of several relation types of interest.","However, these approaches to RE require a large amount of manually annotated training data to achieve good performance, making it difficult to expand the set of target relations.",Multi Citance
420,P05-1053.xml,P11-1056.xml,"In our work, although a few of our patterns involve semantic type comparison, most of the patterns are syntactic in nature.","In this work, we performed RE evaluation on the NIST Automatic Content Extraction (ACE) corpus.","Most prior RE evaluation on ACE data assumed that mentions are already pre-annotated and given as input (Chan and Roth, 2010; Jiang and Zhai, 2007; Zhou et al., 2005).","An exception is the work of (Kambhatla, 2004), where the author evaluated on the ACE-2003 corpus.","In that work, the author did not address the pipelined errors propagated from the mention identification process.",Multi Citance
421,P05-1053.xml,P11-3012.xml,"To generate the features for each of the mention pairs a proprietary JDPA Tokenizer is used for parsing the document and the Stanford Parser (Klein and Manning, 2003) is used to generate parse trees and part of speech tags for the sentences in the documents.",3.2 Features.,"We used Zhou et al.â€™s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).","Additional work has extended these features (Jiang and Zhai, 2007) or incorporated other data sources (e.g. WordNet), but in this paper we focus solely on the initial step of applying these same lexical features to the JDPA Corpus.","The Mention Level, Overlap, Base Phrase Chunk- ing, Dependency Tree, and Parse Tree features are the same as Zhou et al.(except for using the Stanford Parser rather than the Collins Parser).",Multi Citance
422,P05-1053.xml,P11-3012.xml,We ran our system on the ACE-2004 Corpus as a baseline to prove that the system worked properly and could approximately duplicate Zhou et al.’s results.,Using 5-fold cross validation on the newswire and broadcast news documents in the dataset we achieved an average overall F-Measure of 50.6 on the fine-grained relations.,"Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.",4.2 JDPA Sentiment Corpus Results.,"We randomly divided the JDPA Corpus into training (70%), development (10%), and test (20%) datasets.",Single Citance
423,P05-1053.xml,P13-1147.xml,Table 3 shows that our system (bottom) aligns well with the state of the art.,"Our best system (composite kernel with polynomial expansion) reaches an F1 of 70.1, which aligns well to the 70.4 of Sun et al.(2011) that use the same data- split.","This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitioning; ii) different pre-processing; iii) they incorporate features from additional sources, i.e. a phrase chunker, dependency parser and semantic resources (Zhou et al., 2005) (we have on average 9 features/instance, they use 40).","Since we focus on evaluating the impact of semantic similarity in tree kernels, we think our system is very competitive.",Removing gold entity and mention 12 Other weightings/normalizations (like LDA) didn’t improve the results; best was to take the posteriors and add c. 13 http://cs.nyu.edu/˜asun/pub/ACL11_CVFileList.txt information results in a significant F1 drop from 66.3% to 54.2%.,Single Citance
424,P05-1053.xml,W06-1634.xml,"Although such IE attempts have demonstrated near-practical performance, the same sets of patterns cannot be applied to different kinds of information.","A real-world task requires several kinds of IE, thus manually engineering extraction Current Afﬁliation: † FUJITSU LABORATORIES LTD. ‡ Faculty of Informatics, Kogakuin University patterns, which is tedious and time-consuming process, is not really practical.","Techniques based on machine learning (Zhou et al., 2005; Hao et al., 2005; Bunescu and Mooney, 2006) are expected to alleviate this problem in manually crafted IE.","However, in most cases, the cost of manually crafting patterns is simply transferred to that for constructing a large amount of training data, which requires tedious amount of manual labor to annotate text.","To systematically reduce the necessary amount of training data, we divided the task of constructing extraction patterns into a subtask that general natural language processing techniques can solve and a subtask that has speciﬁc properties according to the information to be extracted.",Multi Citance
425,P05-1053.xml,W06-1634.xml,"As an actual IE task, we extracted pairs of interacting protein names from biomedical text.",2.1 Necessity for Full Parsing.,"A technique that many previous approaches have used is shallow parsing (Koike et al., 2003; Yao et al., 2004; Zhou et al., 2005).","Their assertion is 284 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284–292, Sydney, July 2006.",Qc 2006 Association for Computational Linguistics Distance Count (%) Sum (%) −1 0 1 2–5 6–10 11– 54 8 170 337 267 248 5.0 0.7 15.7 31.1 24.6 22.9 5.0 5.7 21.4 52.5 77.1 100.0 Distance −1 means protein word has been annotated as interacting with itself (e.g. “actin polymerization”).,Multi Citance
426,P05-1053.xml,W06-1667.xml,"For example, the relation subtypes “Located”, “Based-In” and “Residence” are difficult to disambiguate even for human experts to differentiate.",The results also show that various lexical and syntactic features contain useful information for the task.,"Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs.",Another observation from the result is that extending the outer context window of entity mention pairs too much may not improve the performance since the process may incorporate more noise information and affect the clustering result.,"As regards the clustering technique, the spectral- based clustering performs better than direct clustering, K-means.",Multi Citance
427,P05-1053.xml,W08-0602.xml,Such approaches have typically been applied to journal texts.,"They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)).",This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).,"Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition.","The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Na¨ıve Bayes classifier to identify entities for information retrieval applications.",Single Citance
428,P05-1053.xml,W08-0602.xml,4.2 Features for Classification.,The SVM classification model is built from lexical and syntactic features assigned to tokens and entity pairs prior to classification.,We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).,"These features are split into 11 sets, as described in Table 3.",The tokN features are POS and surface string taken from a window of N tokens on each side of each paired entity’s mention.,Multi Citance
429,P05-1053.xml,W11-1101.xml,"Using this, they apply rich linguistic and knowledge- based constraints based on coreference annotations, a hierarchy of relations, syntacto-semantic structure, and knowledge from Wikipedia.","In our work, we focus on capturing the latent semantics of the text between the NEs.","A variety of features have been explored for ERD in previous research (Zhou et al., 2005; Zhou et al., 2008; Jiang and Zhai, 2007; Miller et al., 2000).","Syntactic features such as POS tags and dependency path between entities; semantic features such as Word-Net relations, semantic parse trees and types of NEs; and structural features such as which entity came first in the sentence have been found useful for ERD.",We too observe the utility of informative features for this task.,Multi Citance
430,P05-1053.xml,W11-1815.xml,Among them the host and host-part locations have to be related by the part-of relation.,Three teams participated in the challenge.,"BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).",Locations include natural environments and hosts as well as food and medical locations.,"In order to deal with this heterogeneity, we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope (BB) terminoontology.",Multi Citance
431,P06-2124.xml,D11-1084.xml,"Finally, we conclude this paper in Section 6.",There are only a few studies on document-level SMT.,"Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).",Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.,"It shows that the performance of word alignment can be improved with the help of document-level information, which indirectly improves the quality of SMT.",Pre Contiguous
432,P06-2124.xml,D11-1084.xml,There are only a few studies on document-level SMT.,"Representative work includes Zhao et al.(2006), Tam et al.(2007), Carpuat (2009).",Zhao et al.(2006) assumed that the parallel sentence pairs within a document pair constitute a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model.,Tam et al.(2007) proposed a bilingual-LSA model on the basis of a parallel document corpus and built a topic-based language model for each language.,"By automatically building the correspondence between the source and target language models, this method can match the topic-based language model and improve the performance of SMT.",Single Citance
433,P06-2124.xml,P07-1066.xml,"The beauty of the bLSA framework is that the model searches for a common latent topic space in an unsupervised fashion, rather than to require manual interaction.","Since the topic space is language independent, our approach supports topic transfer in multiple language pairs in O(N) where N is the number of languages.","Related work includes the Bilingual Topic Admixture Model (BiTAM) for word alignment proposed by (Zhao and Xing, 2006).","On the other hand, the bLSA framework models P r(c|k) and P r(e|k) which is different from the BiTAM model.","By their different modeling nature, the bLSA model usually supports more topics than the BiTAM model.",Single Citance
434,P06-2124.xml,P10-2025.xml,"cluded in the bilingual training set.We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vo- gel et al., 1996; Och and Ney, 2003), and HMBiTAM (Zhao and Xing, 2008) implemented by us.","GIZA++ is an implementation of IBM-model 4 and HMM, and HMBiTAM corresponds to ζ = 0 in eq. 7.","We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).","We trained the word alignment in two directions: English to French, and French to English.","The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).",Single Citance
435,P06-2124.xml,P10-2025.xml,"We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006).","We trained the word alignment in two directions: English to French, and French to English.","The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006).","We evaluated these results for precision, recall, F- measure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000).",1 http://fjoch.com/GIZA++.html 1 0 k Prec isio n Reca llF mea sure AE R GI Z A ++ sta ndar d with SR H 0.,Multi Citance
436,P06-2124.xml,P11-2032.xml,"In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter space, but does not consider contributions from other points.","Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported.","Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution.","Bayesian inference, the approach in this paper, have recently been applied to several unsupervised learning problems in NLP (Goldwater and Griffiths, 2007; Johnson et al., 2007) as well as to other tasks in SMT such as synchronous grammar induction (Blunsom et al., 2009) and learning phrase alignments directly (DeNero et al., 2008).","Word alignment learning problem was addressed jointly with segmentation learning in Xu et al.(2008), Nguyen et al.(2010), and Chung and Gildea (2009).",Single Citance
437,P06-2124.xml,P12-1048.xml,"In this paper, we propose a novel adaptation method to adapt the translation model for domain- specific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, Jeju, Republic of Korea, 814 July 2012.",Qc 2012 Association for Computational Linguistics monolingual corpora.,"Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection.","For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inha´ng”, and occurs in the sentences related to the geography topic when translated to “he´a`n”.","Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases.",Multi Citance
438,P06-2124.xml,P12-1048.xml,"Final experimental results show that without using any additional resources, these approaches all improve SMT performance sig nificantly.","Our method deals with translation model adaptation by making use of the topical context, so let us take a look at the recent research development on the application of topic models in SMT.","Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity.","Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to cross- lingual language modeling and translation lexicon adaptation.","Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT.",Single Citance
439,P06-2124.xml,P12-1048.xml,"As compared to the above-mentioned works, our work has the following differences.","• We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual cor pora, which are far from full exploitation in the parallel data collection and mixture modeling framework.","â€¢ In addition to the utilization of in-domain monolingual corpora, our method is different from the previous works (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010) in the following aspects: (1) we use a different topic model â€” HTMM which has different assumption from PLSA and LDA; (2) rather than modeling topic-dependent translation lexicons in the training process, we estimate topic-specific lexical probability by taking account of topical context when extracting word pairs, so our method can also be directly applied to topic-dependent phrase probability modeling.","(3) Instead of rescoring phrase pairs online, our approach calculate the translation probabilities offline, which brings no additional burden to translation systems and is suitable to translate the texts without the topic distribution information.","• Different from trigger-based lexicon model and context-dependent translation selection both of which put emphasis on solving the translation ambiguity by the exploitation of the context information at the sentence level, we adopt the topical context information in our method for the following reasons: (1) the topic information captures the context information beyond the scope of sentence; (2) the topical context information is integrated into the posterior probability distribution, avoiding the sparseness of word or POS features; (3) the topical context information allows for more fine-grained distinction of different translations than the genre information of corpus.",Multi Citance
440,P06-2124.xml,P12-1079.xml,Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.,"Topic model (Hofmann, 1999; Blei et al., 2003) is a popular technique for discovering the underlying topic structure of documents.","To exploit topic information for statistical machine translation (SMT), researchers have proposed various topic-specific lexicon translation models (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007) to improve translation quality.",Topic-specific lexicon translation models focus on word-level translations.,"Such models first estimate word translation probabilities conditioned on topics, and then adapt lexical weights of phrases ∗ Corresponding author by these probabilities.",Multi Citance
441,P06-2124.xml,P12-1079.xml,"Based on these parameters (and some hyper-parameters), LDA can infer a topic assignment for each word in the documents.","In the following sections, we will use these parameters and the topic assignments of words to estimate the parameters in our method.","Sentences should be translated in consistence with their topics (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007).","In the hierarchical phrase based system, a synchronous rule may be related to some topics and unrelated to others.","In terms of probability, a rule often has an uneven probability distribution over topics.",Multi Citance
442,P06-2124.xml,P12-2023.xml,This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation.,"In this work, we consider the underlying latent topics of the documents (Blei et al., 2003).","Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment.","In our case, by building a topic distribution for the source side of the training data, we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership.",This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics.,Multi Citance
443,P06-2124.xml,P13-2122.xml,"The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context.","Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data.","To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) or â€˜biTAMâ€™ (Zhao and Xing, 2006).","In contrast to our source language approach, these authors use both source and target information.","Perhaps most relevant are the approaches of Gong et al.(2010) and Eidelman et al.(2012), who both describe adaptation techniques where monolingual LDA topic models are used to obtain a topic distribution over the training data, followed by dynamic adaptation of the phrase table based on the inferred topic of the test document.",Multi Citance
444,P06-2124.xml,W07-0722.xml,This semantic dependency problem could be overcome by learning topic-dependent translation models that capture together the semantic context and the translation process.,"However, there have not been until very recently that the application of mixture modelling in SMT has received increasing attention.","In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.",These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.,The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task.,Post Contiguous
445,P06-2124.xml,W07-0722.xml,"Finally, the main problem in mixture modelling is the linear growth of the set of parameters as the number of components increases.","In the HMM, and also in IBM models, this problem is aggravated because of the use of statistical dictionary entailing a large number of parameters.","A possible solution is the implementation of interpolation techniques to smooth sharp distributions estimated on few events (Och and Ney, 2003; Zhao and Xing, 2006).",$,$,Multi Citance
446,P07-1040.xml,C08-1014.xml,"Furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the N-best list (Ueffing, 2003; Chen et al., 2005; Zens and Ney, 2006).","In this two-pass method, translation performance hinges on the N-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other MT systems to these hypotheses could potentially improve the performance.","This technique is called system combination (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).","We have instead chosen to regenerate new hypotheses from the original N-best list, a technique which we call regeneration.",Regeneration is an intermediate pass between decoding and rescoring as depicted in Figure 2.,Pre Contiguous
447,P07-1040.xml,C08-1014.xml,"N-gram expansion (Chen et al., 2007) regenerates more hypotheses by continuously expanding the partial hypotheses through an n-gram language model trained on the original N-best translations.","And confusion network generates new hypotheses based on confusion network decoding (Matusov et al., 2006), where the confusion network is built on the original N-best translations.","Confusion network and re-decoding have been well studied in the combination of different MT systems (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b).",Researchers have used confusion network to compute consensus translations from the outputs of different MT systems and improve the performance over each single systems.,"(Rosti et al., 2007a) also used re-decoding to do system combination by extracting sentence-specific phrase translation tables from the outputs of different MT systems and running a phrase-based decoding with this new translation table.",Multi Citance
448,P07-1040.xml,D09-1115.xml,System combination aims to find consensus translations among different machine translation systems.,"It has been proven that such consensus translations are usually better than the output of individual systems (Frederking and Nirenburg, 1994).","In recent several years, the system combination methods based on confusion networks developed rapidly (Bangalore et al., 2001; Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007a; Rosti et al., 2007b; Rosti et al., 2008; He et al., 2008), which show state-of-the-art performance in benchmarks.",A confusion network consists of a sequence of sets of candidate words.,Each candidate word is associated with a score.,Multi Citance
449,P07-1040.xml,D09-1115.xml,"In confusion network decoding, a translation is generated by traveling all the nodes from left to right.",So a translation path contains all the nodes.,"While in lattice decoding, a translation path may skip some nodes as some hypothesis arcs may cross more than one backbone arc. Similar to the features in Rosti et al.(2007a), the features adopted by lattice-based model are arc posterior probability, language model probability, the number of null arcs, the number of hypothesis arcs possessing more than one non-null word and the number of all non-null words.",The features are combined in a log-linear model with the arc posterior probabilities being processed specially as follows: ment pair and the lattice are the same.,"Let {harcl } denotes the set of hypothesis arcs,which come from the same node with the cur log p(e/f ) = Narc ) i=1 Ns log () s=1 λsps(arc)) (9) rent backbone arc in the lattice, and harch denotes one of the corresponding hypothesis arcs in the given alignment pair.",Single Citance
450,P07-1040.xml,D09-1115.xml,Nword(e) is the non-null word number and γ is its weight.,"Each arc has different confidences concerned with different systems, and the confidence of system s is denoted by ps(arc).","ps(arc) is increased by 1/(k + 1) if the hypothesis ranking k in the system s contains the arc (Rosti et al., 2007a; He et al., 2008).","Cube pruning algorithm with beam search is employed to search for the consensus output (Huang and Chiang, 2005).",The nodes in the lattice are searched in a topological order and each node retains a list of N best candidate partial translations.,Pre Contiguous
451,P07-1040.xml,N09-2003.xml,"However, the reference translations may not be reachable by the translation system, in which case the oracle-best hypotheses should be substituted in training.","9 Proceedings of NAACL HLT 2009: Short Papers, pages 9–12, Boulder, Colorado, June 2009.","Qc 2009 Association for Computational Linguistics System Combination: In a typical system combination task, e.g. Rosti et al.(2007), each component system produces a set of translations, which are then grafted to form a confusion network.","The confusion network is then rescored, often employing additional (language) models, to select the final translation.","When measuring the goodness of a hypothesis in the confusion network, one requires its score under each component system.",Single Citance
452,P07-1040.xml,P08-2021.xml,$,"Given several systems’ automatic translations of the same sentence, we show how to combine them into a confusion network, whose various paths represent composite translations that could be considered in a subsequent rescoring step.","We build our confusion networks using the method of Rosti et al.(2007), but, instead of forming alignments using the tercom script (Snover et al., 2006), we create alignments that minimize invWER (Leusch et al., 2003), a form of edit distance that permits properly nested block movements of substrings.",Oracle experiments with Chinese newswire and weblog translations show that our confusion networks contain paths which are significantly better (in terms of BLEU and TER) than those in tercom-based confusion networks.,Large improvements in machine translation (MT) may result from combining different approaches to MT with mutually complementary strengths.,Multi Citance
453,P07-1040.xml,P08-2021.xml,"Unfortunately, monotone alignments are often poor, since machine translations (particularly from different models) can vary significantly in their word order.","Thus, when Matusov et al.(2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment.","The procedure described by Rosti et al.(2007) has been shown to yield significant improvements in translation quality, and uses an estimate of Translation Error Rate (TER) to guide the alignment.","(TER is defined as the minimum number of inser 81 Proceedings of ACL08: HLT, Short Papers (Companion Volume), pages 81–84, Columbus, Ohio, USA, June 2008.","Qc 2008 Association for Computational Linguistics tions, deletions, substitutions and block shifts between two strings.)",Single Citance
454,P07-1040.xml,P08-2021.xml,"A remarkable feature of that procedure is that it performs the alignment of the output translations (i) without any knowledge of the translation model used to generate the translations, and (ii) without any knowledge of how the target words in each translation align back to the source words.","In fact, it only requires a procedure for creating pairwise alignments of translations that allow appropriate re-orderings.","For this, Rosti et al.(2007) use the tercom script (Snover et al., 2006), which uses a number of heuristics (as well as dynamic programming) for finding a sequence of edits (insertions, deletions, substitutions and block shifts) that convert an input string to another.","In this paper, we show that one can build better confusion networks (in terms of the best translation possible from the confusion network) when the pairwise alignments are computed not by tercom, which approximately minimizes TER, but instead by an exact minimization of invWER (Leusch et al., 2003), which is a restricted version of TER that permits only properly nested sets of block shifts, and can be computed in polynomial time.","The paper is organized as follows: a summary of TER, tercom, and invWER, is presented in Section",Pre Contiguous
455,P07-1040.xml,P08-2021.xml,"In fact, out of the roughly 2000 total segments in all languages/genres, tercomTER gives a lower number of edits in only 8 cases!",This is a clear indication that ITGs can explore the space of string permutations more effectively than tercom.,ITG-based alignments and tercom-based alignments were also compared in oracle experiments involving confusion networks created through the algorithm of Rosti et al.(2007).,"The algorithm entails the following steps: • Computation of all pairwise alignments between system hypotheses (either using ITGs or tercom); for each pair, one of the hypotheses plays the role of the “reference”.","• Selection of a system output as the “skeleton” of the confusion network, whose words are used as anchors for aligning all other machine translation outputs together.",Single Citance
456,P07-1040.xml,P08-2021.xml,aligned with an epsilon arc of the confusion network.,• Setting the weight of each arc equal to the negative log (posterior) probability of its label; this probability is proportional to the number of systems which output the word that gets aligned in that location.,Note that the algorithm of Rosti et al.(2007) used N -best lists in the combination.,"Instead, we used the single- best output of each system; this was done because not all systems were providing N -best lists, and an unbalanced inclusion would favor some systems much more than others.","Furthermore, for each genre, one of our MT systems was significantly better than the others in terms of word order, and it was chosen as the skeleton.",Single Citance
457,P07-1040.xml,P11-1125.xml,"For example, the last hypothesis in Figure 1(a) has a passive voice grammatical construction while the others are active voice.","This large grammatical difference may produce a longer sentence with spuriously inserted words, as in I saw the blue trees was found in Figure 1(c).",Rosti et al.(2007b) partially resolved the problem by constructing a large network in which each hypothesis was treated as a skeleton and the multiple networks were merged into a single network.,The confusion network approach to system combination encodes multiple hypotheses into a compact lattice structure by using word-level consensus.,"Likewise, we propose to encode multiple hypotheses into a confusion forest, which is a packed forest which represents multiple parse trees in a polynomial space (Billot and Lang, 1989; Mi et al., 2008) Syntactic consensus is realized by sharing tree frag S@ϵ VP@2 Initialization: Scan: [TOP → •S, 0] : ¯1 NP@1 VBD@3 VP@4 VBD@2.1 [X → α • xβ, h] : u [X → αx • β, h] : u PRP DT NN was VBN walke d saw NP@2.",Pre Contiguous
458,P07-1040.xml,P11-1125.xml,"where e = yield(d) is a terminal yield of d, BP (·) and ρn(·) respectively denote brevity penalty andn-gram precision.","Here, we use approximated unclipped n-gram counts (Dreyer et al., 2007) for com puting ρn (·) with a compact state representation (Li and Khudanpur, 2009).","Our baseline confusion network system has an additional penalty feature, hp (m), which is the total edits required to construct a confusion network using the mth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).",5.3 Results.,"Table 2 compares our confusion forest approach (CF) with different orders, a confusion network (CN) and max/min systems measured by BLEU (Pa- pineni et al., 2002).",Single Citance
459,P07-1040.xml,P39_p07.xml,"However, the reference translations may not be reachable by the translation system, in which case the oracle-best hypotheses should be substituted in training.","9 Proceedings of NAACL HLT 2009: Short Papers, pages 912, Boulder, Colorado, June 2009.","@2009 Association for Computational Linguistics System Combination: In a typical system combi nation task, e.g. Rosti et al.(2007), each compo nent system produces a set of translations, which are then grafted to form a confusion network.","The confusion network is then rescored, often employ­ ing additional (language) models, to select the fi­ nal translation.","When measuring the goodness of a hypothesis in the confusion network, one requires its score under each component system.",Single Citance
460,P07-1040.xml,Psem_p07.xml,"In addition to assigning a score to a hypothesis, TER provides an alignment between the hypothesis and the reference, enabling it to be useful beyond general translation evaluation.","While TER has been shown to correlate well with translation quality, it has several flaws: it only considers exact matches when measuring the similarity of the hypothesis and the reference, and it can only compute this measure of similarity against a single reference.",The handicap of using a single reference can be addressed by constructing a lattice of reference translationsthis technique has been used to combine the output of multiple translation systems (Rosti et al. 2007).,TERp does not utilize this methodology and instead addresses the exact matching flaw of TER.,"In addition to aligning words in the hypothesis and reference if they are exact matches, TERp uses stemming and synonymy to allow matches between words.",Single Citance
461,P07-1040.xml,Psem_p07.xml,"Words in bold are shifted, while square brackets are used to indicate other edit types: P for phrase substitutions, T for stem matches, Y for synonym matches, D for deletions, and I for insertions.",These alignments allow TERp to provide quality judgments on translations and to serve as a diagnostic tool for evaluating particular types of translation errors.,"In addition, it may also be used as a general-purpose string alignment toolTER has been used for aligning multiple system outputs to each other for MT system combination (Rosti et al. 2007), a task for which TERp may be even better suited.",TER-Plus: enhancements to Translation Edit Rate Fig.,1 Examples of TERp alignment output.,Single Citance
462,P07-1040.xml,W08-0329.xml,Confusion network decoding has been applied in combining outputs from multiple machine translation systems.,"The earliest approach in (Bangalore et al., 2001) used edit distance based multiple string alignment (MSA) (Durbin et al., 1988) to build the confusion networks.","The recent approaches used pairwise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al., 2006) or edit distance alignments allowing shifts (Rosti et al., 2007).",The alignment method described in this paper extends the latter by incrementally aligning the hypotheses as in MSA but also allowing shifts as in the TER alignment.,The confusion networks are built around a “skeleton” hypothesis.,Multi Citance
463,P07-1040.xml,W08-0329.xml,The incremental hypothesis alignment algorithm combines these two steps.,"All words from the previously aligned hypotheses are available, even if not present in the skeleton hypothesis, when aligning the following hypotheses.","As in (Rosti et al., 2007), confusion networks built around all skeletons are joined into a lattice which is expanded and re- scored with language models.",System weights and language model weights are tuned to optimize the quality of the decoding output on a development set.This paper is organized as follows.,The incremental TER alignment algorithm is described in Section 2.,Single Citance
464,P07-1040.xml,W08-0329.xml,Each match found will increase the system specific word arc confidence by where Figure 2: Network after incremental TER alignment.,each set of two consecutive nodes.,"Other scores for the word arc are set as in (Rosti et al., 2007).",2.1 Benefits over PairWise TER Alignment.,The incremental hypothesis alignment guarantees that insertions between a hypothesis and the current confusion network are always considered when aligning the following hypotheses.,Single Citance
465,P07-1040.xml,W08-0329.xml,The scores may be from different systems as the best performing system in terms of TER was not necessarily the best performing system in terms of the other metrics.,The following three rows show the scores of three combination outputs where the only difference was the hypothesis alignment method.,"The first, syscomb pw, corresponds BLEU System deen fren worst 11.84 16.31 best 28.30 33.13 syscomb 29.05 33.63 Table 3: NIST BLEU scores on the GermanEnglish (deen) and French-English (fren) Europarl test2008 set.to the pairwise TER alignment described in (Rosti et al., 2007).","The second, syscomb giza, corresponds to the pairwise symmetric HMM alignments from GIZA++ described in (Matusov et al., 2006).","The third, syscomb inc, corresponds to the incremental TER alignment presented in this paper.",Multi Citance
466,P07-1040.xml,W09-0441.xml,"While TER has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference.",The handicap of using a single reference can be addressed by the construction of a lattice of reference translations.,"Such a technique has been used with TER to combine the output of multiple translation systems (Rosti et al., 2007).",TERp does not utilize this methodology2 and instead focuses on addressing the exact matching flaw of TER.,"A brief description of TER is presented in Section 2.1, followed by a discussion of how TERp differs from TER in Section 2.2.",Pre Contiguous
467,P98-1046.xml,A00-2034.xml,Au­ tomatic identification of alternations would be a use­ ful tool for extending the classification with new participants.,"Levin's taxonomy might also be used alongside observed behaviour, to predict unseen be­ haviour.","Levin&apos;s classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., ",Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.,Dorr and Jones (1996) extend the classification by using grammatical in­ formation in LDOCE alongside semantic information in WordNet.What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource.,Single Citance
468,P98-1046.xml,A00-2034.xml,"Levin's classification has been extended by other NLP researchers (Dorr and Jones, 1996; Dang et al., <Root> TCM war car migration meal drum time ceremonial migration week month afternoon s p e e c h y e l l i n g Figure 1: TCM for the object slot of the transitive frame of start.",1998).,Dang et al.(1998) modify it by adding new classes which remove the overlap between classes from the original scheme.,Dorr and Jones (1996) extend the classification by using grammatical in­ formation in LDOCE alongside semantic information in WordNet.What is missing is a way of classifying verbs when the relevant information is not available in a manmade resource.,Using corpora bypasses reliance on the availability and adequacy of MRDs.,Single Citance
469,P98-1046.xml,C00-2118.xml,"The experimenta.l results show that our rncthod is powerful, and suited to the classification of lex­ ical items.","However, we have not yet addressed the problem of verbs that can have tnultiple clas­ sifications.","\tVe think that many cases of amÂ­ biguous classification of verb types can be adÂ­ dressed with the notion of intersedive sets inÂ­ troduced by (Dang ct a!., 1998).","This is a.n im­ portant concept that proposes that ""regular"" a.m­ biguity in classification--i.e., sets of verbs that ha.ve the same multi-way classifications according to (Levin, J gg;n--can be captured with a. finer­ gra.i ned notion of l< xical semantic class< s. l x­ tencling our work to exploit this idea requires only to define the classes appropriately; the ba­ sic approach will remain the c;amc.","\Vhen we tmn to consider ambiguity, we must also address the problem that individua.l instances of verbs may come from different classes.",Single Citance
470,P98-1046.xml,C00-2148.xml,"However, Levin classes exhibit in­ consistencies that have hampered researchers' abil­ ity to reference them directly in applications.","Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.","Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes.",We represent these verb classes and their regular sense extensions in the LTAG formalism.,3.1 Overview of formalism.,Pre Contiguous
471,P98-1046.xml,C08-1002.xml,VerbNet.VN is a major electronic English verb lexicon.,"It is organized in a hierarchical structure of classes and subclasses, each subclass inheriting the full characterization of its super-class.","VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.","VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004).","VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type).Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class).",Single Citance
472,P98-1046.xml,J04-1003.xml,"In contrast, of the verbs licensing six different alternations, 14% have one class, 17% have two classes, 12.4% have three classes, 53.6% have four classes, 2% have six classes, and 1% have seven classes.","As ambiguity increases, so does the availability and potential utility of information about diathesis alternations.",Palmer (2000) and Dang et al.(1998) argue that syntactic frames and verb classes are useful for developing principled classifications of verbs.,"We go beyond this, showing that they can also be of assistance in disambiguation.","Consider, for instance, the verb serve, which is a member of four Levin classes: Give, Fit, Masquerade, and Fulfilling.",Multi Citance
473,P98-1046.xml,J05-1004.xml,"Additional semantic information for the verbs is expressed as a set (i.e., conjunction) of semantic predicates, such as motion, contact, transfer_info.","Currently, all Levin verb classes have been assigned thematic labels and syntactic frames, and over half the classes are completely described, including their semantic predicates.","In many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levinâ€™s original classes, adding an additional level to the hierarchy (Dang et al. 1998).",We are also extending the coverage by adding new classes (Korhonen and Briscoe 2004).,"Our objective with the Proposition Bank is not a theoretical account of how and why syntactic alternation takes place, but rather to provide a useful level of representation and a corpus of annotated data to enable empirical study of these issues.",Single Citance
474,P98-1046.xml,N09-1057.xml,These facts about participation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not.,"Grammatically relevant semantic properties of events and their 2 Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006).","participants â€” causation, change of state, and others â€” are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g.(Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)).",The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics.,"First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb.",Multi Citance
475,P98-1046.xml,P03-1009.xml,"In contrast to previous work, we particularly focus on clustering polysemic verbs.","A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated S C F data.","Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.(Jack- endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).","While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.",∗This work was partly supported by UK EPSRC project GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’.,Multi Citance
476,P98-1046.xml,P06-1117.xml,"Thus, from a syntactic point of view, the verbs in one Levin class have a regular behavior, different from the verbs pertaining to other classes.","Also, the classes are semantically coherent and all verbs belonging to one class share the same participant roles.","This constraint of having the same semantic roles is further ensured inside the VN lexicon which is constructed based on a more refined version of the Levinâ€™s classification, called Intersective Levin classes (ILCs) (Dang et al., 1998).",The lexicon provides a regular association between the syntactic and semantic properties of each of the described classes.,It also provides information about the syntactic frames (alternations) in which the verbs participate and the set of possible semantic roles.,Single Citance
477,P98-1046.xml,P99-1051.xml,Levin (1993) assumes that the syntactic realiza­ tion of a verb's arguments is directly correlated with its meaning (cf.also Pinker (1989) for a similar pro­ posal).,Thus one would expect verbs that undergo the same alternations to form a semantically co­ herent class.,"Levin&apos;s study on diathesis alternations has influenced recent work on word sense disamÂ­ biguation (Dorr and Jones, 1996), machine translaÂ­ tion (Dang et al., 1998), and automatic lexical acÂ­ quisition (McCarthy and Korhonen, 1998; Schulte im Walde, 1998).","The part-of-speech tagged version of the British Na­ tional Corpus (BNC), a 100 million word collec­ tion of written and spoken British English (Burnard, 1995), was used to acquire the frames characteris­ tic of the dative and benefactive alternations.","Sur­ face syntactic structure was identified using Gsearch (Keller et al., 1999), a tool which allows the search of arbitrary POS-tagged corpora for shallow syntac­ tic patterns based on a user-specified context-free grammar and a syntactic query.",Multi Citance
478,P98-1046.xml,W00-0202.xml,"The PAR for an action includes the action&apos;s participants (its agent and objects),&apos; as well as kinematic properties such as its path, manner and duration, and dynamic properties, such as its speed and force (see Fig.","1).The representation also allows for traditional statespace properties of actions, such as applicability conditions and preparatory actions that have to be satisfied before the action can be executed, and termina tion conditions and post assertions which determine when an action is concluded and what changes it makes to the environment state.","We created a hierarchy of actions, exploiting the idea that verbs can be represented in a lattice that allows semantically similar verbs, such as motion verbs or verbs of contact, to be closely associatedwith each other under a common parent that cap tures the properties these verbs all share (Dang et al., 1998).",The highest nodes in the hierarchy areoccupied by generalized PAR schemas which represent the basic predicate-argument structures for en tire groups of subordinate actions.,"The lower nodes are occupied by progressively more specific schemas that inherit information from the generalized PARS,and can be instantiated with arguments from natu ral language to represent a specific action such asJohn hit the ball with his bat.",Single Citance
479,P98-1046.xml,W02-1108.xml,"It provides a classification of around 3200 verbs into 48 classes (some ofwhich are split further into more distinctive sub classes, making the total number of 191 classes)according to their participation in 79 alterna tions involving NP and PP complements only.",Work on refining the existent semantic classes and composing novel ones is under way.,"Dang et al.(1998), for example, have refined the current classification by creating intersective classes for those verbs which share membership of more than one Levin class.",Dorr (1997) has created new classes for verbs whose syntactic behaviour differs from the syntactic description of existing Levin classes.,Korhonen (2002b) has proposednovel alternations not covered by Levin — par ticularly those involving sentential complements — with the aim to create new classes for further verb types.,Single Citance
480,P98-1046.xml,W03-0910.xml,Much previous work in the domain of classification of lexical items has focused on verbs This is a natural starting place since the verb is the hook upon which the rest of a sentence hangs Verbs also display a higher degree of variation in their semantics than other lexical types Whereas nouns for example all name some kind of thing verbs can describe an action or a state and involve some number of nonverbal actors in the description It is this latter quality of verbs which most interests us here and indeed which has been the focus of most previous work in verb classification Even the most elementary grammars of English draw a distinction between transitive and intransitive verbs More advanced work has refined this distinction into ever- larger numbers of classes For example the landmark work of Levin (1993) divided over three thousand verbs into 191 classes based partly on shared semantics and partly on shared syntactic alternations More recently the VerbNet project at Penn (Kipper et al 2000) incorporated Levin s verb classification to systematically create verb entries in a lexicon On a purely semantic note WordNet (Miller 1985 Fellbaum 1998) has classified much of the vo-.,cabulary of English not just verbs in terms of relationships such as synonymy hyponymy and others Various attempts worldwide have begun focussing on the argument structure of verbs as part of developing dependency grammars The PropBank project at Penn (Kingsbury and Palmer 2002) is an example of this process for English similar projects are underway for Czech (Hajicova etc) German (Broker1998) and others The FrameNet project at Berke ley (Baker et al 1998) has classified many words in terms of their relation to a relatively small number of core semantic concepts such as commerce and judgment Various attempts have been made toautomatically cluster verbs into semantically meaningful classes using the Levin class as a gold stan dard for evaluation (Gildea 2002 McCarthy 2000 Merlo and Stevenson 2001 Schulte im Walde 2000) In the next two sections we provide background on VerbNet and PropBank which play central roles in the cluster methodology presented here 2.1 VerbNet.VerbNet is a verb lexicon with syntactic and semantic information for English verbs referring to Levin verb classes (Levin 1993) for systematic construction of lexical entries This lexicon exploits the systematic link between syntax and semantics that motivates these classes and thus provides a clear and regular association between syntactic and semantic properties of verbs and verb classes (Kipper et al 2000 Dang et al 2000) Each class in the hier-.,This reorganization which was facilitated by the use of inter- sective Levin classes (Dang et al 1998) refined the classes to account for semantic and syntactic differences within a class,"In a different vein the PropBank project (Kingsbury and Palmer 2002) has endeavoured to describe all the most frequent verbs of English in terms of their argument structure This project has three major differences from previous works First the description of each verb is accompanied by a rich set of examples drawn from real language in this case the Wall Street Journal sections of the Penn Treebank (Marcus 1994) Furthermore the descriptions are based on the usages in the corpus rather than a possible situation where the corpus was mined for sentences fitting preconceived patterns This results in many instances which are perfectly well-formed but unexpected The best example of this is an odd usage of the verb add The Nasdaq composite index added 1 01 to 456 6 on pa try vo ume where the context makes it clear that add is being used as a synonym for rise Second argument structure allows for a richer set of descriptions than merely transitive unaccusative and so forth since any individual verb is allowed to have anywhere between zero and six arguments Third and perhaps most importantly the PropBank descriptions make explicit mention of the different senses of verbs This is crucial because different senses can have different argument structures or different syntactic alternations a detail which is often glossed over in other resources Thus while (1) Recipient) as in ""John taught math to Mary"" Both 1 syntactic frames have semantic predicates expressing Presently there are 64 distinct predicates described.",Transfer of a Message - eve 1 class (( E ERS )) [((cat wn) ((temwnatrat wn) (te:tprcw wn) ..,Single Citance
481,P98-1046.xml,W04-2606.xml,"We also introduce 106 novel diathesis alternations, created as a side product of constructing the new classes.",We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lexicon.,"Lexical-semantic classes which aim to capture the close relationship between the syntax and semantics of verbs have attracted considerable interest in both linguistics and computational linguistics (e.g.(Pinker, 1989; Jackendoff, 1990; Levin, 1993; Dorr, 1997; Dang et al., 1998; Merlo and Stevenson, 2001)).","Such classes can capture generalizations over a range of (cross-)linguistic properties, and can therefore be used as a valuable means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.Verb classes have proved useful in various (multilin gual) natural language processing (NL P) tasks and applications, such as computational lexicography (Kipper et al., 2000), language generation (Stede, 1998), machine translation (Dorr, 1997), word sense disambigua tion (Prescher et al., 2000), document classification (Kla- vans and Kan, 1998), and subcategorization acquisition (Korhonen, 2002).","Fundamentally, such classes define the mapping from surface realization of arguments to predicate-argument structure and are therefore a critical component of any NL P system which needs to recover predicate-argument structure.",Multi Citance
482,P98-1046.xml,W06-2611.xml,"Thus, from a syntactic point of view, the verbs in one Levin class have a regular behavior, different from the verbs pertaining to other classes.","Also, the classes are semantically coherent and all verbs belonging to one class share the same participant roles.","This constraint of having the same semantic roles is further ensured inside the VerbNet lexi con that is constructed based on a more refined version of the Levin classification called Inter- sective Levin classes (Dang et al., 1998).",The lexicon provides a regular association between the syntactic and semantic properties of each of the described classes.,It also provides information about the syntactic frames (alternations) in which the verbs participate and the set of possible semantic roles.,Single Citance
483,P98-1046.xml,W99-0503.xml,butlons that approx1mate the relevant hngutstiC properties?,tmgmsh the verb classes?,"In explormg these quest1ons, we focus on verb clasÂ­ Sificatwn for several reasons Verbs are very ImporÂ­ tant sources of knowledge m many language eng1neermg tasks, and the relat10nsh1ps among verbs apÂ­ pear to play a maJor role m the orgamzatwn and use of this knowledge h.nowledge about verb classe 1s cruc1al for lexical acqu1srt10n m support of language generatiOn and machme translatiOn (Dorr, 1997) and document clC!bsrficatwn (Klavans and Kan, 1998), :yet manual class1ficatwn of large numbers of verb&apos;S 1s a d1fficult and resource mtens1ve task (Levm, 1993 MJ!ler et a!, 1990, Dang et al , 1998)","tmatwns to verb dtatheses We use drathesesalternattOns m the e\.presswn of the ar­ guments of the verb-followmg Levm and Dorr, for two reasons Fnst, verb dratheses are syntacttc cue'> 1 We are aware that a d!stnbuttonal approach rests on one strong assumptiOn regardmg the nature of the repre­ sentatiOns under study semantic not10ns and syntact1c notions are correlated, at least m part Tlus assumptton IS under debate (Bnscoe and Copestake, 1995, Levm, 1993, Dorr and Jones, 1996, Dorr, 1997), but we adopt 1t here w1thout further d1scuss1on 15 to semantic classes, hence they can be more eas1ly captured by corpus-based techn ques Second, usmg verb diatheses reduces no1se There IS a certam con­ sensus (Bnscoe and Copestake, 1995, Pustejovsky, 1995, Palmer, 1999) that verb d1atheses are regular sense extensions Hence focussmg on th1s type of classlficatton allows one to abstract from the prob­ lem of word sense d1sambiguatton and treat residual differences m word senses as no1se m the classifica­ tiOn task We present an m-depth case study, m wh1ch we apply machme learmng techn ques to automatically class1fy a set of verbs based on d1stnbutwns of gram­ matical md1cators of diatheses, extracted from a very large corpus We look at three very mterest­ mg classes of verbs unergat1ves, unaccusat1ves, and object-drop verbs (Levm, 1993) These are mterest­ mg classes because they all participate m the transi­ tiVIty alternatiOn, and they are m1n1mal pa1rs- that 1s, a small number of well-defined d1stmct10ns differ­ entiate then transltlve/mtransJtlve behaviOr Thus, we expect the d1fferences m the1r d1stnbutwns to be small, entallmg a fine-gramed d1scrlmmatwn task that prov1des a challengmg testbed for automat1c class1ficat10n The spec1fic theoretical questwn we mvest1gate IS whether the factors underlymg the verb class d!s­ tmcttons are reflected m the statistical d1stnbut10ns of lex1cal features related to diatheses presented by the mdivJdual verbs m the corpus In domg th1s, we address the questwns above by determmmg what are the lex1cal features that could d1stmgmsh the behav­ Ior of the classes of verbs wtth respect to the relevant diatheses, 'Nh1ch of those features can be gleaned from the corpus, and wh1ch of those, once the sta­ tistical d1stnbut10ns are available, can be used suc­ cessfully by an automatic classifier In Initial work (Stevenson and Merlo, 1999), \\e found that hngmst1cally mot1vated features that d1s­ tmgu1sh the verb classes can be extracted from an annotated, and m one case parsed, corpus These features are sufficient to almost halve the error rate compared to chance (45% reductwn) m auto­ matic verb class1ficat10n, suggestmg that dtstnbu­ twnal data provtdes knowledge useful to the classt­ ficatwn of verbs The focus of our ongmal studj was thP demonstration m pnnctple of IPc>•nmg verb classes from frequency d1stnbutwns of syntactic fea­ tures, and an analysis of the relative contnbutwn of the vanous features to learnmg Th1s paper turns to the Important ne\.t steps of rephcatmg our find­ mgs usmg other trammg methods and learnmg al­ gonthms, and analyzmg the performance on each of the three classes of verbs Th1s more detalied anal­ ysis of accuracy w1thm each class 111 turn leads to the development of a new dtstnbutwnal featme m­ tended to 1m prove dJscrlmmabthty among t\\O of the classes The addttiOn of the ne\\ feature successfully reduces the error rate of ou1 Initial results m classi­ ficatiOn by 19%, for a 56% overall reductiOn m error rate compared to chance 2 Determining the.","Features In th1s sectwn, we present mottvatwn for the llllttal features that we mvest1gated m terms of thetr wle m learnmg the verb classes \Ve first present the lmgmst1cally denved features then turn toe\ tdence from e\.penmental psychohngutsttcs to e\.tend the set of potenttally relevant features 2.1 Features of the Vetb.",Multi Citance
484,P98-1046.xml,W99-0632.xml,"Consider for example verbs participating in one alternation only: of these, 90.4% have one semantic class, 8.6% have two classes, 0.7% have three classes and 0.3% have four classes.","In contrast, of the verbs licensing six different alternations, 14% have one class, I 7% have two classes, 12.4% have three classes, 53.6% have four classes, 2% have six classes and 1% has seven classes.",Palmer (1999) and Dang et a!.(1998) argue that the use of syntactic frames and verb classes can simÂ­ plify the definition of different verb senses.,"Beyond this, we claim that information about the argument structure of a polysemous verb can often help dis­ ambiguating it.","Consider for instance the verb serve which is a member of four classes: GIVE, FIT, MASQUERADE But sometimes we do not have the syntactic infor­ mation that would provide cues for semantic disam­ biguation.",Multi Citance
485,P98-1046.xml,W99-0632.xml,We also intend to experiment with a full scale subcategorization dictionary acquired from the BNC.,We believe this will address issues such as: (a) relations between frames and classes (what are the frames for which the semantic class is predicted most accurately) and (b) relations between verbs and classes (what are the verbs for which the seman­ tic class is predicted most accurately).,"We also plan to experiment with different classification schemes for verb semantics such as WordNet (Miller et al., 1990) and intersective Levin classes (Dang et al., 1998).",$,$,Multi Citance
486,P98-1081.xml,P06-2060.xml,Each classifier gets to vote for its top ranked class and the class with the highest number of votes ’wins’.,Henderson et al (1999) use a Majority Vote scheme where different parsers vote on constituents’ mem bership in a hypothesized parse.,Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.,"In this paper, we induce multiple classifiers by using bagging (Breiman, 1996).","Following Breiman’s approach, we obtain multiple classifiers by first making bootstrap replicates of the training data and training different classifiers on each of the replicates.",Single Citance
487,P98-1081.xml,A00-1024.xml,"For example, the name recognition mod­ ule can make use of the considerable research that exists on name recognition, e.g.(McDonald, 1996), (Mani et al., 1996).","Secondly, individual compo­ nents can be replaced when improved models are available, without affecting other parts of the sys­ tem.","Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance).",In this paper we introduce a simplified version of the unknown word categorizer: one that con­ tains just two components: misspelling identifica­ tion and name identification.,In this section we in­ troduce these components and the 'decision' compo­ nent which combines the results from the individual modules.,Single Citance
488,P98-1081.xml,W00-0733.xml,One can also use the precision ob­ tained by a classifier for a specific output value as a weight (TagPrecision).,"Alternatively, we use as a weight a combination of the precision score for the output tag in combination with the recall score for competing tags (Precision­ Recall).","The most advanced voting method ex­ amines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag­ Pair, Van Halteren et al., (1998)).","151 Apart from these voting methods we have also applied two memory-based learners to the out­ put of the five chunkers: IBlIG and IGTREE, a decision tree variant of IB 1IG (Daelemans et al., 1999).",This approach is called classifier stacking.,Single Citance
489,P98-1081.xml,W00-0733.xml,Data items are represented as sets of feature-value pairs.,"_ Features receive weights which are based on the amount of information they provide for classifying the training data (Daelemans et al., 1999).","We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).",Five are so-called voting methods.,They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.,Post Contiguous
490,P98-1081.xml,W00-0733.xml,"Unlike the voting algorithms, the classifiers do not require a uniform input.",Therefore we have tested if their performance can be improved by supply­ ing them with information about the input of the first classification stage.,"For this purpose we have used the part-of-speech tag of the cur­ rent word as compressed representation of the first stage input (Van Halteren et al., 1998).",The combination methods will generate a list of open brackets and a list of close brackets.,We have converted these to phrases by only using brackets which could be matched with the clos­ est matching candidate and ignoring the others.,Single Citance
491,P98-1081.xml,J01-2002.xml,"Second, current performance levels on this task still leave room for improvement: ""state-of-the-art"" performance for data-driven automatic word class taggers on the usual type of material (e.g., tagging English text with single tags from a low-detail tagset) is at 9697% correctly tagged words, but accuracy levels for specific classes of ambiguous words are much lower.","Finally, a number of rather different methods that automatically generate a fully functional tagging system from annotated text are available off-the-shelf.","First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).","However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.","This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally.",Post Contiguous
492,P98-1081.xml,J01-2002.xml,The final corpus is the slightly smaller (750K words) Eindhoven corpus (Uit den Boogaart 1975) tagged with the Wotan tagset (Berghmans 1994).,"This will let us examine the tagging of a language other than English (namely, Dutch).","Compare this to the &quot;tune&quot; set in van Halteren, Zavrel, and Daelemans (1998).",This consisted of 114K.,"tokens, but, because of a 92.5% agreement over all four taggers, it yielded less than 9K tokens of useful training material to resolve disagreements.",Post Contiguous
493,P98-1081.xml,J01-2002.xml,"For example, Rigau, Atserias, and Agirre (1997) combine different heuris­ tics for word sense disambiguation by voting, and Agirre et al.(1998) do the same for spelling correction evaluation heuristics.","The difference between single classifiers learning to combine information sources, i.e., their input features (see Roth [1998] for a general framework), and the combination of ensembles of classifiers trained on subsets of those features is not always very clear anyway.","For part-of-speech tagging, a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren, Zavrel, and Daelemans (1998) and Brill and Wu (1998).","In both approaches, different tagger gen­ erators were applied to the same training data and their predictions combined using different combination methods, including stacking.","As we now apply the methods of van Halteren, Zavrel, and Daelemans (1998) to WSJ as well, it is easier to make a comparison.",Post Contiguous
494,P98-1081.xml,J01-2002.xml,"As a result, they are restricted to the combination of taggers that all use the same tagset.This is not the case for all the following (arbiter type) combination methods, a fact which we have recently exploited in bootstrapping a word class tagger for a new corpus from existing taggers with completely different tagsets (Zavrel and Daelemans 2000).",2.2 Stacked Probabilistic Voting.,"One of the best methods for tagger combination in (van Halteren, Zavrel, and Daele­ mans 1998) is the TagPair method.",It looks at all situations where one tagger suggests tag1 and the other tag2 and estimates the probability that in this situation the tag should actually be tagx.,"Although it is presented as a variant of voting in that paper, it is in fact also a stacked classifier, because it does not necessarily select one of the tags suggested by the component taggers.",Post Contiguous
495,P98-1081.xml,J01-2002.xml,"However, the inconsistently handled cases do account for 44% of the errors found for our best tagging system.","Under the circumstances, we feel quite justified in assuming that inconsistency is the main cause of the low accuracy scores.28 5.4 Size of the Training Set.","The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.","It appears that our hypothesis at the time, that the stacked systems were plagued by a lack of training data, is correct, since they can now hold their own.","In order to see at which point TagPair is overtaken, we have trained several systems on increasing amounts of training data from LOB.29 Each increment is one of the 10% training corpus parts described above.",Single Citance
496,P98-1081.xml,J01-2002.xml,"In these cases, we report such parameter settings in the introduction to the system.",3.1Data In the current experiments we make use of three corpora.,"The first is the LOB corpus (Johansson 1986), which we used in the earlier experiments as well (van Halteren, Zavrel, and Daelemans 1998) and which has proved to be a good testing ground.","We then switch to Wall Street Journal material (WSJ), tagged with the Penn Treebank II tagset (Marcus, Santorini, and Marcinkiewicz 1993).","Like LOB, it consists of approx­ imately lM words, but unlike LOB, it is American English.",Multi Citance
497,P98-1081.xml,J01-2002.xml,"Although methods for unsupervised training of HMM's do exist, training is usually done in a supervised way by estimation of the above prob­ abilities from relative frequencies in the training data.",The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993).,"In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods.","In the present work, we have replaced this by the TnT system (we will refer to this tagger as HMM below).20 TnT is a trigram tagger (Brants 2000), which means that it considers the previous two tags as features for deciding on the current tag.","Moreover, it considers the capitalization of the previous word as well in its state representation.",Single Citance
498,P98-1081.xml,J01-2002.xml,A closer investigation below shows at which amount of training data the crossover point in quality occurs (for LOB).,Another unresolved issue from the earlier experiments is the effect of making word or context information available to the stacked classifiers.,"With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word","For MBL, there is a degradation only for the WSJ data, and of a much less pronounced nature.","With the other data sets there is an improvement, significantly so for LOB.",Single Citance
499,P98-2143.xml,A00-1020.xml,"Traditionally, these techniques have combined extensive syntactic, se­ mantic, and discourse knowledge.","The acquisition of such knowledge is time-consuming, difficult, and error-prone.","Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).","For example, CogNIAC (Baldwin, 1997), a system based on seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.","For this research, we used a coreference resolution sys­ tem ((Harabagiu and Maiorano, 1999)) that imple­ ments different sets of heuristics corresponding to various forms of coreference.",Multi Citance
500,P98-2143.xml,C02-1027.xml,"language processing in Bulgarian LINGUA is a text processing framework for Bulgarian which automatically performs tokenisation, sentence splitting, part-of-speech tagging, parsing, clause segmentation, section- heading identification and resolution for third person personal pronouns (Figure 1).","All modules of LINGUA are original and purpose- built, except for the module for morphological analysis which uses Krushkov’s morphological analyser BULMORPH (Krushkov, 1997).","The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).","LINGUA was used in a number of projects covering automatic text abridging, word semantic extraction (Totkov and Tanev, 1999) and term extraction.","The following sections outline the basic language processing functions, provided by the language engine.",Single Citance
501,P98-2143.xml,C02-1027.xml,3.1 Adaptation of Mitkovs.,knowledge-poor approach for Bulgarian The anaphora resolution module is implemented as the last stage of the language processing architecture (Figure 1).,"This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).","MARS does not make use of parsing, syntactic or semantic constraints; nor does it employ any form of non-linguistic knowledge.","Instead, the approach relies on the efficiency of sentence splitting, part-of-speech tagging, noun phrase identification and the high performance of the antecedent indicators; knowledge is limited to a small noun phrase grammar, a list of (indicating) verbs and a set of antecedent indicators.",Multi Citance
502,P98-2143.xml,C02-1027.xml,"Before that, the text is pre-processed by a sentence split- ter which determines the sentence boundaries, a part-of-speech tagger which identifies the parts of speech and a simple phrasal grammar which detects the noun phrases.","In the case of complex sentences, heuristic ’clause identification’ rules track the clause boundaries.","LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.",System.,"3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).",Post Contiguous
503,P98-2143.xml,C02-1027.xml,"The antecedent indicators employed in MARS are classified as boosting (such indicators when pointing to a candidate, reward it with a bonus since there is a good probability of it being the antecedent) or impeding (such indicators penalise a candidate since it does not appear to have high chances of being the antecedent).","The majority of indicators are genre- independent and are related to coherence phenomena (such as salience and distance) or to structural matches, whereas others are genre- specific (e.g. term preference, immediate reference, sequential instructions).","Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).","However, we have added 3 new indicators for Bulgarian: selectional restriction pattern, adjectival NPs and name preference.","The boosting indicators are First Noun Phrases: A score of +1 is assigned to the first NP in a sentence, since it is deemed 4 This was done for experimental purposes.",Single Citance
504,P98-2143.xml,C04-1074.xml,Binding.,Binding constraints have been in the focus of linguistic research for more than thirty years.,"They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).","Empirically, binding constraints are rules without exceptions, hence they do not lead to any loss in achievable performance.","The downside is that their restrictive power is quite bad as well (0.3% in our corpus, cf.Table 1).",Pre Contiguous
505,P98-2143.xml,C04-1075.xml,There is a long tradition of work on coreference resolution within computational linguistics.,Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988).,"However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;","Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. These factors can be either “constraints” which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or “preferences” which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).","While a number of approaches use a similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (Dagan and Itai 1990) to decision trees (Soon, Ng and Lim 2001; Ng and Cardie 2002) to pattern induced rules (Ng and Cardie 2002) to centering algorithms (Grosz and Sidner 1986; Brennan, Friedman and Pollard 1987; Strube 1998; Tetreault 2001).",Multi Citance
506,P98-2143.xml,C04-1143.xml,"The anaphora resolver is aided by the gender- categorised named entity classes, enabling it to perform better resolution over a wide variety of names.","A simple linear model is adopted, where the system focuses mainly on nominal and clausal antecedents (Cristea et al., 2000).","The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)",3.5 Sentence Ranking.,"After named entity recognition and anaphora resolution, the summariser ranks the various sentences/phrases that it identifies and selects the best sentences to extract and put in the summary.",Single Citance
507,P98-2143.xml,D09-1101.xml,"First, they only tackle pronoun resolution rather than the full coreference task.","Second, their algorithm is heuristic-based; in particular, the score assigned to a preceding cluster is computed by summing over the weights associated with the factors applicable to the cluster, where the weights are determined heuristically, rather than learned, unlike ours.","Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.","As a result, their cluster-ranking model employs only factors that capture the salience of a cluster, and can therefore be viewed as a simple model of attentional state (see Grosz and Sidner (1986)) realized by coreference clusters.","By contrast, our resolution strategy is learned without applying hand-coded con straints in a separate filtering step.",Single Citance
508,P98-2143.xml,E99-1031.xml,"For our testbed, we implemented a variety of pronoun resolution techniques.",Each technique I Table I: Pronoun resolution modules used in our experiments can run in isolation or with the addition of meta­ modules that combine the output of multiple tech­ niques.,"We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).","Table 1 describes the pronoun resolution techniques implemented at this point, and shows whether they are activated for the Treebank and the TRAINS93 experiments.","Al­ though each module could run for both experi­ ments without error, if the features a particular module uses in the DE were not available, we simply deactivated the module.",Multi Citance
509,P98-2143.xml,H05-1001.xml,3.2 GUITAR: A General-Purpose Anaphoric.,"Resolver The system we used in these experiments, G U I TA R (Poesio and Kabadjov, 2004), is an anaphora resolution system designed to be high precision, modular, and usable as an off-the-shelf component of a NL processing pipeline.","The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)",The current version of G U I TA R does not include methods for resolving proper nouns.,3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.,Single Citance
510,P98-2143.xml,H05-1001.xml,"The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).",The current version of G U I TA R does not include methods for resolving proper nouns.,3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.,"The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).",The noun phrase with the highest aggregate score is proposed as antecedent.,Single Citance
511,P98-2143.xml,H05-1001.xml,3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.,Mitkov’s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as ”antecedent indicators”).,"The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).",The noun phrase with the highest aggregate score is proposed as antecedent.,"3.2.2 Definite Description Resolution The Vieira / Poesio algorithm (Vieira and Poesio, 2000) attempts to classify each definite description as either direct anaphora, discourse-new, or bridging description.",Single Citance
512,P98-2143.xml,I05-2040.xml,"In the ACE project, there are five types of entities defined in EDT: person (PER), geography political Entity (GPE), organization (ORG), location (LOC), and facility (FAC).",Many traditional coreference techniques can be extended to EDT for entity tracking.,"Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.","Recent research (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycherah et al., 2003; Luo et al., 2004) focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name,nominal and pronoun phrase.",One common ap proach applied by them is to first train a binary statistical model to measure how likely a pair of * This work is done while the first author is visiting Microsoft Research Asia.,Multi Citance
513,P98-2143.xml,I08-3014.xml,"Examples of third person ADs are ،ﮦو ،ﮯﺳا ،ﺎﮑﺳا ،ﯽﮑﺳا ،ﮯﮑﺳا ،ﻮﮑﺳا ،نا ،ﯽﮑﻧا ،ﮯﮑﻧا ،ﺎﮑﻧا ﮟﻴﻬﻧا ([vәƱh], [ʊseI], [ʊskɒ], [ʊski], [ʊskeI], [ʊskƏƱ], [ʊn], [ʊnki], [ʊnkeI], [ʊnkɒ], [ʊnheIñ]).",A lot of work has been done in English for the purpose of anaphora resolution and various,"algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).","Work has also been done in South Asian Languages such as Hindi and Malayalam for the purpose of anaphora resolution (Prasad and Strube, 2000; Sobha, 1998).",Prasad and Strube (2000) worked on anaphora resolution in Hindi.,Pre Contiguous
514,P98-2143.xml,I08-3014.xml,"Section-3 presents algorithms, implementation and evaluation for the resolution of personal anaphora; this is followed by the conclusion.","anaphora resolution Factors that can play a very important role in Urdu anaphora resolution beside morphological and lexical filters are topicalized structures, subject preferences, object preferences, repetitions, section heading and distance.","How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.","How these factors are helpful in the resolution of anaphoric devices in Urdu is done by Khan et al (Khan, Ali and Aamir, 2006).","Ali et al also worked on these factors for the resolution of demonstrative ADs in Urdu discourse (Ali, Khan and Aamir, 2007).",Single Citance
515,P98-2143.xml,J01-4003.xml,Our results from this investigation lead to the development of a new syntax­ based ranking of the Cf-list and corpus-based evidence that contradicts the psycholinguistic claims.,The aims of this paper are to compare implementations of pronoun resolution algo­ rithms automatically on a common corpus and to see if results from psycholinguistic experiments can be used to improve pronoun resolution.,"Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.","While manual evaluations have the advantage of allowing the researcher to examine the data closely, they are problematic because they can be time consuming, generally making it difficult to process corpora that are large enough to provide reli­ able, broadly based statistics.","With a system that can run various pronoun resolution algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results.",Multi Citance
516,P98-2143.xml,J01-4005.xml,"Still other approaches to anaphora resolution are based either on machine learn­ ing techniques (Connolly, Burger, and Day 1994; Yamamoto and Sumita 1998; Paul, Yamamato, and Sumita 1999) or on the principles of uncertainty reasoning (Mitkov 1995).",Computational processing of semantic and domain information is relatively expen­ sive when compared with other kinds of knowledge.,"Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).",Such approaches have performed notably well.,Lappin and Leass (1994) de­ scribe an algorithm for pronominal anaphora resolution that achieves a high rate of correct analyses (85%).,Single Citance
517,P98-2143.xml,J01-4006.xml,Example: It was Pat who gave us directions.,"Idioms often include vacuous pronouns, for example, hit it off Prop -it is the ascri ptio n of prop ertie s to an entit y with no exist enti al force (Qui rk and Gre enba um 1973 ).","Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&apos;clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ &quot;7 of the pronouns were non-anaphoric and 16 exophoric&quot; (Mitkov 1998, page 872).","It is unclear what categories of pronouns this statement refers to, since exophoric pronouns are nonanaphoric.","• ""Pleonastic pronouns it (i.e. non-anaphoric it) have not been included in these results"" (Peral, Palomar, and Femindez 1999, page 71).",Single Citance
518,P98-2143.xml,J02-1001.xml,"As an alternative, we propose an approach which, while permitting a unification-based specification of binding constraints, allows for a verification methodology that helps to overcome previous drawbacks.",This alternative approach is based on the rationale that anaphoric nominals can be viewed as binding machines.,"Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.","The former exclude impossible an tecedents and help to circumscribe the set of antecedent candidates; the latter help to pick the most likely candidate, which will be proposed as the antecedent.Binding constraints are a significant subset of such filters.","As they delimit the rel ative positioning of anaphors and their possible antecedents in grammatical geometry,these constraints are crucial to restricting the search space for antecedents and enhanc ing the performance of anaphor resolvers.1 From an empirical perspective, they stemfrom quite robust generalizations and exhibit a universal character, given their param eterized validity across natural languages.",Multi Citance
519,P98-2143.xml,J05-3004.xml,"Because it is inexpensive and needs no hand-modeling of lexical knowledge, it is a promising knowledge source to integrate into anaphora resolution systems.","Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.","Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.","Less attention has been paid to nominal anaphors with full lexical heads, which cover a variety of phenomena, such as coreference (Example (1)), bridging (Clark 1975; Example (2)), and comparative anaphora (Examples (3–4)).1 ∗ School of Computing, University of Leeds, Woodhouse Lane, LS2 9JT Leeds, UK.",Email: markert@comp.leeds.ac.uk.,Pre Contiguous
520,P98-2143.xml,N01-1008.xml,"Computational meth ods based on linguistic and congitive informationwere presented in (Hobbs 1978), (Lappin and Le ass 1994), (Brennan et al.1987), (Grosz et al.1995)and (Webber 1988).","The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.","Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).","For example, COG NIAC (Baldwin 1997), a system based on just sevenordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal refer ence.In our work, we approached the coreference res olution problem by trying to determine how muchmore knowledge is required to supplement the abovementioned knowledge-poor methods and how to de rive that knowledge.","To this end we (1) analyze the data to find what types of anaphor-antecedent pairs are most popular in real-world texts; (2) deviseknowledge-minimalist rules for handling the major ity of those popular cases; and (3) discover what supplementary knowledge is needed for remaining, more difficult cases.",Pre Contiguous
521,P98-2143.xml,N01-1008.xml,7 Conclusion.,"We have introduced a new data-driven method for coreference resolution, implemented in the COCKTAIL system.","Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.","Furthermore, by using an entropy-based method we determine the best partition of corefering expressions in coreference chains.",New rules are learned by applying a bootstrapping methodology that uncovers additional semantic consistency data.,Multi Citance
522,P98-2143.xml,N04-1004.xml,"In many cases, speech/gesture multimodal fusion works in a very similar way, with gestures grounding some of the same anaphoric pronouns (e.g., “this”, “that”, “here”).","One approach to anaphora resolution is to assign a salience value to each noun phrase that is a candidate for acting as a grounding referent, and then to choose the noun phrase with the greatest salience (Lappin and Leass, 1994).","Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).","Salience values are typically computed by applying linguistic knowledge; e.g., recent noun phrases are more salient, gender and number should agree, etc. This knowledge is applied to derive a salience value through the application of a set of predefined salience weights on each feature.","Salience weights may be defined by hand, as in (Lappin and Leass, 1994), or learned from data (Mitkov et al., 2002).",Single Citance
523,P98-2143.xml,P00-1022.xml,Our anaphora resolution approach belongs to the second group.,"In computational processing, semantic and domain information is computationally inefficient when compared to other kinds of knowledge.","Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).","Such approaches, nevertheless, perform notably well.",Lappin and Leass (1994) describe an algorithm for pronominal anaphora resolution that achieves a high rate of correct analyses (85%).,Single Citance
524,P98-2143.xml,P00-1022.xml,"Regarding the success rates reported in Ferrández et al.(1999) for pronominal references (82.2% for Lexesp, 84% for Spanish version of The Blue Book, and 87.3% for the English version), are higher than our 75% success rate for zero-pronouns.",This reduction (from 84% to 75%) is due mainly to the lack of gender information in zero-pronouns.,"Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.","It should be pointed out, however, that he used some knowledge that was very close to the genre13 of the text.",In our,Single Citance
525,P98-2143.xml,P00-1022.xml,pp.,535561,"Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.",In Proceedings of the 36 Annual Meeting of the Association for th it has achieved better results than either of them Computational Linguistics and 17 International have.,"As a future project, the authors shall attempt to evaluate the importance of semantic information for zero-pronoun resolutions in unrestricted texts.",Single Citance
526,P98-2143.xml,P01-1006.xml,3 Comparative evaluation of.,knowledge-poor anaphora resolution approaches The first phase of our project included comparison of knowledge-poorer approaches which share a common pre-processing philosophy.,"We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).",All three of these algorithms share a similar pre-processing methodology: they do not rely on a parser to process the input and instead use POS taggers and NP extractors; nor do any of the methods make use of semantic or real-world knowledge.,We re-implemented all three algorithms based on their original description and personal consultation with the authors to avoid misinterpretations.,Multi Citance
527,P98-2143.xml,P01-1006.xml,"The original version of the algorithm is non-robust, a pronoun being resolved only if one of the rules is applied.","The author also describes a robust extension of the algorithm, which employs two more weak rules that have to be applied if all the others fail.","Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.","The boosting indicators assign a positive score to an NP, reflecting a positive likelihood that it is the antecedent of the current pronoun.","In contrast, the impeding ones apply a negative score to an NP, reflecting a lack of confidence that it is the antecedent of the current pronoun.",Single Citance
528,P98-2143.xml,P01-1006.xml,The evaluation picture would not be accurate even if we compared anaphora resolution systems on the basis of the same data since the pre-processing errors which would be carried over to the systems’ outputs might vary.,"As a way forward we have proposed the idea of the evaluation workbench (Mitkov, 2000b) - an open-ended architecture which allows the incorporation of different algorithms and their comparison on the basis of the same pre-processing tools and the same data.","Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.","anaphora resolution In order to secure a ”fair”, consistent and accurate evaluation environment, and to address the problems identified above, we have developed an evaluation workbench for anaphora resolution which allows the comparison of anaphora resolution approaches sharing common principles (e.g. similar pre-processing or resolution strategy).",The workbench enables the ”plugging in” and testing of anaphora resolution algorithms on the basis of the same pre-processing tools and data.,Multi Citance
529,P98-2143.xml,P03-1023.xml,Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates.,"So far, various algorithms have been proposed to determine the preference relationship between two candidates.","Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.","And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backward- looking centers.","In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success.",Single Citance
530,P98-2143.xml,P04-1017.xml,"Iida et al.(2003) also take into consideration the contextual clues in their coreference resolution system, by using two features to reflect the ranking order of a candidate in Salience Reference List (SRL).","However, similar to common centering models, in their system the ranking of entities in SRL is also heuristic-based.","The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).","However, for an entity, the coreferential length only reflects its global salience in the whole text(s), instead of the local salience in a discourse segment which is nevertheless more informative for pronoun resolution.","Moreover, during resolution, the found coreferential length of an entity is often incomplete, and thus the obtained length value is usually inaccurate for the salience evaluation.",Multi Citance
531,P98-2143.xml,P04-1018.xml,Coreference resolution in this context is defined as partitioning mentions into entities.,"A mention is an instance of reference to an object, and the collection of mentions referring to the same object in a document form an entity.","Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)","One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities.","While this approach has yielded encouraging results, the way mentions are linked is arguably suboptimal in that an instant decision is made when considering whether two mentions are linked or not.",Multi Citance
532,P98-2143.xml,P05-1021.xml,"However, the utility of the corpus-based semantics for pronoun resolution is often argued.","Kehler et al.(2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution.","Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.","(2001), Strube and Muller (2003)).",Could the relatively noisy semantic knowledge give us further system improvement?,Single Citance
533,P98-2143.xml,P05-1021.xml,"This, however, would require extra calculation.","In fact, as candidates of a specific anaphor share the same anaphor context, we can just normalize the semantic feature of a candidate by that of its competitor: StatSem(C, ana) candidate in the local discourse segment.","ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).","Feature StatSem records the statistics-based semantic compatibility computed, from the corpus or the web, by either frequency or probability metric, as described in the previous section.","If a candidate is a pronoun, this feature value would be set to that of its closest nominal antecedent.",Single Citance
534,P98-2143.xml,P06-1006.xml,The issue that arises is how to effectively incorporate the syntactic information embedded in the parse trees to help resolution.,"One common solution seen in previous work is to define a set of features that represent particular syntactic knowledge, such as the grammatical role of the antecedent candidates, the governing relations between the candidate and the pronoun, and so on.","These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).","However, such a solution has its limitation.","The syntactic features have to be selected and defined manually, usually by linguistic intuition.",Multi Citance
535,P98-2143.xml,P07-1068.xml,"In addition, the induced knowledge improves the accuracy of common noun resolution by 26%.","In the past decade, knowledge-lean approaches have significantly influenced research in noun phrase (NP) coreference resolution — the problem of determining which NPs refer to the same real-world entity in a document.","In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).","While these approaches have been reasonably successful (see Mitkov (2002)), Kehler et al.(2004) speculate that deeper linguistic knowledge needs to be made available to resolvers in order to reach the next level of performance.","In fact, semantics plays a crucially important role in the resolution of common NPs, allowing us to identify the coreference relation between two lexically dissimilar common nouns (e.g., talks 536 and negotiations) and to eliminate George W. Bush from the list of candidate antecedents of the city, for instance.",Multi Citance
536,P98-2143.xml,P13-3012.xml,On the English data of the CoNLL’12 shared task the model outperforms most systems which participated in the shared task.,Graph-based coreference resolution.,"While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.",This yields a model similar to the one presented in this paper though Mitkov’s work has only been applied to pronoun resolution.,Nicolae and Nicolae (2006) phrase coreference resolution as a graph clustering problem: they first perform pairwise classification and then construct a graph using the derived confidence values as edge weights.,Single Citance
537,P98-2143.xml,S10-1019.xml,1 Introduction.,Coreference resolution is a field in which major progress has been made in the last decade.,"After a concentration on rule-based systems (cf.e.g.(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.","e.g.(Soon et al., 2001; Ng and Cardie, 2002)).","However, machine learning based coreference resolution is only possible for a very small number of languages.",Pre Contiguous
538,P98-2143.xml,W04-0707.xml,3.1 How much does DN-detection help the.,Vieira / Poesio algorithm?,"G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).","It is implemented in Java, takes its input in X M L format and returns as output its input augmented with the anaphoric relations it has discovered.","G U I TA R has been implemented in such a way as to be fully modular, making it possible, for example, to replace the D D resolution method with alternative implementations.",Multi Citance
539,P98-2143.xml,W04-2310.xml,"if it is so, we then give higher credibility to this particular antecedent.",Table 1: Results 2.5 Learning approach.,"In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.",Fixing these values in a adhoc fashion can clearly give rise to unstable behaviour.,"In our work, we use manually tagged corpora to evaluate the effectiveness of a given weight assignment; these can then be tuned using Genetic Algorithms(Goldberg, 1989).",Multi Citance
540,P98-2143.xml,W04-2310.xml,"As a result of such problems, several heuristics-based approaches have been developed and adopted over the years to achieve partial solutions to the problem.","The pioneering work in the area of anaphora resolution was done by Hobbs (Jerry R. Hobbs, 1978) who designed several early syntactic and semantic heuristics 1998) and (Lappin and Leass, 1994) describe several syntactic heuristics for reflexive, reciprocal and pleonastic anaphora, among others.","Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).","(Ng and Cardie, 2002) proposes a machine learning approach to Anaphora Resolution but generally statistical learning approaches suffer from the problems of small corpuses and corpus dependent learning.","A more general and comprehensive overview of state-of-the-art in anaphora resolution is given in (Mitkov, 1999) and also in (Mitkov et al., 2001).",Single Citance
541,P98-2143.xml,W09-2411.xml,"In these systems, there is a need to identify the different pieces of information that refer to the same discourse entity in order to produce coherent and fluent summaries, disambiguate the references to an entity, and solve anaphoric pronouns.",Coreference is an inherently complex phenomenon.,"Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.",This task will promote the development of linguistic resources –annotated corpora1– and machine-learning techniques oriented to coreference resolution.,"In particular, we aim to evaluate and compare coreference resolution systems in a multilingual context, including Catalan, English, and Spanish languages, and by means of two different evaluation metrics.",Single Citance
542,P98-2143.xml,W99-0104.xml,The knowledge-based method of Lappin and Leass produces better results.,"Never­ theless, RAPSTAT,a version of RAP obtained by using statistically measured preference patterns for the an­ tecedents, produced a slight enhancem nt of perfor­ mance over RAP.","Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.","The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).BOth these al­ gorithm rely only on part-of-sptagging of texts and on patterns for NP identification.",Their per­ formance (close to 90% for certain types of pro­ nouns) indicates that full syntactic knowledge is not required by certain forms of pronominal coreference.,Single Citance
543,P98-2143.xml,W99-0104.xml,"Never­ theless, RAPSTAT,a version of RAP obtained by using statistically measured preference patterns for the an­ tecedents, produced a slight enhancem nt of perfor­ mance over RAP.","Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.","The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).",BOth these al­ gorithm rely only on part-of-sptagging of texts and on patterns for NP identification.,Their per­ formance (close to 90% for certain types of pro­ nouns) indicates that full syntactic knowledge is not required by certain forms of pronominal coreference.,Multi Citance
544,P98-2143.xml,W99-0207.xml,"However, the difficulty of our task can be verified according to the baseline experiment 5So far we have considered the decision tree filter just as a black-box tool.","Further investigations on tree struc­ tures, however, should give us more evidence about the relative importance of the respective features.","However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).",section 3).,"Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expen­ sive in the cost of human effort at development time and limited ability to scale to new domains, more re­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.",Single Citance
545,P98-2143.xml,W99-0207.xml,"However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).","Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.section 3).","Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.","Similarly to them we do not use any sentence parsing or structural analysis, but just rely on morphosyn­ tactic and semantic word information.","Moreover, clues are used about the grammatical and pragmatic functions of expressions as in (Grosz et al., 1995), (Strube, 1998), ·or (Azzam et al., 1998) as well as rule-based empirical approaches like (Nakaiwa and Shirai, 1996) or (Murata and Nagao, 1997), to determine the most salient referent.",Multi Citance
546,W03-0410.xml,D07-1018.xml,"A proper treatment of polysemy is essential in the area of lexical acquisition, since polysemy repre sents a pervasive phenomenon in natural language.","However, previous approaches to the automatic acquisition of semantic classes have mostly disregarded the problem (cf.","Stevenson and Joanis, 2003 for English semantic verb classes","There are a few exceptions to this tradition, such as Pereira et al.(1993), Rooth et al.(1999), Korhonen et al.(2003), who used soft clustering methods for multiple assignment to verb semantic classes.",Our work addresses the lack of methodology in modelling a polysemous classification.,Single Citance
547,W03-0410.xml,D09-1138.xml,"In the experiments, training instances are obtained from VerbNet and/or Sem- Link (Loper et al., 2007), while features are extracted from the British National Corpus or from Wall Street Journal.We empirically compare several settings for model learning by varying the set of features, the source domain and the size of a corpus for feature extraction, and the use of the token-level statistics obtained from a manually disambiguated corpus.","We also provide the analysis of the remaining errors, which will lead us to further improve the supervised learning of a probabilistic semantic lexicon.","Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).","However, their focus has been limited to a small subset of verb classes, and a limited number of monosemous verbs.","The main contributions of the present work are: i) to provide empirical results for the automatic classification of all verbs, including polysemous ones, into all VerbNet classes, and ii) to empirically explore the effective settings for the supervised learning of a probabilistic lexicon of verb semantic classes.",Multi Citance
548,W03-0410.xml,D09-1138.xml,2.2 Related work.,"There has been much research effort invested in the automatic classification of verbs into lexical semantic classes, in a supervised or unsupervised way.","The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).","Our learning framework basically follows the above listed works: features are obtained from an unannotated (automatically parsed) corpus, and gold verb-class associations are used as training instances for machine learning classifiers, such as decision trees and support vector machines.","However, those works targeted a small subset of Levin classes, and a limited number of monosemous verbs; for example, Merlo and Stevenson (2001) studied three classes and 59 verbs, and Joanis et al.(2008) focused on 14 classes and 835 verbs.",Multi Citance
549,W03-0410.xml,D11-1095.xml,"Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification.","In this paper, we experiment with hierarchical Levin-style clustering.","We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)","The method has also been popular in the related task of noun clus Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011.","Qc 2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011).",Single Citance
550,W03-0410.xml,D11-1095.xml,"The extended version of the taxonomy in VerbNet (Kipper, 2005) classifies 5757 verbs.",Its 5 level taxonomy includes 101 top level and 369 subclasses.,"We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).",We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods.,"Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.",Single Citance
551,W03-0410.xml,D11-1095.xml,"We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).",We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods.,"Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.","This gave us 260 verbs in total.T2: The second gold standard is a large, hierarchical gold standard which we extracted from VerbNet as follows: 1) We removed all the verbs that have less than 1000 occurrences in our corpus.","2) In order to minimise the problem of pol- ysemy, we assigned each verb to the class which, according to VerbNet, corresponds to its predominant sense in WordNet (Miller, 1995).",Single Citance
552,W03-0410.xml,D11-1095.xml,"For each verb appearing in T1T3, we extracted all the occurrences (up to 10,000) from the British National Corpus (Leech, 1992) and North American News Text Corpus (Graff, 1995).",3.1 Features and feature extraction.,"Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).",We adopt for our experiments a set of features which have performed well in recent verb clustering works: A: Subcategorization frames (SCFs) and their relative frequencies with individual verbs.,B: A with SCFs parameterized for prepositions.,Multi Citance
553,W03-0410.xml,D11-1095.xml,"The second is local pairwise merging, i.e. the fact that only two clusters can be combined at any level.","For example, in order to group clusters representing Levin classes 9.1, 9.2 and 9.3 into a single cluster representing class 9, the method has to produce intermediate clusters, e.g. 9.{1,2} and 9.3.Such clusters do not always have a semantic inter pretation.","Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).","In the above example, only the clusters 9.{1,2} and 9.3 are consid ered, while alternative clusters 9.{1,3} and 9.2 are ignored.","Ideally, information about all the possible intermediate clusters should be aggregated, but this is intractable in practice.",Single Citance
554,W03-0410.xml,D11-1095.xml,"For AGG, we cut the hierarchy at 13 clusters.",Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2: Performance on T2 using a predefined tree structure.,Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).,Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.,"In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.",Single Citance
555,W03-0410.xml,D11-1095.xml,Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2: Performance on T2 using a predefined tree structure.,Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).,Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.,"In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.","When using this simple feature set, HGFC outperforms the best performing AGG clearly: 8.5% in ACC and 7.3% in Radj . We also compared HGFC against the best reported clustering method on T1 to date – that of spectral clustering by Sun and Korhonen (2009).",Single Citance
556,W03-0410.xml,D11-1095.xml,Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).,Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.,"In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.","When using this simple feature set, HGFC outperforms the best performing AGG clearly: 8.5% in ACC and 7.3% in Radj . We also compared HGFC against the best reported clustering method on T1 to date – that of spectral clustering by Sun and Korhonen (2009).",We used the feature sets C and D which are similar to the features (SCF parameterized by lexical prefences) in their experiments.,Single Citance
557,W03-0410.xml,J06-2001.xml,The results show how much impact a few mistakes may have on the pairwise f-score of the results.,"In addition to defining a difficult task, we also chose strong evaluation measures: Evaluating pairs of objects results in lower numbers than evaluating the individual objects.","For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.","That is, in a first step each induced verb cluster is assigned a gold standard class according to which class captures the majority of the cluster members.","In a second step, each verb in a cluster is evaluated as correct or wrong with respect to its gold standard class, and accuracy/purity of the whole clustering is calculated as the proportion of correct verbs divided by the total number of verbs.",Multi Citance
558,W03-0410.xml,J06-2001.xml,"For example, Joanis (2002) reported an extension of their work that used 802 verbs from 14 classes from Levin (1993).","He defined an extensive feature space with 219 core features (such as part of speech, auxiliary frequency, syntactic categories, and animacy as above) and 1,140 selectional preference features taken from WordNet.As in Schulte im Walde (2000), the selectional preferences did not improve the clustering.","In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.","In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.",Low- frequency and ambiguous verbs were excluded from the classes.,Post Contiguous
559,W03-0410.xml,P03-1009.xml,It is multiplied by a factor that increases with cluster size.,This factor compensates for a bias towards small clusters.,"Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).","We associate with each cluster its most prevalent semantic class, and denote the number of verbs in a cluster K that take its prevalent class by nprevalent(K ).",Verbs that do not take this class are considered as errors.,Single Citance
560,W03-0410.xml,P03-1009.xml,"We argue, however, that evaluation against a monosemous gold standard reveals only part of the picture.","10 Due to differences in task definition and experimental setup, a direct comparison with earlier results is impossible.","For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.",Dif fer ent S en se s Pai rs Fr act io n in clu ste r 0 1 2 3 4 3 9 8 5 6 2 5 12 84 14 37 5 1 % 1 0 % 7 % 3 % 3 % Table 3: Evaluation against the monosemous (Pred.),and pol- ysemous (Multiple) gold standards.,Single Citance
561,W03-0410.xml,P04-2007.xml,"A classification of Spanish verbs based on the same hypothesis has been developed by (Va´zquez et al., 2000).",But manually constructing large-scale verb classifications is a labour-intensive task.,"For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).",In this article we present experiments aiming at automatically classifying Spanish verbs into lexical semantic classes based on their subcategorisation frames.,We adopt the idea that a description of verbs in terms of their syntactic behaviour is useful for acquiring their semantic properties.,Multi Citance
562,W03-0410.xml,P07-3016.xml,"While this method has the advantage of working on POS-tagged, unparsed corpora, it is costly with respect to time and linguistic expertise.","To overcome this drawback, (Joanis and Stevenson, 2003) develop a general feature space for supervised verb classification.","(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.","As unsupervised methods are more sensitive to noisy features, the key issue is to filter out the large number of probably irrelevant features.","They propose a semi- supervised feature selection method which outperforms both hand-selection of features and usage of the full feature set.As in our experiment we do not have a predefined set of semantic classes, we need to apply unsupervised methods.",Single Citance
563,W03-0410.xml,W06-2910.xml,"There are a variety of manual semantic verb classifications; major frameworks are the Levin classes (Levin, 1993), WordNet (Fellbaum, 1998), and FrameNet (Fontenelle, 2003).","The different frameworks depend on different instantiations of semantic similarity, e.g. Levin relies on verb similarity referring to syntax-semantic alternation behaviour, WordNet uses synonymy, and FrameNet relies on situation-based agreement as defined in Fillmore’s frame semantics (Fillmore, 1982).","As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).","Depending on the types of verb classes to be induced, the automatic approaches vary their choice of verbs and classification/clustering algorithm.","However, another central parameter for the automatic induction of semantic verb classes is the selection of verb features.",Multi Citance
564,W03-0410.xml,W06-2910.xml,"Since the target classification determines the similarity and dissimilarity of the verbs, the verb feature selection should model the similarity of interest.","For example, Merlo and Stevenson (2001) classify 60 English verbs which alternate between an intransitive and a transitive usage, and assign them to three verb classes, according to the semantic role assignment in the frames; their verb features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment.","In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.","The verb features need to relate to a behavioural component (modelling the syntax-semantics interplay), but the set of features which potentially influence the behaviour is large, ranging from structural syntactic descriptions and argument role fillers to adverbial adjuncts.","In addition, it is not clear how fine-grained the features should be; for example, how much information is covered by low-level window co-occurrence vs. higher-order syntactic frame fillers?",Multi Citance
565,W03-0410.xml,W06-2910.xml,"Again, we performed an agglomerative hierarchical clustering on the verbs (as modelled by the different feature types).","We cut the hierarchy at a level of 77 clusters, which corresponds to the number of FrameNet classes, and evaluated against the FrameNet classes.","For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.(Stevenson and Joanis, 2003; Korhonen et al., 2003)","classes from all GermaNet synsets, and created a hard classification for these classes, by randomly deleting additional senses of a verb so we have a fixed cut in the hierarchy as based on the gold standard, as opposed to the evaluation in Section 3 where we explored the optimal cut level.",frames grammar relations f-pp f-pp-pref n na na NP PP NP&PP ADV Assoc 37.50 37.80 35.90 37.18 39.25 39.14 37.97 41.28 38.53 GN 46.98 49.14 58.01 53.37 51.90 53.10 54.21 51.77 51.82 FN 33.50 32.76 29.46 30.13 32.74 34.16 28.72 33.91 35.24 co-occurrence: window-20 all cut ADJ ADV N V Assoc 39.33 39.45 37.31 36.89 39.33 38.84 GN 51.53 52.42 50.88 47.79 52.86 49.12 FN missing 32.84 31.08 31.00 34.24 31.75 Table 4: Accuracy for induced verb classes.,Multi Citance
566,W03-0410.xml,E09-1072.xml,The task of animacy classification bears some resemblance to the task of named entity recognition (NER) which usually makes reference to a ‘person’ class.,"However, whereas most NER systems make extensive use of orthographic, morphological or contextual clues (titles, suffixes) and gazetteers, animacy for nouns is not signaled overtly in the same way.","Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.",This is thus equivalent to treatment of animacy as a lexical semantic property and the classification strategy is based on generalization of morphosyntactic behaviour of common nouns over large quantities of data.,"Due to the small size of the Talbanken05 treebank and the small amount of variation, this strategy was pursued for the acquisition of animacy information.",Multi Citance
567,W04-0213.xml,C04-1061.xml,"1 In our corpus of newspaper commentaries (Stede,.","Our position is that progress in discourse parsing relies on the one hand on a more thorough understanding of the underlying issues, and on the other hand on the availability of human-annotated corpora, which can serve as a resource for in-depth studies of discourse- structural phenomena, and also for training statistical analysis programs.","Two examples of such corpora are the RST Tree Corpus by (Marcu et al., 1999) for English and the Potsdam Commentary Corpus (Stede, 2004) for German.","Producing such resources is a labour- intensive task that requires time, trained annotators, and clearly specified guidelines on what relation to choose under which circumstances.","Nonetheless, rhetorical analysis remains to be in part a rather subjective process (see section 2).",Multi Citance
568,W04-0213.xml,C04-1061.xml,"“Ideally”, discourse analysis proceeds incrementally from left to right, where for each new segment, an attachment point and a relation (or more than one of each, cf.SDRT) are computed and the discourse structure grows step by step.","This view is taken for instance in SDRT (Asher, Lascarides, 2003), which places emphasis on the notion of ‘right frontier’ (also discussed recently by (Webber et al., 2003)).","However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged.","Both annotators agreed that a strict left-to-right approach is highly impractical, because the intended argumentative structure of the text often becomes clear only in retrospect, after reflecting the possible contributions of the segments to the larger scheme.","Thus they very soon settled on a bottom-up approach: First, mark the transparent cases, in which a connective undoubtedly signals a relation between two segments.4 Then, see how the resulting pieces fit together into a structure that mirrors the argument presented.",Single Citance
569,W04-0213.xml,C04-1061.xml,"over to PDTB, where the annotations are more complex than in our step 1 but do not go as far as building rhetorical structures.",mark a relation between matrix clause and embedded clause.,"Annotators have to also make syntactic judgements, which is not the case in our approach (where syntax would be done on a different annotation layer, see (Stede, 2004)).","In the following, we briefly explain the most important problematic issues with annotating German connectives and the way we deal with them, using our annotation scheme for Con- Ano.",3.1 Issues with German connectives.,Single Citance
570,W04-0213.xml,C08-2009.xml,We examine corpus-extracted examples as soft constraints.,"We show how to use Regular Tree Gramamrs to process such constraints, and how the representation of some constraints depends on the expressive power of this formalism.","Discourse structures cannot always be described completely, either because they are ambiguous (Stede, 2004), or because a discourse parser fails to analyse them completely.","In either case, underspecification formalisms (UFs) can be used to represent partial information on discourse structure.","UFs are used in semantics to model structural ambiguity without disjunctive enumeration of the readings (van Deemter and Peters, 1996).",Single Citance
571,W04-0213.xml,P06-3008.xml,Section 5 is a summary and Section 6 directions for future work.,under construction 2.1 Corpus data.,"For the purpose of language engineering and linguistic investigation, we are constructing a Chinese corpus comparable to the English WSJRST treebank and the German Potsdam Commentary Corpus (Carlson et al. 2003; Stede 2004).","Texts in our corpus were downloaded from the official website of People’s Daily 1 , where important Caijingpinlun2 (CJPL) articles 1 www.people.com.cn.",2 Caijinpinglun (CJPL) in Chinese means “financial and.,Multi Citance
572,W04-0213.xml,P08-2062.xml,Underspecification-based algorithms for processing partially disambiguated discourse structure must cope with extremely high numbers of readings.,"Based on previous work on dominance graphs and weighted tree grammars, we provide the first possibility for computing an underspecified discourse description and a best discourse representation efficiently enough to process even the longest discourses in the RST Discourse Treebank.",Discourse processing has emerged as a highly relevant source of information for applications such as information extraction and automatic summarisation (Taboada and Mann (2006) outline this and further applications).,"In either case, only partial information on discourse structure is available.","To handle such information, underspecification formalisms can be used.",Single Citance
573,W04-0213.xml,P08-2062.xml,"Furthermore, we show how weighted RTGs can be used to represent constraints and preferences on the discourse structure.","Taking all these results together, we show for the first time how the globally optimal discourse representation based on some preference model can be computed efficiently from an UDR.","Following annotation schemes like the one of Stede (2004), we model discourse structures by binary trees.",Fig.,(1b-f) represent the potential structures of (1).,Single Citance
574,W04-0213.xml,W06-2709.xml,"Currently, data from 13 different languages is elicited and annotated with information from various linguistic levels (morphosyntax, phonology, semantics, and information structure).An interesting query might look for nominal phrases (const=np) that are new in the discourse (given=new) and belong to the (information-) focus of a sentence (focus=ans), e.g. for investigating the phonological realization of these.","The according query has the form: const=np & given=new & focus=ans & #1 = #2.4 Queries in ANNIS can be restricted to subsets of a corpus, by queries such as focus=ans & doc=*8111*, which searches for all answer foci in the data that has been elicited by means of the task 8111 in the questionnaire, yielding matching data from all languages in our database.","Discourse studies The Potsdam Commentary Corpus, PCC (Stede, 2004), consists of 173 newspaper commentaries, annotated for morphosyn- tax, coreference, discourse structure according to Rhetorical Structure Theory, and information structure.",A question of interest here is the information- structural pattern of sentences introducing discourse segments that elaborate on another part of the discourse: elaboration & rel=satellite & (cat=vroot & aboutness-topic) & #1 > #2 & #2 = #3.,Another research issue is the relationship of coreference and discourse structure.,Single Citance
575,W04-0213.xml,W07-1525.xml,II.,"Extended Scheme: steps 1 to 3 accordingly These stages correspond to the 3 annotation levels within the Core and Extended Schemes respectively, because annotating at all levels at the same time has proved to be very labor-intensive and more time-consuming than one level at a time.","The original annotation guidelines were drafted in 2004 by the authors for the annotation of the Potsdam Commentary Corpus of German newspaper commentaries (PCC) (Stede, 2004)","After a series of annotation experiments, the PoCoS Core Scheme was applied to the PCC by two instructed annotators, students of linguistics, whose portions had an overlap of 19 texts (11%).","Based upon these texts, inter-annotator agreement was calculated using different agreement scores along the methodology of PopescuBelis et al.(2004).",Multi Citance
576,W04-0213.xml,W07-1530.xml,"The corpus on the one hand is a collection of “raw” data, which is used for genre- oriented statistical explorations.","On the other hand, we have identified two sub-corpora that are subject to a rich multi-level annotation (MLA).","The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes.","The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme 3 Lascarides, 2003) glue logic for building discourse (Brants et al., 2002) and Annotate tool.","In addition, structures is insufficient to deal with open domain text, and we cannot envision an extended version at the present time able to deal with the problem.",Single Citance
577,W04-0213.xml,W07-1530.xml,The units could either be simple (elementary discourse units: ED Us) or they could be complex.,We assumed that in principle the units were recursively generated and could have an arbitrary though finite degree of complexity.,"(Manfred Stede, Potsdam) Construction of the Potsdam Commentary Corpus (PCC) began in 2003 and is still ongoing.","It is a 1 The Message Understanding Conference, wwwnlpir.",nist.gov/related projects/muc/.,Single Citance
578,W04-0213.xml,W11-0401.xml,"For English there is also the Discourse Relations Reference Corpus (Taboada and Renkema, 2008).",This corpus includes 65 texts specified and it does not include texts annotated by several people.,"Another well-known corpus is the Potsdam Commentary Corpus, for German (Stede, 2004; Reitter and Stede, 2003).",This corpus includes 173 texts on politics from the online newspaper MÃ¤rkische Allgemeine Zeitung.,"It contains 32,962 words and 2,195 sentences.",Post Contiguous
579,W04-0213.xml,W12-3205.xml,"For all other kinds of discourse structures, dedicated manual annotation has been required, both for segmentation and labelling, and many of these resources have been made available for other researchers.","For fine-grained functional structure, there is the ART corpus (Liakata et al., 2010)2.","For discourse relations annotated in the RST framework, there is the RST Discourse TreeBank of English text (Carlson et al., 2003), available through the Linguistic Data Consortium (LDC), as well as similarly annotated corpora in Spanish (da Cunha et al., 2011), Portugese (Pardo et al., 2008) and German (Stede, 2004).","For discourse relations annotated in the lexically- grounded approach first described in (Webber and Joshi, 1998), there is the Penn Discourse TreeBank (Prasad et al., 2008) in English, as well as corpora in Modern Standard Arabic (AlSaif and Markert, 2010; AlSaif and Markert, 2011), Chinese (Xue, 2005; Zhou and Xue, 2012), Czech (Mladova´ et al., 2008), Danish (BuchKromann et al., 2009; BuchKromann and Korzen, 2010), Dutch (van der Vliet et al., 2011), Hindi (Oza et al., 2009), and Turkish (Zeyrek and Webber, 2008; Zeyrek et al., 2009; Zeyrek et al., 2010).","Also available are discourse- annotated journal articles in biomedicine (Prasad et al., 2011) and discourse-annotated dialogue (Tonelli et al., 2010).",Multi Citance
580,W04-0213.xml,W13-2708.xml,"erzählen (tell) betonen (emphasize) werden (will) haben (have) schreiben (write) mitteilen (share) berichten (report) meinen (state) erklären (explain) sagen (say) most used speech verbs identities are mobilised by entities of political debate (i.e. persons, organisations, etc.); the detection of indirect speech is mandatory for any such analysis.",4.4 Comm entary/ Report Classif ication A useful distinction for political scientists dealing with newspaper articles is the distinction between articles that report objectively on events or back 0.00% 5.00% 10.00% 15.00% 20.00% 25.00% 30.00% 35.00% 40.00% grounds and editorials or press commentaries.,"We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz Figure 4: 10 most used verbs (lemma) in indirect speech.",4.3 Using Indirect Speech.,"Other modules benefit from the identification of indirect speech, as can be seen from Sentence 7.",Single Citance
581,W04-0213.xml,W13-3306.xml,"The interpretation of meanwhile plied to SMT (Eidelman et al., 2012) or decoding with document-wide features (Hardmeier et al., 2012).","A recently published article summarizes most of the work on SMT with the broader perspective of discourse, lexical cohesion and co- reference (Hardmeier, 2013).","For discourse relations and DCs especially, more and more annotated resources have become available in several languages, such as English (Prasad et al., 2008), French (PeÂ´ryWoodley et al., 2009; Danlos et al., 2012), German (Stede, 2004)","therefore CONTRASTIVE and not TEMPORAL: SOURCE: Apple Computer Inc., meanwhile<COMPARISONCONTRAST>, is expected to show improved earnings for the period ended September.","BASELINE: Spolecˇnost Apple Computer Inc., mezit´ım by meˇla uka´zat lepsˇ´ı pˇr´ıjmy za obdob´ı koncˇ´ıc´ı v za´ˇr´ı.",Multi Citance
582,W06-3909.xml,D09-1025.xml,"We reimplemented Pas¸ca et al.’s (2006) state-of-the-art web- scale fact extractor, which, given seed instances of a binary relation, finds instances of that relation.","We extract entities of a class, such as Actors, by instantiating typical relations involving that class such as act-in(Actor, Movie).","We instantiate such relations instead of the classical is-a patterns since these have been shown to bring in too many false positives, see (Pantel and Pennacchiotti, 2006) for a discussion of such generic patterns.",The extractor’s confidence score for each instance is used by the Ranker to score the entities being extracted.,Section 4.1 lists the system parameters we used in our experiments.,Single Citance
583,W06-3909.xml,D09-1025.xml,"Most commonly used are is-a pattern families such as those first proposed by Hearst (1992) (e.g., ‘Y such as X’ for matching ‘actors such as Brad Pitt’).",Minimal supervision is used in the form of small sets of manually provided seed patterns or seed instances.,"This approach is very common in both the NLP and Semantic Web communities (Cimiano and Staab, 2004; Cafarella et al., 2005; Pantel and Pennacchiotti, 2006; Pasca et al., 2006).","The distributional approach uses contextual evidence to model the instances of a given class, following the distributional hypothesis (Harris, 1964).","Weakly supervised, these methods take a small set of seed instances (or the class label) and extract new instances from noun phrases that are most similar to the seeds (i.e., that share similar contexts).",Pre Contiguous
584,W06-3909.xml,P09-1045.xml,"Another approach is to identify seeds using hyponym patterns like, * is a [NAMED ENTITY] (Meij and Katrenko, 2007).",This leads us to our first investigation of seed variability and the methodology used to compare bootstrapping algorithms.,"Typically algorithms are compared using one set of handpicked seeds for each category (Pennacchiotti and Pantel, 2006; McIntosh and Curran, 2008).",This approach does not provide a fair comparison or any detailed analysis of the algorithms under investigation.,"As we shall see, it is possible that the seeds achieve the maximum precision for one algorithm and the minimum for another, and thus the single comparison is inappropriate.",Multi Citance
585,W06-3909.xml,P09-1113.xml,"An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007).","Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base.","A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).","These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion.",The resulting patterns often suffer from low precision and semantic drift.,Multi Citance
586,W06-3909.xml,P170300_w06.xml,To design specific linguistic relations in each distinct lawsuit is challenging and not required.,Our aim is to suggest relations suit able for any discussion concept.,"To generalize the task, we first determine noun phrases in the data following the definition in (Pennacchiotti and Pantel 2006).",The phrases are combinations of adjectives and nouns.,The technical steps include standard part-of-speech tagging including grammar based chunk parser.,Single Citance
587,W06-3909.xml,P846406_w06.xml,3.6 Skimming to Obtain Relations.,"In a completely separate stream of work, Pantel and Pennacchiotti investigated the extraction of axioms from the text using the statistical text harvesting paradigm.","In (Pennacchiotti and Pantel, 2006; Pantel and Pennacchiotti, 2006) they report the results.","Their harvester, called Espresso, uses weak supervision to learn lexical patterns that typically signal relations of interest, and then applies these relations to extract all likely instances of the relation from the book.",These extractions are then reformulated as HNF axioms and added to the system’s knowledge.,Multi Citance
588,W06-3909.xml,Ponto_w06.xml,"The theoretical idea is that, in the problematic sentence above, an RMRS output would contain the predicate associated with the identity copula be with a first argument corresponding to the term Firemouth cichlid and a second argument corresponding to cichlids, regardless of the word order, modification and so on.","Thus, having obtained the RMRS representation of a given corpus, it would be possible to extract ontological relationships from a semantic structure that abstracts over those morphological and syntactic details that do not affect the ontological relationship.","As pointed out by Pennacchiotti and Pantel [6], most ontology extraction systems so far have focused on generalised is-a or part-of relationships.","Our work involves extracting general hyponymic relations with RMRS and applying a filter to the results to obtain biological, taxonomic relationships.","The corpus was gathered by extracting 12,200 animal articles from the Wikipedia online encyclopaedia (http://www.wikipedia.org/), providing a semi-edited setting where the added robustness of semantics might prove its usefulness.",Single Citance
589,W06-3909.xml,Ponto_w06.xml,The conclusion presents different avenues for future work.,"The most widely used framework for automatic ontology extraction was originally proposed by Marti Hearst [5], who introduced the use of lexico-syntactic patterns for the extraction of hyponymic relationships.","Hearsts method has since then been followed by the most successful systems, such as the Espresso system proposed by Pennacchiotti and Pantel [6], based on bootstrapping.","Aside from this pattern-based approach, new clustering methods have also been investigated.",The main idea is to group terms that appear in the same kind of context and label the resulting clusters.,Single Citance
590,W06-3909.xml,PSS07_w06.xml,Statistical and machine learning systems have almost completely replaced the intricate handcrafted knowledge of earlier language understanding systems.,"Some recent IE systems have extracted domain- independent relations, but still narrowly focused on a few relations.","Pennachiotti and Pantel developed a system that extracts the relations such as is-a, part-of, and succession from the Trec9 corpus and is-a, part-of, production, and reaction from a chemistry corpus (Pennacchiotti &amp; Pantel 2006).","Snow, Jurafsky, and Ng used a hyponym classifier to extract is-a relations from a large collection of newswire and encyclopedia text (Snow, Jurafsky, & Ng 2006).","Recently, a few systems have been created that do “open IE”.",Single Citance
591,W08-2222.xml,P13-1138.xml,"We first split each document in the corpus into sentences and create a shallow Discourse Representation Structure (following Discourse Representation Theory (Kamp and Reyle, 1993)) of each sentence.",The DRS consists of semantic predicates and named entity tags.,"We use Boxer semantic analyzer (Bos, 2008) to extract semantic predicates such as EVENT or DATE.","In parallel, domain specific named entity tags are identified and, in conjunction with the semantic predicates, are used to create templates.",We developed the named-entity tagger for the weather domain ourselves.,Single Citance
592,W08-2222.xml,Q13-1015.xml,"For example, the lexicon may contain the entry: write f- (S\NP)/NP : λ yλ x.write!(x, y) Crucially, there is a transparent interface between the syntactic category and the semantics.","For example the transitive verb entry above defines the verb syntactically as a function mapping two noun- phrases to a sentence, and semantically as a binary relation between its two argument entities.","This means that it is relatively straightforward to deterministically map parser output to a logical form, as in the Boxer system (Bos, 2008).",This Every dog barks NP↑/N N S\NP λ pλ q.∀x[ p(x) =⇒ q(x)] λ x.dog!(x) λ x.bark!(x) > NP↑ λ q.∀x[dog!(x) =⇒ q(x)] > S ∀x[dog!(x) =⇒ bark!(x)] Figure 1: A standard logical form derivation using CCG.,"The NP↑ notation means that the subject is type-raised, and taking the verb-phrase as an argument—so is an abbreviation of S/(S\NP).",Single Citance
593,W08-2222.xml,S12-1040.xml,"Negation lies at the heart of deductive inference, of which consistency checking (searching for contradictions in texts) is a prime example in natural language understanding.",It shouldn’t therefore come as a surprise that detecting negation and adequately representing its scope is of utmost importance in computational semantics.,"In this paper we present and evaluate a system that transforms texts into logical formulas â€“ using the C&amp;C tools and Boxer (Bos, 2008) â€“ in the context of the shared task on recognising negation in English texts (Morante and Blanco, 2012).",We will first sketch the background and the basics of the formalism that we employ in our analysis of negation (Section 2).,In Section 3 we explain how we detect negation cues and scope.,Multi Citance
594,W08-2222.xml,S13-1002.xml,"The simplest models compute a phrase vector by adding the vectors for the individual words (Landauer and Dumais, 1997) or by a component-wise product of word vectors (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010).","Other approaches, in the emerging area of distributional compositional semantics, use more complex methods that compute phrase vectors from word vectors and tensors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011).","Wide-coverage logic-based semantics Boxer (Bos, 2008) is a software package for wide-coverage semantic analysis that produces logical forms using Discourse Representation Structures (Kamp and Reyle, 1993).","It builds on the C&C CCG parser (Clark and Curran, 2004).","Markov Logic In order to combine logical and probabilistic information, we draw on existing work in Statistical Relational AI (Getoor and Taskar, 2007).",Multi Citance
595,W08-2222.xml,W10-1750.xml,"In this work, as a first proposal, instead of elaborating on novel similarity measures, we have borrowed and extended the Discourse Representation (DR) metrics defined by Gime´nez and Ma`rquez (2009).",These metrics analyze similarities between automatic and reference translations by comparing their respective discourse representations over individual sentences.,"For the discursive analysis of texts, DR metrics rely on the C&amp;C Tools (Curran et al., 2007), specifically on the Boxer component (Bos, 2008).",This software is based on the Discourse Representation Theory (DRT) by Kamp and Reyle (1993).,DRT is a theoretical framework offering a representation language for the examination of con- textually dependent meaning in discourse.,Multi Citance
596,W08-2222.xml,W11-2408.xml,"There is more work to be done to arrive at a reliable, inference-ready knowledge base of such rules.","The primary desideratum is to produce a logical representation for the rules such that they can be used in the EPILOG reasoner (Schubert and Hwang, 2000).","Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements.",We have a preliminary version of a logical form generator that derives LFs from TreeBank parses that can support this direction.,Further filtering techniques (based both on the surface form and the logical form) should keep the desired inference rules while improving quality.,Single Citance
597,W08-2222.xml,W13-2101.xml,"Surface Realization is the task of producing fluent text from some kind of formal, abstract representation of meaning (Reiter and Dale, 2000).","However, while it is obvious what the output of a natural language generation component should be, namely text, there is little to no agreement on what its input formalism should be (Evans et al., 2002).","Since open-domain semantic parsers are able to produce formal semantic representations nowadays (Bos, 2008; Butler and Yoshimoto, 2012), it would be natural to see generation as a reversed process, and consider such semantic representations as input of a surface realization component.","The idea of using large text corpora annotated with formal semantic representations for robust generation has been presented recently (Basile and Bos, 2011; Wanner et al., 2012).","The need for formal semantic representations as a basis for NLG was expressed already much earlier by Power (1999), who derives semantic networks enriched with scope information from knowledge representations for content planning.",Multi Citance
598,W08-2222.xml,W13-3209.xml,Vector-assisted logic The first class of approaches seeks to use distributional models of word semantics to enhance logic-based models of textual inference.,"The work which best exemplifies this strand of research is found in the efforts of Garrette et al.(2011) and, more recently, Beltagy et al.(2013).","This line of research converts logical representations obtained from syntactic parses using Bosâ€™ Boxer (Bos, 2008)","As this class of approaches deals with improving logic-based models rather than giving a dis- tributional account of logical function words, we view such models as orthogonal to the effort presented in this paper.",Logic with vectors The second class of approaches seeks to integrate boolean-like logical operations into distributional semantic models using existing mechanisms for representing and composing semantic vectors.,Single Citance
599,W09-0621.xml,D12-1016.xml,It comprises a total of 9.8 million newswire articles from seven distinct sources.,"In previous work (Roth and Frank, 2012), we introduced GigaPairs, a sub-corpus extracted from Gigaword that includes over 160,000 pairs of newswire articles from distinct sources.",GigaPairs has been derived from Gigaword using the pairwise similarity method on headlines presented by Wubben et al. (2009) ,"In addition to calculating the similarity of news titles, we impose an additional date constraint to further increase the precision of extracted pairs of texts.",Random inspection of about 100 documents revealed only two texts describing different events.,Single Citance
600,W09-0621.xml,PS15684-W09.xml,Yan et al. [8] have extracted multilingual phrasal paraphrases by aligning deﬁnition sentences extracted from Wikipedia articles.,A minimally supervised approach was then used to extract paraphrases by computing global and local similarity measures between phrasal pairs from the aligned sentences.,Wubben et al. have compared Clustering against pairwise matching for extracting paraphrases from news corpora [9].,A k-means clustering approach was used to subdivide already existing clusters of headlines.,Sentence-level paraphrases were then extracted by matching all possible sentence pairs within each cluster.,Single Citance
601,W09-0621.xml,PS15684-W09.xml,The fuzzy hierarchical clustering approach has been implemented in Java and the WordNet electronic dictionary has been used as an additional resource.,The performance of the proposed approach has been evaluated on two different corpora: the MSRPC and the MSRVDC.,The obtained results have been compared with that of the Paraphrase Acquisition system developed by Wubben et al. [9].,4.1.,Evaluation metrics.,Single Citance
602,W09-0621.xml,PS15684-W09.xml,The 143 clusters identiﬁed were ﬁxed as reference classes against which the created clusters have been judged.,"In terms of entropy, the best performance was registered by the rigorous system, with WSD, threshold 0.2 and using only the Table 5 Comparative performance on MSRPC.","Approach Accuracy % Precision % Recall % F-Measure % Fernando et al. (2008) [40] 74.1 75.2 91.3 82.4 Mihalcea et al. (2006) [41] 70.3 69.6 97.7 81.3 Proposed system variant WSD, threshold = 0.2, top 50% 66.4 81.0 65.2 72.3 Cosine similarity, threshold = 0.7 (Wubben et al. [9]) 62.5 80.5 56.6 66.5 k-Means clustering (Wubben et al. [9]) 60.6 70.6 71.0 70.8 FCM clustering 36.2 68.1 9.5 16.7 Table 6 Performance of proposed system on MSRVDC Dataset 1.",Without WSD With WSD Threshold = 0.15 Threshold = 0.2 Threshold = 0.15 Threshold = 0.2 Top 50% All Top 50% All Top 50% All Top 50% All Entropy % 5.68 10.98 5.55 10.9 5.06 6.88 4.7 56.86 Purity % 64.82 79.42 65.62 78.77 63.08 70.8 61.34 70.05 v-Measure % 83.87 83.05 84.04 82.97 83.98 84.,14 84.25 84.3 The highest value for each evaluation measure has been given in bold.,Multi Citance
603,W09-0621.xml,W10-4223.xml,If the similarity exceeds a defined upper threshold it is accepted; if it is below a defined lower threshold it is rejected.,"In the case that it lies between the thresholds, the process is repeated but then with word vectors taken from a snippet from the corresponding news article.","This method, described in earlier work Wubben et al. (2009), was reported to yield a precision of 0.76 and a recall of 0.41 on clustering actual Dutch paraphrases in a headline corpus.",We adapted this method to English.,Our data consists of English headlines that appeared in Google News over the period of April to September 2006.,Single Citance
604,W09-0621.xml,w11-1604.xml,"The use of monolingual parallel text corpora was first suggested by Barzilay and McKeown (2001), who built their corpus using various alternative human-produced translations of literary texts and then applied machine learning or multi-sequence alignment for extracting paraphrases.","In a similar vein, Pang et al.(2003) used a corpus of alternative English translations of Chinese news stories in combination with a syntax-based algorithm that automatically builds word lattices, in which paraphrases can be identified.","So-called comparable monolingual corpora, for instance independently written news reports describing the same event, in which some pairs of sentences exhibit partial semantic overlap have also been investigated (Shinyama et al., 2002; Barzilay and Lee, 2003; Shen et al., 2006; Wubben et al., 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al., 2004), consisting of 5,801 sentence pairs, sampled from a larger corpus of news articles.","However, it is rather small and contains no sub- sentential allignments.",Cohn et al.(2008) developed a parallel monolingual corpus of 900 sentence pairs annotated at the word and phrase level.,Multi Citance
605,W09-0621.xml,w11-1604.xml,"We train all systems on the DAESO data (218,102 lines of aligned sentences) and test on a held-out set consisting of manually aligned headlines that ap 1 This method is implemented in the Moses package in the program relax-parse as option SAMT 4 Table 2: Examples of output of the phrase-based and syntax-based systems Source Phrase-based Syntax-based jongen ( 7 ) zwaargewond na aanrijding 7jarige gewond na botsing jongen ( 7 ) zwaar gewond na aanrijding boy (7) severely-injured after crash 7-year-old injured after collision boy (7) severely injured after crash Source jeugdwerkloosheid daalt vooral bij voldoende opleiding youth-unemployment drops especially with adequate training Phrase-based werkloosheid jongeren daalt , vooral bij voldoende studie unemployment youths drops, especially with sufficient study Syntax-based * jeugdwerkloosheid daalt vooral in voldoende opleiding youth-unemployment drops especially in adequate training Source kritiek op boetebeleid ns criticism of fining-policy ns Phrase-based * kritiek op de omstreden boetebeleid en criticism of the controversial and Syntax-based kritiek op omstreden boetebeleid nederlandse spoorwegen criticism of controversial fining-policy dutch railways Source weer bestuurders radboud weg again directors radboud [hospital] leaving Phrase-based * weer de weg ziekenhuis again the leaving hospital Syntax-based alweer bestuurders ziekenhuis weg yet-again directors hospital leaving peared in May 2006.2 We test on 773 headlines that have three or more aligned paraphrasing reference headlines.","We use an SRILM (Stolcke, 2002) language model trained on the Twente news corpus3.","To investigate the effect of the amount of training data on results, we also train a phrase-based model on more data by adding more aligned headlines originating from data crawled in 2010 and aligned using (i i=LD(1..8)  Ni  N I STi)tf.idf scores over headline clusters and Cosine sim ilarity as described in (Wubben et al., 2009), resulting in an extra 612,158 aligned headlines.",Evaluation is based on the assumption that a good paraphrase is well-formed and semantically similar but structurally different from the source sentence.,"We therefore score the generated paraphrases not only by an MT metric (we use NIST scores), but also factor in the edit distance between the input sentence and the output sentence.",Multi Citance
606,W11-0815.xml,D13-1060.xml,"Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking-oriented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set.A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words.","These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al.(2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011).","The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002).","Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant perfor We focus on a particular subset of MWEs, English phrasal verbs.","A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking.",Multi Citance
607,W11-0815.xml,W12-3311.xml,"Finlayson and Kulkarni (2011) exemplify that the word world has 9 senses in Wordnet 1.6, record has 14, but world record has only 1.","• POS tagging and parsing: recent work in parsing and POS tagging indicates that MWEs can help remove syntactic ambiguities (Seretan, 2008).","Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011).",kingdom come and by and large.,"3 For example, Smadja (1993) classifies them according to syntactic function while Sag et al.(2002) classify them according to flexibility.",Single Citance
608,W11-0815.xml,W13-3208.xml,"This study follows Biemann and Giesbrecht (2011), who attempted to find a list of non- compositional expressions whose meaning is not fully determined by the meaning of its constituents and their combination.","The task turned out to be frustratingly hard (Johannsen et al., 2011).","Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010).",We extend this motivation by stating that WSMs could also benefit from a set of non-compositional expressions.,"Specifically, WSMs could treat semantically non-compositional expressions as single units.",Multi Citance
609,W11-0815.xml,W15-0909.xml,"The attempt is marginally successful, and we speculate on whether a larger-scale attempt is likely to have greater impact.",While the explicit identification of multiword expressions (“MWEs”: Sag et al.,"(2002), Baldwin and Kim (2009)) has been shown to be useful in various NLP applications (Ramisch, 2012), recent work has shown that automatic prediction of the degree of compositionality of MWEs also has utility, in applications including information retrieval (IR: Acosta et al.","For instance, Acosta et al.(2011) showed that by considering non-compositional MWEs as a single unit, the effectiveness of document ranking in an IR system improves, and Carpuat and Diab (2010) showed that by adding compositionality scores to the Moses SMT system (Koehn et al., 2007), they could improve translation quality.",This paper presents the first attempt to use MWE compositionality scores for the evaluation of MT system outputs.,Multi Citance
610,W95-0104.xml,A00-2019.xml,Corpus-based WSD systems identify the intended sense of a polysemous word by (1) collecting a set of example sentences for each of its various senses and (2) extracting salient contextual cues from these sets to (3) build a statistical model for each sense.,"They identify the intended sense of a word in a novel sentence by extracting its contextual cues and selecting the most similar word sense model (e.g., Leacock, Chodorow and Miller (1998), Yarowsky (1993)).","Golding (1995) showed how methods used for WSD (decision lists and Bayesian classifiers) could be adapted to detect errors resulting from common spelling confusions among sets such as there, their, and they&apos;re.",He extracted contexts from correct usage of each confusable word in a training corpus and then identified a new occurrence as an error when it matched the wrong context.,"However, most grammatical errors are not the result of simple word confusions.",Single Citance
611,W95-0104.xml,A97-1025.xml,"Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of prob­ lems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration.",This class of prob­ lems has been attacked by many others.,"A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992).","Recently, Golding and Schabes (1996) de­ scribed a system, Tribayes, that combines a trigram model of the words' parts of speech with a Bayesian classifier.","The trigram component of the system is used to make decisions for those confusion sets that 166 documents D' 0 terms X r x r r x d t X d t x r Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, Sand D'.",Multi Citance
612,W95-0104.xml,A97-1025.xml,The pair of sentence and confusion word vectors with the largest cosine is identified and the corresponding confusion word is chosen as the most likely word for the test sentence.,The predicted word is compared to the correct word and a tally of correct predictions is kept.,The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996).,Seven of the 18 confusion sets contain words that are all the same part of speech and the remaining 11 con­ tain words with different parts of speech.,Golding and Schabes (1996) have already shown that using a trigram model to predict words from a confusion set based on the expected part of speech is very effec­ tive.,Single Citance
613,W95-0104.xml,C04-1131.xml,The NB classifier assumes the features are independent given the sense.,"During classification, it chooses the sense with the highest posterior probability.","We have also selected a decision list classifier (DL) which is similar to the classifier used by (Yarowsky, 1994) for words having two senses, and extended for more senses by (Golding, 1995).","DL classifier is further developed in (Audibert, 2003).","In DL, features are sorted in order of decreasing strength, where strength reflects feature reliability for decision-making.",Multi Citance
614,W95-0104.xml,D07-1012.xml,"However, a complete evaluation is missing.",The idea of disambiguating between the elements of confusion sets is related to word sense disambiguation.,Golding (1995) builds a classifier based on a rich set of context features.,Mays et al.(1991) apply the noisy channel model to the disambiguation problem.,For each candidate correction S′ of the input S the probability P (S′)P (S|S′) is calculated and the most likely correction selected.,Single Citance
615,W95-0104.xml,D11-1119.xml,Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis.,Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic similarity/distance measures in WordNet.Both systems report performance that is lower than systems developed more recently.,"A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995)","Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word.","The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006).",Multi Citance
616,W95-0104.xml,E06-1030.xml,5.3 Results.,Our experiments compare the results when the three corpora were trained using the same algorithm.,The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.,"For the WSJ testing set, the 2 billion word Web Corpus does not achieve the performance of the Gigaword (see Table 4).","However, the 10 billion word Web Corpus results approach that of the Gigaword.",Single Citance
617,W95-0104.xml,E99-1024.xml,"statistical methods proposed for the word sense disambiguation problem(Fujii, 1998).","Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem.","For that problem, some statistical methods have been applied and succeeded(Golding, 1995; GoldÂ­ ing and Schabes, 1996).","Hence, statistical meth­ ods are certainly valid for the homophone prob­ lem.","In particular, the decision list is valid for the homophone problem(Shinnou, 1998).",Pre Contiguous
618,W95-0104.xml,H01-1052.xml,2.1 Confusion Set Disambiguation.,Several methods have been presented for confusion set disambiguation.,"The more recent set of techniques includes multiplicative weight-update algorithms [4], latent semantic analysis [7], transformation-based learning [8], differential grammars [10], decision lists [12], and a variety of Bayesian classifiers [2,3,5].","In all of these papers, the problem is formulated as follows: Given a specific confusion set (e.g. {to, two, too}), all occurrences of confusion set members in the test set are replaced by some marker.","Then everywhere the system sees this marker, it must decide which member of the confusion set to choose.",Post Contiguous
619,W95-0104.xml,J98-1006.xml,"A comparison is made to the sense assigned by a human judge, and the classifier's decision is scored as correct or incorrect.",TLC uses a Bayesian approach to find the sense Si that is the most probable given the cues Cj contained in a context window of ±k positions around the polysemous target word.,"For each si, the probability is computed with Bayes&apos; rule: As Golding (1995) points out, the term p(c_kf ..","Of course, the sparse data problem affects these probabilities too, and so TLC uses the Good-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values of p(cj I si), including providing probabilities for cues that did not occur in the training.","TLC actually uses the mean of the Good-Turing value and the training-derived value for p(cj I si)· When cues do not appear in training, it uses the mean of the Good­ Turing value and the global probability of the cue p(cj), obtained from a large text corpus.",Single Citance
620,W95-0104.xml,N03-2035.xml,"To automatically extract the discriminative features from feature space and to combine them in disambiguation, we have to investigate an efficient technique in our task.",The problem becomes how to select and combine various kinds of features.,"Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.","Later, Golding and Roth [4] applied Winnow algorithm in the same task and found that the algorithm performs comparably to the Bayesian hybrid method when using pruned feature sets, and is better when using unpruned sets or unfamiliar test set.In this paper, we propose a unified framework in solving the problems of word boundary ambiguity and homograph ambiguity altogether.","Our approach employs both local and long-distance contexts, which can be automatically extracted by a machine learning technique.",Single Citance
621,W95-0104.xml,N03-2035.xml,"These methods can successful capture long distance word association, but cannot capture local context information and sentence structure.","Decision trees [2] can handle complex condition, but they have a limitation in consuming very large parame ter spaces and they solve a target problem by applying only the single strongest feature.","Hybrid approach [3, 12] combines the strengths of other techniques such as Bayesian classifier, n-gram, and decision list.",It can be capture both local and long distance context in disambiguation task.,4 Our Model.,Single Citance
622,W95-0104.xml,N03-2035.xml,Their parts of speech and pronunciations are manually tagged by linguists.,"The resulting corpus is divided into two parts; the first part, about 80% of corpus, is utilized for training and the rest is used for testing.","In the experiment, we classify the data into three group depending on types of text ambiguity according to section 2: CDSA, CISA and Homograph, and compare the results from different approaches; Winnow, Bayseian hybrid [3] and POS trigram.",The results are shown in Table 1.,information and make the task of Thai homograph dis- ambiguity more accurate.,Single Citance
623,W95-0104.xml,N04-1016.xml,The task is to infer which word in a confusion set is the correct one in a given context.,"This choice can be either syn tactic (as for {then, than}) or semantic (as for {principal, principle}).A number of machine learning methods have been pro posed for context-sensitive spelling correction.","These include a variety of Bayesian classifi ers (Golding, 1995; Golding and Schabes, 1996), decision lists (Golding, 1995) transformation-based learning (Mangu and Brill, 1997), Latent Semantic Analysis (LSA) (Jones and Martin, 1997), multiplicative weight update algorithms (Golding and Roth, 1999), and augmented mixture models (Cucerzan and Yarowsky, 2002).",Context word features record the presence of a word within a fi xed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small number of words and/or part- of-speech tags to the left or right of the target word.,The results obtained by a variety of classifi cation methods are given in Table 6.,Multi Citance
624,W95-0104.xml,N04-1016.xml,Context word features record the presence of a word within a fi xed window around the target word (bag of words); collocational features capture the syntactic environment of the target word and are usually represented by a small number of words and/or part- of-speech tags to the left or right of the target word.,The results obtained by a variety of classifi cation methods are given in Table 6.,All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).,"Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71†‡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24†‡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.","The method takes into account collocational features, i.e., words that are adjacent to the target word.",Single Citance
625,W95-0104.xml,N04-1016.xml,The results obtained by a variety of classifi cation methods are given in Table 6.,All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995).,"Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5: Performance of Altavista counts and BNC counts for context sensitive spelling correction (data from Cucerzan and Yarowsky 2002) Model Accuracy Baseline BNC 70.00 Baseline Altavista 72.98 Best BNC 80.71â€ â€¡ Golding (1995) 81.40 Jones and Martin (1997) 84.26 Best Altavista 89.24â€ â€¡ Golding and Schabes (1996) 89.82 Mangu and Brill (1997) 92.79 Cucerzan and Yarowsky (2002) 92.20 Golding and Roth (1999) 94.23 Table 6: Performance comparison with the literature for context sensitive spelling correction the Brown corpus, using 80% for training and 20% for testing.3 We devised a simple, unsupervised method for performing spelling correction using web counts.","The method takes into account collocational features, i.e., words that are adjacent to the target word.","For each word in the confusion set, we used the web to estimate how frequently it co-occurs with a word or a pair of words immediately to its left or right.",Multi Citance
626,W95-0104.xml,N04-1016.xml,"Table 5 shows that the best result (89.24%) for the web- based approach is obtained with a context of one word to the left and one word to the right of the target word ( f (w1 , t , w2 )).","The BNC-based models perform consistently worse than the web-based models with the exception of f (t , w1 )/t ; the best Altavista model performs sig nifi cantly better than the best BNC model.","Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.",that both the best Altavista model and the best BNC model outperform their respective baselines.,"A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).",Single Citance
627,W95-0104.xml,N04-1016.xml,"Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing.",that both the best Altavista model and the best BNC model outperform their respective baselines.,"A comparison with the literature shows that the best Altavista model outperforms Golding (1995), Jones and Martin (1997) highest accuracy on the task is achieved by the class of multiplicative weight-update algorithms such as Winnow (Golding and Roth, 1999).",Both the best BNC model and the best Altavista model perform signifi cantly worse than this model.,Note that Golding and Roth (1999) use algorithms that can handle large numbers of features and are robust to noise.,Multi Citance
628,W95-0104.xml,N10-1019.xml,"This is, at least in part, based on the recognition that non-native speakers of English now outnumber native speakers by 2:1 in some estimates, so any tool in this domain could be of tremendous value.","While earlier work in both native and non-native error correction was focused on the construction of grammars and analysis systems to detect and correct specific errors (see Heift and Schulze, 2005 for a detailed overview), more recent approaches have been based on data-driven methods.","The majority of the data-driven methods use a classification technique to determine whether a word is used appropriately in its context, continuing the tradition established for contextual spelling correction by Golding (1995) and Golding and Roth (1996).",The words investigated are typically articles and prepositions.,They have two distinct advantages as the subject matter for investigation: They are a closed class and they comprise a substantial proportion of learners’ errors.,Multi Citance
629,W95-0104.xml,P01-1005.xml,"Example confusion sets include: {principle , principal}, {then, than}, {to,two,t oo}, and {weather,whether}.",Numerous methods have been presented for confusable disambiguation.,"The more recent set of techniques includes mult iplicative weight- update algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation- based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).",Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in speech.,The memory-based learner used only the word before and word after as features.,Multi Citance
630,W95-0104.xml,P96-1010.xml,"In addition, huge word-trigram tables need to be available at run time.","More­ over, word trigrams are ineffective at capturing long­ distance properties such as discourse topic and tense.","Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction.","However, we report experiments that show that these methods are of limited effective­ ness for cases such as {their, there, they're} and {than, then}, where the predominant distinction to be made among the words is syntactic.",Confusion set Train Test Most freq.,Multi Citance
631,W95-0104.xml,P96-1010.xml,"Empirical evaluation of the trigram method demonstrates that it performs well when the words to be discriminated have different parts of speech, but poorly when they have the same part of speech.","In the latter case, it is reduced to simply guessing whichever word in the confusion set is the most com­ mon representative of its part-of-speech class.","We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech.","We confirm experimentally that Bayes and Trigrams have complementary performance, Trigrams being better when the words in the confusion set have dif­ ferent parts of speech, and Bayes being better when they have the same part of speech.","We introduce a hybrid method, Tribayes, that exploits this com­ plementarity by invoking each method when it is strongest.",Single Citance
632,W95-0104.xml,P96-1010.xml,"The previous section showed that the part-of-speech trigram method works well when the words in the confusion set have different parts of speech, but es­ sentially cannot distinguish among the words if they have the same part of speech.","In this case, a more effective approach is to learn features that char­ acterize the different contexts in which each word tends to occur.","A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).","We adopt the Bayesian hybrid method, which we will call Bayes, having experi­ mented with each of the methods and found Bayes to be among the best-performing for the task at hand.","This method has been described elsewhere (Golding, 1995) and so will only be briefly reviewed here; how­ ever, the version used here uses an improved smooth­ ing technique, which is mentioned briefly below.",Multi Citance
633,W95-0104.xml,P96-1010.xml,"A number of feature-based methods have been proposed, including Bayesian classifiers (Gale, Church, and Yarowsky, 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995), and, more recently, a method based on the Winnow multiplicative weight-updating algorithm (Golding and Roth, 1996).",We adopt the Bayesian hybrid method,"This method has been described elsewhere (Golding, 1995)","where T = t1 ...tn, and P(t;lt;-2t;_I) is the proba­ bility of seeing a part-of-speech tag t; given the two preceding part-of-speech tags t;_ 2 and ti-l· Equa­ tions 1 and 2 will also be used to tag sentences W and W' with their most likely part-of-speech se­ quences.","This will allow us to determine the tag that 1To enable fair comparisons between sequences of dif­ ferent length (as when considering maybe and may be), we actually compare the per-word geometric mean of the sentence probabilities.",Pre Contiguous
634,W95-0104.xml,P98-2138.xml,Using word N-gram could recover some word-level in­ formation but requires an extremely large cor­ pus to estimate all parameters accurately and consumes vast space resources to store the huge word N-gram table.,"In addition, the model losses generalized information at the level of POS.","For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996).","Among them, the feature­ based methods were shown to be superior to other approaches.",This is because the methods can combine several kinds of features to deter­ mine the appropriate word in a given context.,Multi Citance
635,W95-0104.xml,P98-2138.xml,"For ex­ ample, suppose that the centroid word is know, then all possible words in 1-edit distance con­ fusion set are {know, knob, knop, knot, knew, enow, snow, known, now}.","Furthermore, words with probability lower than a threshold are ex­ cluded from the set.For example, if a specific OCR has low probability of substituting t with w, ""knof' should be excluded from the set.","Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations.","Context-word features is used to test for the presence of a particular word within +f- M words of the target word, and collocations test for a pattern of up to L contiguous words and/or part-of-speech tags around the target word.","In our experiment M and L is set to 10 and 2, respectively.",Multi Citance
636,W95-0104.xml,W01-0502.xml,"Examples include part-of speech tagging, word-sense disambiguation, accent restoration, word choice selection in machine translation, context-sensitive spelling correction, word selection in speech recognition and identifying discourse markers.","Machine learning methods have become the most popular technique in a variety of classification problems of these sort, and have shown significant success.","A partial list consists of Bayesian classifiers (Gale et al., 1993), decision lists (Yarowsky, 1994), Bayesian hybrids (Golding, 1995)","based methods (Zavrel et al., 1997), linear classifiers (Roth, 1998; Roth, 1999) and transformation- based learning (Brill, 1995).","In many of these classification problems a significant source of difficulty is the fact that the number of candidates is very large – all words in words selection problems, all possible tags in tagging problems etc. Since general purpose learning algorithms do not handle these multi-class classification problems well (see below), most of the studies do not address the whole problem; rather, a small set of candidates (typically two) is first selected, and the classifier is trained to choose among these.",Multi Citance
637,W95-0104.xml,W02-1005.xml,"General purpose Spelling Correction is also a long-standing task (e.g. McIlroy, 1982), traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context.",Yet Kukich (1992) observed that about 2550% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling cor rectors.,"Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995)","Generally, both tasks involve the selection between a relatively small set of alternatives per keyword (e.g. sense id’s such as church/BU ILD IN G and church/IN STITU T IO N or commonly confused spellings such as quiet and quite), and are dependent on local and long-distance collocational and syntactic patterns to resolve between the set of alternatives.","Thus both tasks can share a common feature space, data representation and algorithm infrastructure.",Single Citance
638,W95-0104.xml,W02-1005.xml,We also present the results obtained by the different algorithms on 75 MMVC.,"70 65.41 66.72 65 60 55 65.58 66.71 61.84 59.66 68.61 62.75 69.68 another WSD standard set, SEN SEVA L-1, also by performing 5-fold cross validation on the original 50 45 41.79 45.94 46.25 training data.","For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995)","Finally, we present the results obtained by the investigated methods on a single run on the Senseval1 and Senseval2 test data.",The described models were initially trained and tested by performing 5-fold cross-validation on the SEN SEVA L-2 English lexical-sample-task training data.,Single Citance
639,W95-0104.xml,W04-3238.xml,"The former approaches were based on the impractical assumption that all possible syntactic uses of all words (i.e. part-of-speech) are known, and presented both recall and precision problems because many of the substitution errors are not syntactically anomalous and many unusual syntactic constructions do not contain errors.","The latter approach had very limited success under the assumptions that each sentence contains at most one misspelled word, each misspelling is the result of a single point change (insertion, deletion, substitution, or transposition), and the defect rate (the relative number of errors in the text) is known.","A different body of work (e.g. Golding, 1995; Golding and Roth, 1996; Mangu and Brill, 1997) focused on resolving a limited number of cognitive substitution errors, in the framework of context sensitive spelling correction (CSSC).","Although promising results were obtained (9295% accuracy), the scope of this work was very limited as it only addressed known sets of commonly confused words, such as {peace, piece}).",1.1 Spell Checking of Search Engine Queries.,Multi Citance
640,W95-0104.xml,W06-1624.xml,The first matched feature is applied to make a predication.,"Obviously, how to measure the confidence of features is a very important issue for the decision list.","We use the metric described in (Yarowsky, 1994; Golding, 1995).",Provided that P ( s | f ) > 0 for all i : The corresponding slot names can be automatically extracted from the domain model.,A domain model is usually a hierarchical structure of c onfid e n c e ( f ) = m a x P ( si i | f ) (1) the relevant concepts in the application domain.,Multi Citance
641,W95-0104.xml,W06-3604.xml,"With IGTR EE we have an arguably competitive efficient, but one-shot learning algorithm; IGTR EE does not need an iterative procedure to set weights, and can also handle a large feature space.","Instead of viewing all positional features as containers of thousands of atomic word features, it treats the positional features as the basic tests, branching on the word values in the tree.","More generally, as a precursor to the above- mentioned work, confusable disambiguation has been investigated in a string of papers discussing the application of various machine learning algorithms to the task (Yarowsky, 1994; Golding, 1995;","In this article we explored the scaling abilities of IGTR EE, a simple decision-tree algorithm with favorable asymptotic complexities with respect to multi-label classification tasks.","IGTR EE is applied to word prediction, a task for which virtually unlimited amounts of training examples are available, with very large amounts of predictable class labels; and confusable disambiguation, a specialization of word prediction focusing on small sets of confusable words.",Multi Citance
642,W95-0104.xml,W12-0304.xml,Atwell (1987) was among the first to use a statistical and knowledge-poor approach to detect grammatical errors in POS-tagging.,"Other studies, such as those by Knight and Chandler (1994) or Han et al.(2006), for instance, proved more successful than rule-based systems in the task of detecting article-related errors.","There are also other studies (Yarowsky, 1994; Golding, 1995 or Golding and Roth, 1996) that report the application of decision lists and Bayesian classifiers for spell checking; however, these models cannot be applied to grammar error detection.","Burstein et al.(2004) present an idea similar to the present paper, since they use n-grams for grammar checking.","In their case, however, the model is much more complicated since it uses a machine learning approach trained on a corpus of correct English and using POS-tags bigrams as features apart from word bi- grams.",Multi Citance
643,W95-0104.xml,W96-0108.xml,Mays and colleagues [1991] have exploited word trigrams to detect and correct both the non-word and real-word errors that were artificially generated from 100 sentences.,Church and Gale [1991] have used a Bayesian classifier method to improve the performance for non-word error correction.,Golding [1995] has applied a hybrid Bayesian method for real-word error correction and Golding and Schabes [1996] have combined a POS trigram and Bayesian methods for the same purpose.,The goal of the work described here is to investigate the effectiveness and efficiency of SLM­ based methods applied to the problem of OCR error correction.,"Since POS-based methods are not effective in distinguishing among candidates with the same POS tags and since methods based on word-trigram models involve extensive training data and require that huge word-trigram tables be available at run time, we used a word-bigram SLM as the first step in our investigation.",Multi Citance
644,W95-0104.xml,W98-1234.xml,"Of course, this isn't exhaustive, there are other possible techniques but we aim to give the main ideas.","We'll speak about the main modules of our program : Spelling correction, Different uses of WordNet, and Generation of comments.","Our module used for spelling correction was developed on the basis of works by Brill [1], Brill and Marcus [2), Golding [3), Golding and Schabes [4], and Powers [5).","Alan Turing was a brilliant British mathematician who played an important role in the development of part-of-speech (verbs, nouns, adjectives, adverbs).",These sets are divided into semanticals categories (e.g. synonymous for nouns...).,Multi Citance
645,X96-1048.xml,A97-1028.xml,"There is currently much interest, in both research and com­ mercial arenas, in natural language processing systems which can perform multilingual information extraction (IE), the task of automatically identifying the various aspects of a text that are of interest to specific users.","An example of IE is the Named Entity (NE) task, which has become established as the important first step in many other IE tasks, provid­ ing information useful for coreference and template filling.","Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).","Several organized evaluations have been held to determine the state-of-the-art in NE systems, and there are commercial systems available.","The goal of the NE task is to automatically identify the boundaries of a variety of phrases in a raw text, and then to categorize the phrases identified.",Single Citance
646,X96-1048.xml,J00-4003.xml,"These advantages are thought by many to offset some of the obvious disadvantages of this way of developing NLP theories-in particular, the fact that, given the current state of language processing technology, many hypotheses of interest cannot be tested yet (see below).","As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic","interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.","The system we present was developed to be evaluated in a quantitative fashion, as well, but because of the problems concerning agreement between annotators observed in our previous study, we evaluated the sys­ tem both by measuring precision/recall against a ""gold standard,"" as done in MUC, and by measuring agreement between the annotations produced by the system and those proposed by the annotators.","The decision to develop a system that could be quantitatively evaluated on a large number of examples resulted in an important constraint: we could not make use of inference mechanisms such as those assumed by traditional computational theories of definite description resolution (e.g., Sidner 1979; Carter 1987; Alshawi 1990; Poesio 1993).",Pre Contiguous
647,X96-1048.xml,J00-4003.xml,(Class IV includes cases of idiomatic expressions or doubts expressed by the annotators).,"The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.","example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.","6 Tn this experiment, our subjects could also classify a definite description as ""idiomatic"" or.","""doubt""--see tables below.",Pre Contiguous
648,X96-1048.xml,W97-1307.xml,Kennedy and Boguraev {1996) then report a 75% accuracy for an algorithm that approximates Lappin and Leass's with more robust and coarse-grained syntactic input.,"After describ­ ing the algorithm in the next section, I will briefly compare the present approach with these pronoun resolution approaches.","Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.","Note that only identity of reference was evaluated there.2 The three main factors in this algorithm are (a) accessible text regions, (b) semantic consistency, and {c) dynamic syntactic preference.",The algo­ rithm is invoked for each sentence after the earlier finite-state transd uction phases have determined the best sequence(s) of nominal and verbal expres­ sions.,Single Citance
649,X96-1048.xml,P06-1059.xml,"In this paper we focus on the Named Entity Recognition (NER) task, which is the first step in tackling more complex tasks such as relation extraction and knowledge mining.","Biomedical NER (BioNER) tasks are, in general, more difficult than ones in the news domain.","For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).","Many of the previous studies of BioNER tasks have been based on machine learning techniques including Hidden Markov Models (HMMs) (Bikel et al., 1997), the dictionary HMM model (Kou et al., 2005) and Maximum Entropy Markov Models (MEMMs) (Finkel et al., 2004).","Among these methods, conditional random fields (CRFs) (Lafferty et al., 2001) have achieved good results (Kim et al., 2005; Settles, 2004), presumably because they are free from the so-called label bias problem by using a global normalization.",Multi Citance
